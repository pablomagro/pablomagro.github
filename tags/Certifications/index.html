<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Tag: Certifications | Technical Tips to Revitalize Your Devices and Memory</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Some tech things to refresh and don&#39;t forget">
<meta property="og:type" content="website">
<meta property="og:title" content="Technical Tips to Revitalize Your Devices and Memory">
<meta property="og:url" content="https://blog.pablo-magro-gaspar.site/tags/Certifications/index.html">
<meta property="og:site_name" content="Technical Tips to Revitalize Your Devices and Memory">
<meta property="og:description" content="Some tech things to refresh and don&#39;t forget">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Pablo Magro Gaspar">
<meta name="twitter:card" content="summary">
<meta name="twitter:creator" content="@https:&#x2F;&#x2F;twitter.com&#x2F;esrhim">
<link rel="publisher" href="https://plus.google.com/u/0/113537401711657998977?rel=author">
  
    <link rel="alternate" href="/atom.xml" title="Technical Tips to Revitalize Your Devices and Memory" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-70668709-6', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Technical Tips to Revitalize Your Devices and Memory</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://blog.pablo-magro-gaspar.site"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-AWS-Certified-SysOps-Administrator-Associate-SOA-C02" class="article article-type-post"
  itemscope itemtype="http://schema.org/Blog" id="post-hexo-tags-structureddata-keywords" class="article article-type-post" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2023/05/15/AWS-Certified-SysOps-Administrator-Associate-SOA-C02/" class="article-date">
  <time datetime="2023-05-15T01:27:03.000Z" itemprop="datePublished">15 May 2023</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/AWS/">AWS</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/05/15/AWS-Certified-SysOps-Administrator-Associate-SOA-C02/">AWS Certified SysOps Administrator – Associate (SOA-C02)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Summary-of-concepts-for-AWS-SysOps-Administrator-Certification"><a href="#Summary-of-concepts-for-AWS-SysOps-Administrator-Certification" class="headerlink" title="Summary of concepts for AWS SysOps Administrator Certification."></a>Summary of concepts for AWS SysOps Administrator Certification.</h1><h2 id="CloudWatch"><a href="#CloudWatch" class="headerlink" title="CloudWatch"></a>CloudWatch</h2><h3 id="AWS-CloudWatch-Metrics"><a href="#AWS-CloudWatch-Metrics" class="headerlink" title="AWS CloudWatch Metrics"></a>AWS CloudWatch Metrics</h3><p>CloudWatch provides <code>metrics for every services in AWS</code></p>
<ul>
<li><code>Metric</code> is a variable to monitor (CPUUtilization, NetworkIn…)</li>
<li>Metrics belong to <code>namespaces</code></li>
<li>Dimension is an attribute of a metric (instance id, environment, etc…).</li>
<li>Up to 30 dimensions per metric</li>
<li>Metrics have <code>timestamps</code></li>
<li>Can create CloudWatch dashboards of metrics</li>
</ul>
<p>AWS Provided metrics (AWS pushes them):</p>
<ol>
<li><code>Basic</code> Monitoring (default): metrics are collected at a <code>5 minute</code> internal</li>
<li><code>Detailed</code> Monitoring (paid): metrics are collected at a <code>1 minute</code> interval</li>
<li>Includes <code>CPU, Network, Disk and Status Check Metrics</code></li>
</ol>
<p>Custom metric (<strong>yours to push</strong>):</p>
<ol>
<li>Basic Resolution: 1 minute resolution</li>
<li>High Resolution: all the way to 1 second resolution</li>
<li>Include <code>RAM</code>, application level metrics</li>
<li>Make sure the IAM permissions on the EC2 instance role are correct !</li>
</ol>
<p><u><strong>RAM is NOT included in the AWS EC2 metrics</strong></u></p>
<h4 id="CloudWatch-Custom-Metrics"><a href="#CloudWatch-Custom-Metrics" class="headerlink" title="CloudWatch Custom Metrics"></a>CloudWatch Custom Metrics</h4><p>You can retrieve custom metrics from your applications or services using the <code>StatsD</code> and <code>collectd</code> protocols. StatsD is supported on both Linux servers and servers running Windows Server. collectd is supported only on Linux</p>
<ul>
<li><p>Possibility to define and send your own custom metrics to CloudWatch</p>
</li>
<li><p>Example: memory (RAM) usage, disk space, number of logged in users …</p>
</li>
<li><p>Use API call <code>PutMetricData</code></p>
</li>
<li><p>Ability to use dimensions (attributes) to segment metrics</p>
<ul>
<li>Instance.id</li>
<li>Environment.name</li>
</ul>
</li>
<li><p>Metric resolution (<code>StorageResolution</code> API parameter – two possible value):</p>
<ul>
<li>Standard: 1 minute (60 seconds)</li>
<li>High Resolution: 1&#x2F;5&#x2F;10&#x2F;30 second(s) – Higher cost</li>
</ul>
</li>
<li><p>Important <code>👀 Exam</code>: <strong><code>Accepts metric data points two weeks in the past and two hours in the future (make sure to configure your EC2 instance time correctly)</code></strong></p>
</li>
<li><p>You can <code>use AWS CLI or API</code> to <code>upload</code> the <code>data metrics</code> to CloudWatch.</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aws cloudwatch put-metric-data --metric-name PageViewCount --namespace MyService --value 2 --timestamp 2023-01-01-14T08:00:00.000Z</span><br></pre></td></tr></table></figure>

<h3 id="CloudWatch-Dashboards"><a href="#CloudWatch-Dashboards" class="headerlink" title="CloudWatch Dashboards"></a>CloudWatch Dashboards</h3><ul>
<li>Great way to setup custom dashboards for quick access to key metrics and alarms</li>
<li><strong><code>Dashboards are global</code></strong></li>
<li><strong>&#96;&#96;Dashboards can include graphs from different AWS accounts and regions</strong>&#96;&#96;** <code>👀 Exam</code></li>
<li>You can change the time zone &amp; time range of the dashboards</li>
<li>You can setup automatic refresh (10s, 1m, 2m, 5m, 15m)</li>
<li>Dashboards can be shared with people who don’t have an AWS account (public, email address, 3rd party SSO provider through Amazon Cognito)</li>
</ul>
<p>CloudWatch Logs - Sources</p>
<ul>
<li>SDK, CloudWatch Logs Agent, CloudWatch Unified Agent</li>
<li>Elastic Beanstalk: collection of logs from application</li>
<li>ECS: collection from containers</li>
<li>AWS Lambda: collection from function logs</li>
<li>VPC Flow Logs: VPC specific logs</li>
<li>API Gateway</li>
<li>CloudTrail based on filter</li>
<li>Route53: Log DNS queries</li>
</ul>
<h3 id="CloudWatch-Logs-Subscriptions"><a href="#CloudWatch-Logs-Subscriptions" class="headerlink" title="CloudWatch Logs Subscriptions"></a>CloudWatch Logs Subscriptions</h3><ul>
<li><p><strong><code>Get a real-time log events from CloudWatch Logs for processing and analysis</code></strong></p>
</li>
<li><p>Send to Kinesis Data Streams, Kinesis Data Firehose, or Lambda</p>
</li>
<li><p><code>Subscription Filter </code>– filter which logs are events delivered to your destination</p>
</li>
<li><p><code>Cross-Account Subscription</code> – send log events to resources in a different AWS account (KDS, KDF)</p>
</li>
</ul>
<h3 id="Alarms"><a href="#Alarms" class="headerlink" title="Alarms"></a>Alarms</h3><p>CloudWatch alarms allow you to monitor metrics and trigger actions based on defined thresholds. In this case, you can create a CloudWatch alarm that monitors the CPU utilization metric of the EC2 instance. When the CPU utilization reaches 100%, the alarm will be triggered, and you can configure actions such as sending notifications or executing automated actions to address the unresponsiveness issue.</p>
<h3 id="Alarm-Targets"><a href="#Alarm-Targets" class="headerlink" title="Alarm Targets:"></a>Alarm Targets:</h3><ul>
<li><p><code>EC2</code> - Stop, Terminate, Reboot, or Recover an EC2 Instance</p>
</li>
<li><p><code>EC2 Auto Scaling</code> - Trigger Auto Scaling Action, scaling up or down.</p>
</li>
<li><p><code>SNS</code> - Send notification to SNS (from which you can do pretty much anything)</p>
</li>
<li><p><code>Composite Alarms are monitoring the states of multiple other alarms</code></p>
</li>
</ul>
<h3 id="EC2-Instance-Recovery"><a href="#EC2-Instance-Recovery" class="headerlink" title="EC2 Instance Recovery"></a>EC2 Instance Recovery</h3><p>StatusCheckFailed_System</p>
<ul>
<li><p>Status Check:</p>
<ul>
<li><code>Instance status = check the EC2 VM</code></li>
<li><code>System status = check the underlying hardware</code></li>
</ul>
</li>
<li><p><code>Recovery: Same Private, Public, Elastic IP, metadata, placement group</code></p>
<pre><code>👀 Alarms can be created based on CloudWatch Logs Metrics Filters
</code></pre>
</li>
<li><p>Test an alarm using <code>aws set-alarm-state</code></p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">👀 aws cloudwatch set-alarm-state --alarm-name <span class="string">&quot;TerminateInHighCPU&quot;</span> --state-value ALARM --state-reason <span class="string">&quot;testing purposes&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="CloudWatch-Synthetics"><a href="#CloudWatch-Synthetics" class="headerlink" title="CloudWatch Synthetics"></a>CloudWatch Synthetics</h3><p>CloudWatch Synthetics canaries are <code>automated/configurable scripts</code> that <code>monitor</code> the <code>availability and performance of applications, endpoints, and APIs</code>. They are <code>designed to simulate user interactions with an application and provide insights into its behavior</code>.</p>
<p>Canaries are created using scripts written in Node.js or Python and are scheduled to run at regular intervals. These scripts perform tasks such as navigating through a website, clicking on specific elements, submitting forms, and validating responses. By executing these predefined actions, canaries can monitor the functionality, responsiveness, and performance of an application or API.</p>
<p>CloudWatch Synthetics canaries collect data on metrics like response time, latency, availability, and success rates. They can also be configured to generate alarms when certain conditions are met, allowing proactive identification and remediation of issues.</p>
<h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/awssupport/latest/user/trusted-advisor-check-reference.html">https://docs.aws.amazon.com/awssupport/latest/user/trusted-advisor-check-reference.html</a></p>
<h3 id="Amazon-EventBridge-formerly-CloudWatch-Events"><a href="#Amazon-EventBridge-formerly-CloudWatch-Events" class="headerlink" title="Amazon EventBridge (formerly CloudWatch Events)"></a>Amazon EventBridge (formerly CloudWatch Events)</h3><ul>
<li>Schedule: Cron jobs (scheduled scripts) - Schedule Every hour -&gt;  Trigger script on Lambda function</li>
<li>Event Pattern: Event rules to react to a service doing something - IAM Root User Sign in Event -&gt;  SNS Topic with Email Notification</li>
<li>Trigger Lambda functions, send SQS&#x2F;SNS messages…</li>
</ul>
<p><img src="/../images/AWS-EventBridge-Rules.png" alt="EventBridge Rules"></p>
<h3 id="Service-Quotas-CloudWatch-Alarms"><a href="#Service-Quotas-CloudWatch-Alarms" class="headerlink" title="Service Quotas CloudWatch Alarms"></a>Service Quotas CloudWatch Alarms</h3><ul>
<li>Notify you when you’re close to a service quota value threshold</li>
<li>Create CloudWatch Alarms on the Service Quotas console</li>
<li>Example: Lambda concurrent executions</li>
<li>Helps you know if you need to request a quota increase or shutdown resources before limit is reached</li>
</ul>
<h3 id="Alternative-Trusted-Advisor-CW-Alarms"><a href="#Alternative-Trusted-Advisor-CW-Alarms" class="headerlink" title="Alternative: Trusted Advisor CW Alarms"></a>Alternative: Trusted Advisor CW Alarms</h3><ul>
<li>Limited number of Service Limits checks in Trusted Advisor (~50)</li>
<li>Trusted Advisor publishes its check results to CloudWatch</li>
</ul>
<h3 id="👀-For-each-production-EC2-instance-create-an-Amazon-CloudWatch-alarm-for-Status-Check-Failed-System-Set-the-alarm-action-to-recover-the-EC2-instance-Configure-the-alarm-notification-to-be-published-to-an-Amazon-Simple-Notification-Service-Amazon-SNS-topic"><a href="#👀-For-each-production-EC2-instance-create-an-Amazon-CloudWatch-alarm-for-Status-Check-Failed-System-Set-the-alarm-action-to-recover-the-EC2-instance-Configure-the-alarm-notification-to-be-published-to-an-Amazon-Simple-Notification-Service-Amazon-SNS-topic" class="headerlink" title="👀 For each production EC2 instance, create an Amazon CloudWatch alarm for Status Check Failed: System. Set the alarm action to recover the EC2 instance. Configure the alarm notification to be published to an Amazon Simple Notification Service (Amazon SNS) topic."></a><code>👀</code> For each production EC2 instance, create an <code>Amazon CloudWatch alarm</code> for Status <code>Check Failed: System</code>. Set the alarm action to <code>recover the EC2 instance</code>. Configure the alarm notification to be published to an Amazon Simple Notification Service (Amazon SNS) topic.</h3><p><code>Explanation</code>: By creating a <code>CloudWatch alarm</code> for Status <code>Check Failed: System</code>, you can<code> automate the recovery task of EC2 instances</code> (<code>stop, terminate, reboot, or recover your Amazon EC2 instances</code>). When the system health check fails for an EC2 instance, the alarm will be triggered and perform the configured action to recover the instance. This eliminates the need for manual intervention. Additionally, configuring the alarm to publish notifications to an SNS topic allows you to receive notifications whenever a system health check fails.</p>
<h3 id="Status-Check"><a href="#Status-Check" class="headerlink" title="Status Check"></a>Status Check</h3><p>Automated checks to identify <code>hardware</code> and <code>software issues</code>.</p>
<p>System Status Checks</p>
<ul>
<li>Monitors problems with AWS systems (software&#x2F;hardware issues on the physical host, loss of system power, …)</li>
<li>Check <code>Personal Health Dashboard</code> for any scheduled critical maintenance by AWS to your instance’s host</li>
<li>Resolution: stop and start the instance (instance migrated to a new host)<ul>
<li>Either wait for AWS to fix the host, OR</li>
<li>Move the EC2 instance to a new host &#x3D; STOP &amp; START the instance (if EBS backed)
Instance Status Checks</li>
</ul>
</li>
<li>Monitors software&#x2F;network configuration of your instance (invalid network configuration, exhausted memory, …)</li>
<li>Resolution: reboot the instance or change instance configuration.</li>
</ul>
<h4 id="Status-Checks-CW-Metrics-amp-Recovery-OJO-👀"><a href="#Status-Checks-CW-Metrics-amp-Recovery-OJO-👀" class="headerlink" title="Status Checks - CW Metrics &amp; Recovery - OJO 👀"></a>Status Checks - CW Metrics &amp; Recovery - OJO <code>👀</code></h4><ul>
<li>CloudWatch Metrics (1 minute interval)<ul>
<li><code>StatusCheckFailed_System</code></li>
<li><code>StatusCheckFailed_Instance</code></li>
<li><code>StatusCheckFailed</code> (for both)</li>
</ul>
</li>
<li>Option 1: <code>CloudWatch Alarm</code><ul>
<li>Recover EC2 instance with the same private&#x2F;public IP, EIP, metadata, and Placement Group</li>
<li>Send notifications using SNS  trigger</li>
</ul>
</li>
<li>Option 2: <code>Auto Scaling Group</code><ul>
<li>Set min&#x2F;max&#x2F;desired 1 to recover an instance but <code>won&#39;t keep the same private and elastic IP</code>.&#96;</li>
</ul>
</li>
</ul>
<h4 id="Determine-which-instance-use-the-most-bandwidth"><a href="#Determine-which-instance-use-the-most-bandwidth" class="headerlink" title="Determine which instance use the most bandwidth"></a>Determine which instance use the most bandwidth</h4><p><code>NetworkIn</code> <code>and NetworkOut</code></p>
<h4 id="Identify-the-processing-power-required"><a href="#Identify-the-processing-power-required" class="headerlink" title="Identify the processing power required"></a>Identify the processing power required</h4><p><code>👀</code> <code>CPUUtilization</code> specifies the percentage of allocated EC2 compute units that are currently in use on the instance. This metric identifies the <code>processing power required</code> to <code>run</code> an <code>application on a selected instance</code>. This metric is expressed in Percent.</p>
<h4 id="Number-of-users"><a href="#Number-of-users" class="headerlink" title="Number of users."></a>Number of users.</h4><p><code>👀</code> <code>ActiveConnectionCount</code> This metric <code>represents</code> the <code>total number of concurrent TCP connections active</code> from clients to the load balancer and from the load balancer to targets.</p>
<h4 id="RAMUtilization-is-NOT-available-as-an-EC2-metric"><a href="#RAMUtilization-is-NOT-available-as-an-EC2-metric" class="headerlink" title="RAMUtilization is NOT available as an EC2 metric"></a>RAMUtilization is <code>NOT available as</code> an <code>EC2 metric</code></h4><p><code>RAMUtilization</code> You can publish your own metrics to CloudWatch using the AWS CLI or an API. You can view statistical graphs of your published metrics with the AWS Management Console. Metrics produced by AWS services are standard resolution by default.</p>
<h4 id="5xx-server-errors"><a href="#5xx-server-errors" class="headerlink" title="5xx server errors"></a>5xx server errors</h4><p>To monitor the number of 500 Internal Error responses that you’re getting, you can enable Amazon CloudWatch metrics. Amazon S3 CloudWatch request metrics include a metric for 5xx server errors.</p>
<h4 id="Q-4xxx"><a href="#Q-4xxx" class="headerlink" title="*Q 4xxx"></a><code>*Q</code> 4xxx</h4><p>You can set an alarm to notify operators when the 404 filter metric exceeds a threshold.</p>
<h4 id="Events"><a href="#Events" class="headerlink" title="Events"></a>Events</h4><p>You can run <code>CloudWatch Events</code> rules according to a <code>schedule</code>.</p>
<h4 id="EBS-Snapshots"><a href="#EBS-Snapshots" class="headerlink" title="EBS Snapshots"></a>EBS Snapshots</h4><p>It is possible to <code>create an automated snapshot of an existing Amazon Elastic Block Store (Amazon EBS)</code> volume on a schedule. You can choose a fixed rate to create a snapshot every few minutes or use a cron expression to specify that the snapshot is made at a specific time of day.</p>
<p>Snapshots are incremental backups, which means that only the blocks on the device that have changed after your most recent snapshot are saved. This minimizes the time required to create the snapshot and saves on storage costs by not duplicating data. Each snapshot contains all of the information that is needed to restore your data (from the moment when the snapshot was taken) to a new EBS volume.</p>
<p>Reference: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/TakeScheduledSnapshot.html">Schedule Automated Amazon EBS Snapshots Using CloudWatch Events</a></p>
<h3 id="Q-Filters"><a href="#Q-Filters" class="headerlink" title="*Q Filters"></a>*Q <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html">Filters</a></h3><p>You can create a <code>count</code> of <code>404</code> errors <code>and exclude</code> other <code>4xx</code> errors with a filter pattern on 404 errors.</p>
<h3 id="Agents"><a href="#Agents" class="headerlink" title="Agents"></a>Agents</h3><p>If your AMI contains a CloudWatch agent, it’s automatically installed on EC2 instances when you create an EC2 Auto Scaling group. With the stock Amazon Linux AMI, you need to install it (AWS recommends to install via yum).</p>
<h4 id="Install-Agents-to-track-the-state-of-each-of-the-instances"><a href="#Install-Agents-to-track-the-state-of-each-of-the-instances" class="headerlink" title="Install Agents to track the state of each of the instances"></a>Install Agents to track the state of each of the instances</h4><p>You must attach the <code>CloudWatchAgentServerRole</code> IAM role to the EC2 instance to be able to run the CloudWatch agent on the instance. This role enables the CloudWatch agent to perform actions on the instance.</p>
<h3 id="Publish-custom-metrics-to-CloudWatch"><a href="#Publish-custom-metrics-to-CloudWatch" class="headerlink" title="Publish custom metrics to CloudWatch."></a>Publish custom metrics to CloudWatch.</h3><p>You can publish your own metrics to CloudWatch using the AWS CLI or an API. You can view statistical graphs of your published metrics with the AWS Management Console. CloudWatch stores data about a metric as a series of data points. Each data point has an associated time stamp. You can even publish an aggregated set of data points called a statistic set.</p>
<p>The <code>put-metric-data</code> <code>command</code> publishes metric data to Amazon CloudWatch, which associates it with the specified metric. If the specified metric does not exist, CloudWatch creates the metric which can take up to fifteen minutes for the metric to appear in calls to <code>ListMetrics</code>.</p>
<h3 id="Collect-process-metrics-with-the-procstat-plugin"><a href="#Collect-process-metrics-with-the-procstat-plugin" class="headerlink" title="Collect process metrics with the procstat plugin"></a>Collect process metrics with the <code>procstat</code> plugin</h3><p>The <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Agent-procstat-process-metrics.html">procstat</a> plugin enables you to <code>collect metrics from individual processes</code>. It is supported on Linux servers and on servers running Windows Server 2012 or later.</p>
<hr>
<h2 id="CloudTrail"><a href="#CloudTrail" class="headerlink" title="CloudTrail"></a>CloudTrail</h2><p><code>Provides governance, compliance and audit for your AWS Account</code></p>
<ul>
<li>CloudTrail is <code>enabled by default</code>!</li>
<li>Get <code>an history of events / API calls made within your AWS Account</code> by:<ul>
<li>Console</li>
<li>SDK</li>
<li>CLI</li>
<li>AWS Services</li>
</ul>
</li>
<li>Can put logs from CloudTrail into CloudWatch Logs or S3</li>
<li><code>A trail can be applied to All Regions (default) or a single Region</code></li>
<li>If a resource is deleted in AWS, investigate CloudTrail first!</li>
</ul>
<h3 id="CloudTrail-Insights"><a href="#CloudTrail-Insights" class="headerlink" title="CloudTrail Insights"></a>CloudTrail Insights</h3><ul>
<li>👀 Enable <code>CloudTrail Insights to detect unusual activity</code> in your account:<ul>
<li>inaccurate resource provisioning</li>
<li>hitting service limits</li>
<li>Bursts of AWS IAM actions</li>
<li>Gaps in periodic maintenance activity</li>
</ul>
</li>
<li>CloudTrail Insights analyzes normal management events to create a baseline</li>
<li>And then <code>continuously analyzes write events to detect unusual patterns</code>.<ul>
<li>Anomalies appear in the CloudTrail console</li>
<li>Event is sent to Amazon S3</li>
<li>An EventBridge event is generated (for automation needs)</li>
</ul>
</li>
</ul>
<h3 id="CloudTrail-–-Integration-with-EventBridge"><a href="#CloudTrail-–-Integration-with-EventBridge" class="headerlink" title="CloudTrail – Integration with EventBridge"></a>CloudTrail – Integration with EventBridge</h3><ul>
<li>Used to react to any API call being made in your account</li>
<li>CloudTrail is not “real-time”:<ul>
<li>Delivers an event within 15 minutes of an API call</li>
<li>Delivers log files to an S3 bucket every 5 minutes</li>
</ul>
</li>
</ul>
<h3 id="CloudTrail-–-Organizations-Trails"><a href="#CloudTrail-–-Organizations-Trails" class="headerlink" title="CloudTrail – Organizations Trails"></a>CloudTrail – Organizations Trails</h3><ul>
<li>A trail that will log all events for all AWS accounts in an AWS Organization</li>
<li>Log events for management and member accounts</li>
<li>Trail with the same name will be created in every AWS account (IAM permissions)</li>
<li>Member accounts can’t remove or modify the organization trail (view only)</li>
</ul>
<h3 id="CloudTrail-Log-File-Integrity-Validation"><a href="#CloudTrail-Log-File-Integrity-Validation" class="headerlink" title="CloudTrail - Log File Integrity Validation"></a>CloudTrail - Log File Integrity Validation</h3><p><strong><code>Digest Files</code></strong>:</p>
<ul>
<li><p>References the log files for the last hour and contains a hash of each</p>
</li>
<li><p>Stored in the same S3 bucket as log files (different folder)</p>
</li>
<li><p><code>Helps you determine whether a log file was modified/deleted after CloudTrail delivered it</code></p>
</li>
<li><p><code>Hashing using SHA-256, Digital Signing using SHA- 256 with RSĂ</code></p>
</li>
<li><p><code>Protect the S3 bucket using bucket policy, versioning, MFA Delete protection, encryption, object lock</code></p>
</li>
<li><p>Protect files using IAM</p>
<h3 id="Q-To-ensure-that-SysOps-administrators-can-easily-verify-that-the-CloudTrail-log-files-have-not-been-deleted-or-changed-the-following-action-should-be-taken"><a href="#Q-To-ensure-that-SysOps-administrators-can-easily-verify-that-the-CloudTrail-log-files-have-not-been-deleted-or-changed-the-following-action-should-be-taken" class="headerlink" title="Q. To ensure that SysOps administrators can easily verify that the CloudTrail log files have not been deleted or changed, the following action should be taken:"></a>Q. To ensure that SysOps administrators can easily verify that the CloudTrail log files have not been deleted or changed, the following action should be taken:</h3><p>Enable <code>CloudTrail log file integrity validation</code> when the trail is created or updated.</p>
<p><code>Explanation</code>: Enabling <code>CloudTrail log file integrity validation</code> ensures that the log files are protected against tampering or unauthorized modification. CloudTrail uses SHA-256 hashes to validate the integrity of the log files stored in Amazon S3. By enabling this feature, the SysOps administrators can easily verify the integrity of the log files and ensure that they have not been deleted or changed</p>
</li>
</ul>
<h3 id="Cloud-Trail-Integration-with-EventBridge-AWS-CloudTrail"><a href="#Cloud-Trail-Integration-with-EventBridge-AWS-CloudTrail" class="headerlink" title="Cloud Trail - Integration with EventBridge AWS CloudTrail"></a>Cloud Trail - Integration with EventBridge AWS CloudTrail</h3><ul>
<li>Used to react to any API call being made in your account</li>
<li>Cloud Trail is not “real-time”:<ul>
<li>Delivers an event within 15 minutes of an API call</li>
<li>Delivers log files to an S3 bucket every 5 minutes</li>
</ul>
</li>
</ul>
<h3 id="CloudTrail-–-Organizations-Trails-1"><a href="#CloudTrail-–-Organizations-Trails-1" class="headerlink" title="CloudTrail – Organizations Trails"></a>CloudTrail – Organizations Trails</h3><ul>
<li>A trail that will log all events for all AWS accounts in an AWS Organization</li>
<li>Log events for management and member accounts</li>
<li>Trail with the same name will be created in every AWS account (IAM permissions)</li>
<li><code>Member accounts can’t remove or modify the organization trail (view only)</code></li>
</ul>
<hr>
<h2 id="👀-Q-AWS-Config"><a href="#👀-Q-AWS-Config" class="headerlink" title="👀 *Q AWS Config"></a><code>👀 *Q</code> AWS Config</h2><ul>
<li>Helps with <code>auditing</code> and recording <strong><code>compliance</code></strong> of your AWS resources.</li>
<li>Helps <code>record configurations</code> and changes over time.
Questions that can be solved by AWS Config:<ul>
<li>Is there <strong>unrestricted SSH</strong> access to my security groups?</li>
<li>Do my <strong>buckets have any public access</strong>?</li>
<li>How has my <strong>ALB configuration changed over time</strong>?</li>
</ul>
</li>
<li>You can <code>receive alerts</code> (SNS notifications) for any changes</li>
<li>AWS Config is a <code>per-region service</code>.</li>
<li>Can be aggregated across regions and accounts.</li>
<li>Possibility of storing the configuration data into S3 (analyzed by Athena)</li>
</ul>
<p>AWS Config <code>keeps track of</code> the <code>configuration</code> of <code>your AWS resources and their relationships to other resources</code>. It can also <code>evaluate</code> those AWS resources for <u><strong><code>compliance</code></strong></u>. This service uses rules that can be configured to evaluate AWS resources against desired configurations.</p>
<p>For example,</p>
<ul>
<li>can track <code>changes</code> to <em><code>CloudFormation stacks</code></em>.</li>
</ul>
<p>AWS Config can track changes to CloudFormation stacks. A CloudFormation stack is a collection of AWS resources that you can manage as a single unit. With AWS Config, you can review the historical configuration of your CloudFormation stacks and review all changes that occurred to them.</p>
<p>For more information about how AWS Config can track changes to CloudFormation deployments, see <code>cloudformation-stack-drift-detection-check</code>.</p>
<ul>
<li>there are AWS Config <code>rules</code> that <code>check</code> whether or not your <em><code>Amazon S3 buckets have logging enabled</code></em> or your <code>IAM users have an MFA device enabled</code>.</li>
</ul>
<p>👀 AWS Config is a service that <code>enables</code> you to <code>assess</code>, <code>audit</code>, and <code>evaluate</code> the <code>configurations</code> of your <code>AWS resources</code>. It provides detailed <code>inventory</code> and <code>configuration</code> history of your resources, as well as configuration change notifications. With AWS Config, you can track the configuration of your S3 bucket, including its bucket policy.</p>
<p>AWS Config rules use AWS Lambda functions to perform the compliance evaluations, and the Lambda functions return the compliance status of the evaluated resources as compliant or noncompliant. The non-compliant resources are remediated using the remediation action associated with the AWS Config rule. With the Auto-Remediation feature of AWS Config rules, the remediation action can be executed automatically when a resource is found non-compliant.</p>
<p><code>AWS Config Auto Remediation feature</code> has auto remediate feature for any non-compliant S3 buckets using the following AWS Config rules:
s3-bucket-logging-enabled s3-bucket-server-side-encryption-enabled s3-bucket-public-read-prohibited s3-bucket-public-write-prohibited
These AWS Config rules act as controls to prevent any non-compliant S3 activities.</p>
<h3 id="Config-Rules"><a href="#Config-Rules" class="headerlink" title="Config Rules"></a>Config Rules</h3><p>AWS Config provides a number of AWS managed rules that address a wide range of security concerns such as checking if you <code>encrypted</code> your Amazon Elastic Block Store (Amazon EBS) volumes, tagged your resources appropriately, and enabled multi-factor authentication (MFA) for root accounts.</p>
<ul>
<li>Can use AWS <code>managed</code> config <code>rules</code> (over 75)</li>
<li>Can make <strong><code>custom config rules (must be defined in AWS Lambda)</code></strong>.<ul>
<li>Ex: evaluate if each EBS disk is of type gp2</li>
<li>Ex: evaluate if each EC2 instance is t2.micro</li>
</ul>
</li>
<li>Rules can be evaluated &#x2F; triggered:<ul>
<li>For each config change</li>
<li>And &#x2F; or: at regular time intervals</li>
</ul>
</li>
<li><strong><code>AWS Config Rules does not prevent actions from happening (no deny)</code></strong>.</li>
</ul>
<p>Managed rules:</p>
<ul>
<li><code>require-tags</code>: managed rule in AWS Config. This rule checks if a resource contains the tags that you specify.</li>
</ul>
<h3 id="Config-Rules-–-Remediations"><a href="#Config-Rules-–-Remediations" class="headerlink" title="Config Rules – Remediations"></a>Config Rules – Remediations</h3><p>Has auto remediate feature for any non-compliant S3 buckets using the following AWS Config rules:</p>
<p>s3-bucket-logging-enabled s3-bucket-server-side-encryption-enabled s3-bucket-public-read-prohibited s3-bucket-public-write-prohibited</p>
<p>These AWS Config rules act as controls to prevent any non-compliant S3 activities.</p>
<ul>
<li><code>Automate remediation of non-compliant resources using SSM Automation Documents</code>.</li>
<li>Use AWS-Managed Automation Documents or create custom Automation Documents<ul>
<li><code>Tip: you can create custom Automation Documents that invokes Lambda function</code>.</li>
</ul>
</li>
<li>You can set <code>Remediation Retries</code> if the resource is still non-compliant after autoremediation.</li>
</ul>
<h3 id="AWS-Config-Auto-Remediation"><a href="#AWS-Config-Auto-Remediation" class="headerlink" title="AWS Config Auto Remediation"></a>AWS Config Auto Remediation</h3><h3 id="Config-Rules-–-Notifications"><a href="#Config-Rules-–-Notifications" class="headerlink" title="Config Rules – Notifications"></a>Config Rules – Notifications</h3><ul>
<li>Use EventBridge to trigger notifications when AWS resources are noncompliant</li>
<li>Ability to send configuration changes and compliance state notifications to SNS (all events – use SNS Filtering or filter at client-side)</li>
</ul>
<h3 id="AWS-Config-–-Aggregators"><a href="#AWS-Config-–-Aggregators" class="headerlink" title="AWS Config – Aggregators"></a>AWS Config – Aggregators</h3><ul>
<li>The aggregator is created <code>in one central aggregator account</code>.</li>
<li>Aggregates <code>rules, resources, etc... across multiple accounts &amp; regions</code>.</li>
<li>If using <code>AWS Organizations</code>, no need for individual Authorization</li>
<li>Rules are created in each individual source AWS account</li>
<li>Can <code>deploy rules</code> to multiple target accounts using <code>CloudFormation StackSets</code></li>
</ul>
<h2 id="CloudWatch-vs-CloudTrail-vs-Config"><a href="#CloudWatch-vs-CloudTrail-vs-Config" class="headerlink" title="CloudWatch vs CloudTrail vs Config"></a>CloudWatch vs CloudTrail vs Config</h2><ul>
<li>CloudWatch<ul>
<li>Performance monitoring (metrics, CPU, network, etc…) &amp; dashboards</li>
<li>Events &amp; Alerting</li>
<li>Log Aggregation &amp; Analysis</li>
</ul>
</li>
<li>CloudTrail<ul>
<li>Record API calls made within your Account by everyone</li>
<li>Can define trails for specific resources</li>
<li>Global Service</li>
</ul>
</li>
<li>Config<ul>
<li>Record configuration changes</li>
<li>Evaluate resources against compliance rules</li>
<li>Get timeline of changes and compliance</li>
</ul>
</li>
</ul>
<hr>
<h2 id="Q-AWS-Artifact"><a href="#Q-AWS-Artifact" class="headerlink" title="*Q AWS Artifact"></a><code>*Q</code> <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/artifact/latest/ug/what-is-aws-artifact.html">AWS Artifact</a></h2><p>AWS Artifact <code>keeps compliance-related reports and agreements</code>.</p>
<p><code>👀</code></p>
<hr>
<h2 id="RDS"><a href="#RDS" class="headerlink" title="RDS"></a>RDS</h2><h3 id="Advantage-over-using-RDS-versus-deploying"><a href="#Advantage-over-using-RDS-versus-deploying" class="headerlink" title="Advantage over using RDS versus deploying"></a>Advantage over using RDS versus deploying</h3><ul>
<li>RDS is a managed service:<ul>
<li>Automated provisioning, OS patching</li>
<li>Continuous backups and restore to specific timestamp (Point in Time Restore)!</li>
<li>Monitoring dashboards</li>
<li>Read replicas for improved read performance</li>
<li>Multi AZ setup for DR (Disaster Recovery)</li>
<li>Maintenance windows for upgrades</li>
<li>Scaling capability (vertical and horizontal)</li>
<li>Storage backed by EBS (gp2 or io1)</li>
</ul>
</li>
<li>BUT you can’t SSH into your instances</li>
</ul>
<h3 id="RDS-Read-Replicas-for-read-scalability"><a href="#RDS-Read-Replicas-for-read-scalability" class="headerlink" title="RDS Read Replicas for read scalability"></a>RDS Read Replicas for read scalability</h3><ul>
<li>Up to 15 Read Replicas</li>
<li>Within <code>AZ, Cross AZ or Cross Region</code>.</li>
<li>Replication is <code>ASYNC</code>.</li>
<li>Replicas can be promoted to their own DB.</li>
</ul>
<h3 id="RDS-Read-Replicas-–-Network-Cost"><a href="#RDS-Read-Replicas-–-Network-Cost" class="headerlink" title="RDS Read Replicas – Network Cost"></a>RDS Read Replicas – Network Cost</h3><ul>
<li>In AWS there’s a network cost when data goes from one AZ to another</li>
<li><code>For RDS Read Replicas within the same region, you don’t pay that fee</code>.</li>
</ul>
<h3 id="RDS-Multi-AZ-Disaster-Recovery"><a href="#RDS-Multi-AZ-Disaster-Recovery" class="headerlink" title="RDS Multi AZ (Disaster Recovery)"></a>RDS Multi AZ (Disaster Recovery)</h3><ul>
<li><code>SYNC</code> replication.</li>
<li>One DNS name – automatic app failover to standby</li>
<li>Increase <code>availability</code>.</li>
<li>Failover in case of loss of AZ, loss of network, instance or storage failure</li>
</ul>
<p><code>👀 Exam</code> - <code>The Read Replicas be setup as Multi AZ for Disaster Recovery (DR)</code>.</p>
<h3 id="Lambda-in-VPC"><a href="#Lambda-in-VPC" class="headerlink" title="Lambda in VPC"></a>Lambda in VPC</h3><ul>
<li>You must define the VPC ID, the Subnets and the Security Groups</li>
<li>Lambda will create an ENI (Elastic Network Interface) in your subnets</li>
<li><code>AWSLambdaVPCAccessExecutionRole</code></li>
</ul>
<h3 id="RDS-Proxy-for-AWS-Lambda"><a href="#RDS-Proxy-for-AWS-Lambda" class="headerlink" title="RDS Proxy for AWS Lambda"></a>RDS Proxy for AWS Lambda</h3><ul>
<li>When using Lambda functions with RDS, it opens and maintains a database connection</li>
<li>This can result in a <code>“TooManyConnections”</code> exception</li>
<li>With <code>RDS Proxy</code>, you no longer need code that handles cleaning up idle connections and managing connection pools</li>
</ul>
<h3 id="DB-Parameter-Groups"><a href="#DB-Parameter-Groups" class="headerlink" title="DB Parameter Groups"></a>DB Parameter Groups</h3><ul>
<li><p>You can configure the DB engine using Parameter Groups</p>
</li>
<li><p>Dynamic parameters are applied immediately</p>
</li>
<li><p>Static parameters are applied after instance reboot</p>
</li>
<li><p>You can modify parameter group associated with a DB (must reboot)</p>
</li>
<li><p><em><code>Must-know parameter</code></em>:</p>
<ul>
<li>PostgreSQL &#x2F; SQL Server: <code>rds.force_ssl=1</code> &#x3D;&gt; force SSL connections</li>
<li>MySQL &#x2F; MariaDB: <code>require_secure_transport</code>&#x3D;1 &#x3D;&gt; force SSL connections</li>
</ul>
</li>
</ul>
<h3 id="RDS-Events-amp-Event-Subscriptions"><a href="#RDS-Events-amp-Event-Subscriptions" class="headerlink" title="RDS Events &amp; Event Subscriptions"></a>RDS Events &amp; Event Subscriptions</h3><p>RDS keeps record of events related to:</p>
<ul>
<li><p>DB instances</p>
</li>
<li><p>Snapshots</p>
</li>
<li><p>Parameter groups, security groups …</p>
</li>
<li><p>RDS Event Subscriptions</p>
<ul>
<li>Subscribe to events to be notified when an event occurs using SNS</li>
<li>Specify the Event Source (instances, SGs, …) and the Event Category (creation, failover, …)</li>
</ul>
</li>
<li><p><code>RDS delivers events to EventBridge</code></p>
</li>
</ul>
<h3 id="RDS-with-CloudWatch"><a href="#RDS-with-CloudWatch" class="headerlink" title="RDS with CloudWatch"></a>RDS with CloudWatch</h3><p>CloudWatch metrics associated with RDS (gathered from the hypervisor):</p>
<ul>
<li><p><code>DatabaseConnections</code></p>
</li>
<li><p><code>SwapUsage</code></p>
</li>
<li><p><code>ReadIOPS / WriteIOPS</code></p>
</li>
<li><p><code>ReadLatency / WriteLatency</code></p>
</li>
<li><p><code>ReadThroughPut / WriteThroughPut</code></p>
</li>
<li><p><code>DiskQueueDepth</code></p>
</li>
<li><p><code>FreeStorageSpace</code></p>
</li>
<li><p><code>Enhanced Monitoring</code> (gathered from an agent on the DB instance). <code>👀</code></p>
<ul>
<li>Useful when you need to see how <code>different processes or threads use the CPU</code>.</li>
</ul>
</li>
<li><p>Access to over 50 new CPU, memory, file system, and disk I&#x2F;O metrics</p>
</li>
</ul>
<p>Amazon RDS provides <code>metrics</code> in real time for the operating system (OS) that your DB instance runs on. You can view the metrics for your DB instance using the console. Also, you can consume the &#96;Enhanced Monitoring&#96;&#96; JSON output from Amazon CloudWatch Logs in a monitoring system of your choice.</p>
<h3 id="RDS-storage-autoscaling"><a href="#RDS-storage-autoscaling" class="headerlink" title="RDS storage autoscaling"></a><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html#USER_PIOPS.Autoscaling">RDS storage autoscaling</a></h3><p>With RDS storage autoscaling, you can set the desired maximum storage limit. Autoscaling will manage the storage size. RDS storage autoscaling monitors actual storage consumption and then scales capacity automatically when actual utilization approaches the provisioned storage capacity.</p>
<h3 id="👀-Q-Enable-Enhanced-Monitoring"><a href="#👀-Q-Enable-Enhanced-Monitoring" class="headerlink" title="👀 *Q Enable Enhanced Monitoring"></a><code>👀</code> <code>*Q</code> Enable Enhanced Monitoring</h3><hr>
<h2 id="Amazon-Aurora-DB"><a href="#Amazon-Aurora-DB" class="headerlink" title="Amazon Aurora DB"></a>Amazon Aurora DB</h2><ul>
<li>Aurora is a proprietary technology from AWS (not open sourced)</li>
<li>Postgres and MySQL are both supported as Aurora DB (that means your drivers will work as if Aurora was a Postgres or MySQL database)</li>
<li>Aurora is “AWS cloud optimized” and claims 5x performance improvement over MySQL on RDS, over 3x the performance of Postgres on RDS</li>
<li><code>Aurora storage automatically grows in increments of 10GB, up to 128 TB</code>.</li>
<li>Aurora can have up to 15 replicas and the replication process is faster than MySQL (sub 10 ms replica lag)</li>
<li>Failover in Aurora is instantaneous. It’s HA (High Availability) native.</li>
<li>Aurora costs more than RDS (20% more) – but is more efficient</li>
</ul>
<h3 id="Aurora-High-Availability-and-Read-Scaling"><a href="#Aurora-High-Availability-and-Read-Scaling" class="headerlink" title="Aurora High Availability and Read Scaling"></a>Aurora High Availability and Read Scaling</h3><ul>
<li><p>One Aurora Instance takes writes (master)</p>
</li>
<li><p><code>Support for Cross Region Replication</code></p>
<p>  <code>Shared storage Volume</code>: Replication + Self Healing + Auto expanding</p>
<p>  Reader Endpoint Connection Load Balancing</p>
</li>
</ul>
<h3 id="RDS-amp-Aurora-Security"><a href="#RDS-amp-Aurora-Security" class="headerlink" title="RDS &amp; Aurora Security"></a>RDS &amp; Aurora Security</h3><ul>
<li><code>At-rest encryption</code>:<ul>
<li>Database master &amp; replicas encryption using AWS KMS - must be defined as launch time</li>
<li>If the master is not encrypted, the read replicas cannot be encrypted
To encrypt an un-encrypted database, go through a DB snapshot &amp; restore as encrypted</li>
</ul>
</li>
<li><code>In-flight encryption</code>: TLS-ready by default, use the AWS TLS root certificates client-side</li>
<li><code>IAM Authentication: IAM roles</code> to connect to your database (instead of username&#x2F;pw)</li>
<li><code>Security Groups</code>: Control Network access to your RDS &#x2F; Aurora DB</li>
<li><code>No SSH available except on RDS Custom</code></li>
<li><code>Audit Logs can be enabled and sent to CloudWatch Logs for longer retention</code></li>
</ul>
<h3 id="Aurora-for-SysOps"><a href="#Aurora-for-SysOps" class="headerlink" title="Aurora for SysOps"></a>Aurora for SysOps</h3><ul>
<li><p>You can associate a priority tier (0-15) on each Read Replica</p>
<ul>
<li>Controls the failover priority</li>
<li>RDS will promote the Read Replica with the highest priority (lowest tier)</li>
<li>If replicas have the same priority, RDS promotes the largest in size</li>
<li>If replicas have the same priority and size, RDS promotes arbitrary replica</li>
</ul>
</li>
<li><p><code>You can migrate an RDS MySQL snapshot to Aurora MySQL Cluster</code></p>
</li>
</ul>
<h3 id="Connect-to-Amazon-Aurora-DB-cluster-from-outside-a-VPC"><a href="#Connect-to-Amazon-Aurora-DB-cluster-from-outside-a-VPC" class="headerlink" title="Connect to Amazon Aurora DB cluster from outside a VPC"></a>Connect to Amazon Aurora DB cluster from outside a VPC</h3><p>To connect to an Amazon Aurora DB cluster directly from outside the VPC, the instances in the cluster must meet the following requirements:</p>
<ol>
<li>The DB instance must have a public IP address.</li>
<li>The DB instance must be running in a publicly accessible subnet.</li>
</ol>
<p>For Amazon Aurora DB instances, you can’t choose a specific subnet. Instead, choose a DB subnet group when you create the instance. Create a DB subnet group with subnets of similar network configuration. For example, a DB subnet group for Public subnets.</p>
<h3 id="Metrics-to-generate-reports-on-the-Aurora-DB-Cluster-and-its-replicas"><a href="#Metrics-to-generate-reports-on-the-Aurora-DB-Cluster-and-its-replicas" class="headerlink" title="Metrics to generate reports on the Aurora DB Cluster and its replicas"></a>Metrics to generate reports on the Aurora DB Cluster and its replicas</h3><ol>
<li><code>AuroraReplicaLagMaximum</code> - This metric captures the <code>maximum amount of lag between the primary instance and each Aurora DB instance in the DB cluster</code>.</li>
<li><code>AuroraBinlogReplicaLag</code> - This metric captures the <code>amount of time a replica DB cluster running</code> on Aurora MySQL-Compatible Edition lags behind the source DB cluster.
This metric reports the value of the Seconds_Behind_Master field of the MySQL SHOW SLAVE STATUS command. This metric is useful for monitoring replica lag between Aurora DB clusters that are replicating across different AWS Regions.</li>
<li><code>AuroraReplicaLag</code> - This metric captures the <code>amount of lag</code> an Aurora replica experiences <code>when replicating updates from the primary instance</code>.</li>
<li><code>InsertLatency</code> - This metric captures <code>the average duration of insert operations</code>.</li>
</ol>
<h2 id="👀-Aurora-Reader-Endpoint"><a href="#👀-Aurora-Reader-Endpoint" class="headerlink" title="👀  Aurora Reader Endpoint"></a>👀  Aurora Reader Endpoint</h2><p>To <code>perform queries</code>, you can connect to the reader endpoint, with Aurora automatically performing load-balancing among all the Aurora <code>Replicas</code>.</p>
<p>A reader endpoint for an Aurora DB cluster provides load-balancing support for read-only connections to the DB cluster. Use the reader endpoint <code>for read operations</code>, such as queries. By processing those statements on the read-only Aurora Replicas, this endpoint reduces the overhead on the primary instance. It also helps the cluster to scale the capacity to handle simultaneous SELECT queries, proportional to the number of Aurora Replicas in the cluster. Each Aurora DB cluster has one reader endpoint.</p>
<p>Reference:<a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html">Amazon Aurora connection management</a></p>
<hr>
<h2 id="Amazon-ElastiCache-Overview"><a href="#Amazon-ElastiCache-Overview" class="headerlink" title="Amazon ElastiCache Overview"></a>Amazon ElastiCache Overview</h2><ul>
<li><p>The same way RDS is to get managed Relational Databases…</p>
</li>
<li><p>ElastiCache is to get managed <code>Redis or Memcached</code></p>
</li>
<li><p>Caches are in-memory databases with really high performance, low latency</p>
</li>
<li><p><code>Helps reduce load off of databases for read intensive workloads</code></p>
</li>
<li><p><code>Helps make your application stateless</code></p>
</li>
<li><p>AWS takes care of OS maintenance &#x2F; patching, optimizations, setup, configuration, monitoring, failure recovery and backups</p>
<pre><code>Using ElastiCache involves heavy application code changes
</code></pre>
</li>
</ul>
<h3 id="ElastiCache-Replication-Redis-Cluster-Mode-Disabled"><a href="#ElastiCache-Replication-Redis-Cluster-Mode-Disabled" class="headerlink" title="ElastiCache Replication (Redis): Cluster Mode Disabled"></a>ElastiCache Replication (Redis): Cluster Mode Disabled</h3><ul>
<li>One primary node, up to 5 replicas</li>
<li>Asynchronous Replication</li>
<li>The primary node is used for read&#x2F;write</li>
<li>The other nodes are read-only</li>
<li><code>One shard, all nodes have all the data</code></li>
<li>Guard against data loss if node failure</li>
<li>Multi-AZ enabled by default for failover</li>
<li>Helpful to scale read performance</li>
<li>Horizontal and vertical</li>
</ul>
<h3 id="ElastiCache-Replication-Cluster-Mode-Enabled"><a href="#ElastiCache-Replication-Cluster-Mode-Enabled" class="headerlink" title="ElastiCache Replication: Cluster Mode Enabled"></a>ElastiCache Replication: Cluster Mode Enabled</h3><p>Data is partitioned across shards (helpful to scale writes)</p>
<ul>
<li>Automatically increase&#x2F;decrease the desired shards or replicas</li>
<li>Supports both Target Tracking and Scheduled Scaling Policies</li>
<li>Works only for Redis with Cluster Mode Enabled</li>
</ul>
<hr>
<h2 id="Memcached"><a href="#Memcached" class="headerlink" title="Memcached"></a>Memcached</h2><h3 id="Fix-high-Memcached-evictions"><a href="#Fix-high-Memcached-evictions" class="headerlink" title="Fix high Memcached evictions"></a>Fix high Memcached evictions</h3><p>To fix the issue of high Memcached evictions in Amazon ElastiCache, the following actions should be taken:</p>
<ol>
<li><code>Increase</code> the <code>size of the nodes</code> in the cluster: This allows for more available memory in each node, reducing the likelihood of evictions due to limited cache space.</li>
<li><code>Increase</code> the <code>number of nodes</code> in the cluster: By adding more nodes, the overall cache capacity increases, reducing the chance of evictions.</li>
</ol>
<p>The <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/CacheMetrics.WhichShouldIMonitor.html#metrics-evictions">Evictions metric</a> for Amazon ElastiCache for Memcached represents the number of nonexpired items that the cache evicted to provide space for new items. If you are experiencing evictions with your cluster, it is usually a sign that you need to scale up (use a node that has a larger memory footprint)
or scale out (add additional nodes to the cluster) to accommodate the additional data</p>
<h2 id="VPC"><a href="#VPC" class="headerlink" title="VPC"></a><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/vpc/latest/userguide/how-it-works.html#what-is-privatelink">VPC</a></h2><p>With Amazon Virtual Private Cloud (Amazon VPC), you can launch AWS resources in a logically isolated virtual network that you’ve defined. This virtual network closely resembles a traditional network that you’d operate in your own data center, with the benefits of using the scalable infrastructure of AWS.</p>
<p><code>enableDnsHostnames</code> – Indicates whether the instances launched in the VPC get public DNS hostnames. If this attribute is true, instances in the VPC get public DNS hostnames, but only <code>if</code> the <code>enableDnsSupport</code> attribute is also set to <code>true</code>.</p>
<p><code>enableDnsSupport</code> – Indicates whether the DNS resolution is supported for the VPC. If this attribute is false, the Amazon-provided DNS server in the VPC that resolves public DNS hostnames to IP addresses is not enabled. If this attribute is true, queries to the Amazon provided DNS server at the 169.254.169.253 IP address, or the reserved IP address at the base of the VPC IPv4 network range plus two will succeed.</p>
<p>By default, both attributes are set to <code>true</code> in a default VPC or a VPC created by the VPC wizard. By default, only the <code>enableDnsSupport</code> attribute is set to true in a VPC created on the Your VPCs page of the VPC console or using the AWS CLI, API, or an AWS SDK.</p>
<h3 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h3><p><code>Regardless of the type of subnet, the internal IPv4 address range of the subnet is always private</code>. AWS never announces these address blocks to the internet.</p>
<p><code>When you create a VPC, you must specify a range of IPv4 addresses for the VPC</code> in the form of a Classless Inter-Domain Routing (CIDR) block; for example, 10.0.0.0&#x2F;16. This is the primary CIDR block for your VPC.</p>
<p><code>Subnets created in a VPC can communicate with each other</code>, this is default behaviour. The main route table facilitates this communication.</p>
<p>Reference: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/vpc/latest/userguide/how-it-works.html">How Amazon VPC works</a></p>
<h3 id="Connect-the-Lambda-function-to-a-private-subnet-that-has-a-route-to-a-NAT-gateway-deployed-in-a-public-subnet-of-the-VPC"><a href="#Connect-the-Lambda-function-to-a-private-subnet-that-has-a-route-to-a-NAT-gateway-deployed-in-a-public-subnet-of-the-VPC" class="headerlink" title="Connect the Lambda function to a private subnet that has a route to a NAT gateway deployed in a public subnet of the VPC."></a>Connect the Lambda function to a private subnet that has a route to a NAT gateway deployed in a public subnet of the VPC.</h3><p><code>Explanation</code>: By connecting the Lambda function to a private subnet with a route to a NAT gateway, the function can access resources within the VPC while also leveraging the NAT gateway to access the internet and communicate with third-party APIs. The NAT gateway acts as a bridge between the private subnet and the internet, allowing the Lambda function to securely access external resources.</p>
<h3 id="VPC-Endpoint"><a href="#VPC-Endpoint" class="headerlink" title="VPC Endpoint"></a>VPC Endpoint</h3><p>A VPC Endpoint allows you to <code>connect</code> your <code>VPC</code> directly <code>to AWS services</code> without the need for <code>internet gateways</code>, <code>NAT gateways</code>, or <code>VPN connections</code>. It enables private communication between your VPC and the AWS service without going over the internet.</p>
<p>To configure a VPC Endpoint for accessing AWS Systems Manager APIs, you can follow these steps:</p>
<ol>
<li>Create a VPC Endpoint for AWS Systems Manager in your Amazon VPC. This creates an elastic network interface with a private IP address within your VPC.</li>
<li>Update the route tables in your VPC to route traffic destined for the AWS Systems Manager API endpoints to the VPC Endpoint. This ensures that traffic is directed through the VPC Endpoint instead of going over the internet.</li>
<li>Verify that your on-premises instances and AWS managed instances are configured to use the appropriate VPC and route tables.</li>
</ol>
<h3 id="Egress-only-Internet-Gateway"><a href="#Egress-only-Internet-Gateway" class="headerlink" title="Egress-only Internet Gateway"></a>Egress-only Internet Gateway</h3><p>An egress-only internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows outbound communication over IPv6 from instances in your VPC to the internet, and prevents the internet from initiating an IPv6 connection with your instances.</p>
<p>An egress-only internet gateway is for use with <code>IPv6 traffic only</code>. To enable outbound-only internet communication <code>over IPv4</code>, use a <code>NAT gateway</code> instead.</p>
<p>Reference: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html">Enable outbound IPv6 traffic using an egress-only internet gateway</a></p>
<h3 id="Carrier-gateway"><a href="#Carrier-gateway" class="headerlink" title="Carrier gateway"></a>Carrier gateway</h3><p>A Carrier gateway is a highly available virtual appliance that <code>provides outbound IPv6 internet connectivity for instances in your VPC</code>. It acts as a gateway between your VPC and the internet, <code>allowing</code> IPv6 <code>traffic</code> to flow <code>in and out</code> of your <code>VPC</code>. By configuring a Carrier gateway, you can enable outbound communication over IPv6 for the EC2 instances in the private subnets while keeping them isolated from direct internet access.</p>
<h2 id="Security-Groups"><a href="#Security-Groups" class="headerlink" title="Security Groups"></a>Security Groups</h2><h3 id="The-reason-for-the-issue-where-the-new-EC2-instances-are-unable-to-mount-the-Amazon-EFS-file-system-in-a-new-Availability-Zone-could-be"><a href="#The-reason-for-the-issue-where-the-new-EC2-instances-are-unable-to-mount-the-Amazon-EFS-file-system-in-a-new-Availability-Zone-could-be" class="headerlink" title="The reason for the issue where the new EC2 instances are unable to mount the Amazon EFS file system in a new Availability Zone could be:"></a>The reason for the issue where the new EC2 instances are unable to mount the Amazon EFS file system in a new Availability Zone could be:</h3><p>The security group for the mount target does not allow inbound NFS connections from the security group used by the EC2 instances.</p>
<p><code>Explanation</code>: When mounting an Amazon EFS file system from EC2 instances, the security group associated with the mount target should allow inbound NFS (Network File System) connections from the security group used by the EC2 instances. By default, the security group associated with the mount target allows inbound connections from the default security group of the VPC. If the EC2 instances are using a different security group, it needs to be added to the mount target’s security group’s inbound rules to allow NFS connections.</p>
<p>👀 - Only support <code>allow</code> rules. You have to allow incoming traffic from your customers to your instances</p>
<h2 id="CloudFormation"><a href="#CloudFormation" class="headerlink" title="CloudFormation"></a>CloudFormation</h2><h3 id="cfn-init"><a href="#cfn-init" class="headerlink" title="cfn-init"></a>cfn-init</h3><ul>
<li>AWS::CloudFormation::Init must be in the Metadata of a resource</li>
<li>With the cfn-init script, it helps make complex EC2 configurations readable</li>
<li>The EC2 instance will query the CloudFormation service to get init data</li>
<li>Logs go to &#x2F;var&#x2F;log&#x2F;cfn-init.log</li>
</ul>
<p>(More readable compared with user data scripts)</p>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">UserData:</span></span><br><span class="line">  <span class="attr">Fn::Base64:</span></span><br><span class="line">    <span class="type">!Sub</span> <span class="string">|</span></span><br><span class="line"><span class="string">      #!/bin/bash -xe</span></span><br><span class="line"><span class="string">      # Get the latest CloudFormation package</span></span><br><span class="line"><span class="string">      yum update -y aws-cfn-bootstrap</span></span><br><span class="line"><span class="string">      # Start cfn-init</span></span><br><span class="line"><span class="string">      /opt/aws/bin/cfn-init -s $&#123;AWS::StackId&#125; -r MyInstance --region $&#123;AWS::Region&#125; || error_exit &#x27;Failed to run cfn-init&#x27;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Metadata:</span></span><br><span class="line">  <span class="attr">Comment:</span> <span class="string">Install</span> <span class="string">a</span> <span class="string">simple</span> <span class="string">Apache</span> <span class="string">HTTP</span> <span class="string">page</span></span><br><span class="line">  <span class="attr">AWS::CloudFormation::Init:</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">packages:</span></span><br><span class="line">        <span class="attr">yum:</span></span><br><span class="line">          <span class="attr">httpd:</span> []</span><br><span class="line">      <span class="attr">files:</span></span><br><span class="line">        <span class="string">&quot;/var/www/html/index.html&quot;</span><span class="string">:</span></span><br><span class="line">          <span class="attr">content:</span> <span class="string">|</span></span><br><span class="line"><span class="string">            &lt;h1&gt;Hello World from EC2 instance!&lt;/h1&gt;</span></span><br><span class="line"><span class="string">            &lt;p&gt;This was created using cfn-init&lt;/p&gt;</span></span><br><span class="line"><span class="string"></span>          <span class="attr">mode:</span> <span class="string">&#x27;000644&#x27;</span></span><br><span class="line">      <span class="attr">commands:</span></span><br><span class="line">        <span class="attr">hello:</span></span><br><span class="line">          <span class="attr">command:</span> <span class="string">&quot;echo &#x27;hello world&#x27;&quot;</span></span><br><span class="line">      <span class="attr">services:</span></span><br><span class="line">        <span class="attr">sysvinit:</span></span><br><span class="line">          <span class="attr">httpd:</span></span><br><span class="line">            <span class="attr">enabled:</span> <span class="string">&#x27;true&#x27;</span></span><br><span class="line">            <span class="attr">ensureRunning:</span> <span class="string">&#x27;true&#x27;</span></span><br></pre></td></tr></table></figure>

<h3 id="cfn-signal-amp-wait-conditions"><a href="#cfn-signal-amp-wait-conditions" class="headerlink" title="cfn-signal &amp; wait conditions"></a>cfn-signal &amp; wait conditions</h3><ul>
<li><p>We still don’t know how to tell CloudFormation that the EC2 instance got properly configured after a <code>cfn-init</code></p>
</li>
<li><p>For this, we can use the <code>cfn-signal</code> script!</p>
<ul>
<li>We run cfn-signal right after cfn-init</li>
<li>Tell CloudFormation service to keep on going or fail</li>
</ul>
</li>
<li><p>We need to define <code>WaitCondition</code>:</p>
<ul>
<li>Block the template until it receives a signal from cfn-signal</li>
<li>We attach a <code>CreationPolicy</code> (also works on EC2, ASG)</li>
</ul>
</li>
</ul>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Start cfn-signal to the wait condition</span></span><br><span class="line"><span class="string">/opt/aws/bin/cfn-signal</span> <span class="string">-e</span> <span class="string">$?</span> <span class="string">--stack</span> <span class="string">$&#123;AWS::StackId&#125;</span> <span class="string">--resource</span> <span class="string">SampleWaitCondition</span> <span class="string">--region</span> <span class="string">$&#123;AWS::Region&#125;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">SampleWaitCondition:</span></span><br><span class="line">  <span class="attr">Type:</span> <span class="string">AWS::CloudFormation::WaitCondition</span></span><br><span class="line">  <span class="attr">CreationPolicy:</span></span><br><span class="line">    <span class="attr">ResourceSignal:</span></span><br><span class="line">      <span class="attr">Timeout:</span> <span class="string">PT2M</span></span><br><span class="line">      <span class="attr">Count:</span> <span class="number">1</span></span><br></pre></td></tr></table></figure>

<h3 id="StackSets"><a href="#StackSets" class="headerlink" title="StackSets"></a>StackSets</h3><h4 id="Use-AWS-CloudFormation-StackSets-for-Multiple-Accounts-in-an-AWS-Organization"><a href="#Use-AWS-CloudFormation-StackSets-for-Multiple-Accounts-in-an-AWS-Organization" class="headerlink" title="Use AWS CloudFormation StackSets for Multiple Accounts in an AWS Organization:"></a><code>Use</code> AWS CloudFormation <code>StackSets for Multiple Accounts in an AWS Organization</code>:</h4><p>Use AWS CloudFormation <code>StackSets</code> to <code>deploy a template</code> to <code>each account</code> to create the new IAM roles.</p>
<p><code>Explanation</code>: AWS CloudFormation StackSets allows you to deploy a CloudFormation template across multiple AWS accounts. By using StackSets, you can create and manage the same IAM roles in each account within the organization. This ensures consistent deployment of roles across accounts and simplifies the management process.</p>
<p><code>Reference</code>: <a target="_blank" rel="noopener" href="https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/">New: Use AWS CloudFormation StackSets for Multiple Accounts in an AWS Organization</a></p>
<h3 id="Q-To-lunch-the-last-AMI"><a href="#Q-To-lunch-the-last-AMI" class="headerlink" title="*Q To lunch the last AMI."></a><code>*Q</code> To lunch the last AMI.</h3><p>Use the Parameters section in the template to specify the Systems Manager (SSM) Parameter, which contains the latest version of the Windows regional AMI ID.</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Parameters:</span></span><br><span class="line">  <span class="attr">LatestWindowsAMIParameter:</span></span><br><span class="line">    <span class="attr">Type:</span> <span class="string">AWS::SSM::Parameter::Value&lt;AWS::EC2::Image::Id&gt;</span></span><br><span class="line">    <span class="attr">Default:</span> <span class="string">/LatestWindowsAMI</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="attr">Parameters:</span></span><br><span class="line">  <span class="attr">LatestWindowsAMIParameter:</span></span><br><span class="line">    <span class="attr">Type:</span> <span class="string">AWS::SSM::Parameter::Value&lt;AWS::EC2::Image::Id&gt;</span></span><br><span class="line">    <span class="attr">Default:</span> <span class="string">/LatestWindowsAMI</span></span><br></pre></td></tr></table></figure>

<h3 id="Q-UpdatePolicy-attribute"><a href="#Q-UpdatePolicy-attribute" class="headerlink" title="*Q UpdatePolicy attribute"></a><code>*Q</code> <code>UpdatePolicy</code> attribute</h3><p>By adding the <code>UpdatePolicy</code> attribute in CloudFormation and enabling the WaitOnResourceSignals property, the Auto Scaling group update process will be handled more gracefully. This approach allows CloudFormation to monitor the health and success of each instance during the update process before moving on to the next instance.</p>
<p>Appending a health check at the end of the user data script allows the instance to signal CloudFormation that it has successfully completed its initialization. This helps ensure that the instance is fully operational before proceeding to the next instance in the Auto Scaling group update process.</p>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">CreationPolicy:</span></span><br><span class="line">  <span class="attr">ResourceSignal:</span></span><br><span class="line">    <span class="attr">Count:</span> <span class="string">&#x27;3&#x27;</span></span><br><span class="line">    <span class="attr">Timeout:</span> <span class="string">PT15M</span></span><br><span class="line"><span class="attr">UpdatePolicy:</span></span><br><span class="line">  <span class="attr">AutoScalingRollingUpdate:</span></span><br><span class="line">    <span class="attr">MinInstancesInService:</span> <span class="string">&#x27;1&#x27;</span></span><br><span class="line">    <span class="attr">MaxBatchSize:</span> <span class="string">&#x27;2&#x27;</span></span><br><span class="line">    <span class="attr">PauseTime:</span> <span class="string">PT1M</span></span><br><span class="line">    <span class="attr">WaitOnResourceSignals:</span> <span class="string">&#x27;true&#x27;</span></span><br></pre></td></tr></table></figure>

<h3 id="👀-DependsOn-attribute"><a href="#👀-DependsOn-attribute" class="headerlink" title="👀 DependsOn attribute"></a>👀 <code>DependsOn</code> attribute</h3><p>With the DependsOn attribute, you can specify that the <code>creation</code> of a specific resource <code>follows another</code>. When you add a DependsOn attribute to a resource, that resource is created only after the creation of the resource specified in the DependsOn attribute.</p>
<hr>
<h2 id="AWS-Backup"><a href="#AWS-Backup" class="headerlink" title="AWS Backup"></a>AWS Backup</h2><ul>
<li>Fully managed service</li>
<li><code>Centrally manage and automate backups across AWS services</code>.</li>
<li>No need to create custom scripts and manual processes</li>
<li>Supported services:<ul>
<li>Amazon <code>EC2 / Amazon EBS</code></li>
<li>Amazon <code>S3</code></li>
<li>Amazon <code>RDS (all DBs engines) / Amazon Aurora / Amazon DynamoDB</code></li>
<li>Amazon <code>DocumentDB / Amazon Neptune</code></li>
<li>Amazon <code>EFS / Amazon FSx (Lustre &amp; Windows File Server)</code></li>
</ul>
</li>
<li>AWS Storage Gateway (Volume Gateway)</li>
<li>Supports cross-region backups</li>
<li>Supports cross-account backups</li>
<li>On-Demand and Scheduled backups</li>
<li>Tag-based backup policies</li>
<li>You create backup policies known as <code>Backup Plans</code><ul>
<li>Backup frequency (every 12 hours, daily, weekly, monthly, cron expression)</li>
<li>Backup window</li>
<li>Transition to Cold Storage (Never, Days, Weeks, Months, Years)</li>
<li>Retention Period (Always, Days, Weeks, Months, Year</li>
</ul>
</li>
</ul>
<p><code>👀 *Q</code>
AWS Backup is a fully managed and cost-effective backup service that simplifies and <code>automates data</code> backup across AWS services including <code>Amazon EBS</code>, <code>Amazon EC2</code>, <code>Amazon RDS</code>, <code>Amazon Aurora</code>, <code>Amazon DynamoDB</code>, <code>Amazon EFS</code>, and AWS <code>Storage Gateway</code>. In addition, AWS Backup <code>leverages</code> AWS <code>Organizations</code> to implement and maintain a central view of backup policy across resources in a multi-account AWS environment. Customers <code>simply tag</code> and associate their <code>AWS resources</code> with backup policies managed by AWS Backup for Cross-Region data replication.</p>
<h3 id="👀-Q-For-the-production-account-a-SysOps-administrator-must-ensure-that-all-data-is-backed-up-daily-for-all-current-and-future-Amazon-EC2-instances-and-Amazon-Elastic-File-System-Amazon-EFS-file-systems-Backups-must-be-retained-for-30-days"><a href="#👀-Q-For-the-production-account-a-SysOps-administrator-must-ensure-that-all-data-is-backed-up-daily-for-all-current-and-future-Amazon-EC2-instances-and-Amazon-Elastic-File-System-Amazon-EFS-file-systems-Backups-must-be-retained-for-30-days" class="headerlink" title="👀 *Q  For the production account, a SysOps administrator must ensure that all data is backed up daily for all current and future Amazon EC2 instances and Amazon Elastic File System (Amazon EFS) file systems. Backups must be retained for 30 days"></a><code>👀 *Q</code>  For the production account, a SysOps administrator must ensure that all data is backed up daily for all current and future Amazon EC2 instances and Amazon Elastic File System (Amazon EFS) file systems. Backups must be retained for 30 days</h3><p>Create a backup plan in AWS Backup. Assign resources by resource ID, selecting all existing EC2 and EFS resources that are running in the account. Edit the backup plan daily to include any new resources. Schedule the backup plan to run every day with a lifecycle policy to expire backups after 30 days.</p>
<p><code>Explanation</code>: AWS Backup provides a centralized and automated solution for backing up data. By creating a backup plan and assigning resources by resource ID, you can easily include all existing EC2 instances and EFS file systems in the backup process. Editing the backup plan daily ensures that any new resources are automatically included in the backups. By scheduling the backup plan to run every day and configuring a lifecycle policy to expire backups after 30 days, you meet the requirement of daily backups with a retention period of 30 days.</p>
<h3 id="AWS-Backup-Vault-Lock"><a href="#AWS-Backup-Vault-Lock" class="headerlink" title="AWS Backup Vault Lock"></a>AWS Backup Vault Lock</h3><ul>
<li>Enforce a WORM (Write On1ce Read Many) state for all the backups that you store in your AWS Backup Vault</li>
<li>Additional layer of defense to protect your backups against:<ul>
<li>Inadvertent or malicious delete operations</li>
<li>Updates that shorten or alter retention periods</li>
</ul>
</li>
<li>Even the root user cannot delete backups when enabled</li>
</ul>
<hr>
<h2 id="AWS-Shared-Responsibility-Model"><a href="#AWS-Shared-Responsibility-Model" class="headerlink" title="AWS Shared Responsibility Model"></a>AWS Shared Responsibility Model</h2><ul>
<li>AWS responsibility - Security <code>of</code> the Cloud<ul>
<li>Protecting infrastructure (hardware, software, facilities, and networking) that runs all the AWS services</li>
<li>Managed services like S3, DynamoDB, RDS, etc.</li>
</ul>
</li>
<li>Customer responsibility - Security <code>in</code> the Cloud<ul>
<li>For EC2 instance, customer is responsible for management of the guest OS (including security patches and updates), firewall &amp; network configuration, IAM</li>
<li>Encrypting application data</li>
</ul>
</li>
<li>Shared controls:<ul>
<li>Patch Management, Configuration Management, Awareness &amp; Training</li>
</ul>
</li>
</ul>
<hr>
<h2 id="DDoS-Distributed-Denial-of-service-Protection-on-AWS"><a href="#DDoS-Distributed-Denial-of-service-Protection-on-AWS" class="headerlink" title="DDoS (Distributed Denial-of-service ) Protection on AWS"></a>DDoS (Distributed Denial-of-service ) Protection on AWS</h2><ul>
<li><code>AWS Shield Standard</code>: protects against DDoS attack for your website and applications, for all customers at no additional costs.</li>
<li><code>AWS Shield Advanced</code>: 24&#x2F;7 premium DDoS protection.</li>
<li><code>AWS WAF</code>: Filter specific requests based on rules.</li>
<li><code>CloudFront and Route 53</code>:<ul>
<li>Availability protection using global edge network</li>
<li>Combined with AWS Shield, provides attack mitigation at the edge</li>
</ul>
</li>
<li>Be ready to scale – leverage <code>AWS Auto Scaling</code>.</li>
</ul>
<h2 id="AWS-WAF-–-Web-Application-Firewall"><a href="#AWS-WAF-–-Web-Application-Firewall" class="headerlink" title="AWS WAF – Web Application Firewall"></a>AWS WAF – Web Application Firewall</h2><ul>
<li><p>Protects your web applications from common web exploits (Layer 7)</p>
</li>
<li><p><code>Layer 7 is HTTP</code> (vs Layer 4 is TCP)</p>
</li>
<li><p>Deploy on <code>Application Load Balancer, API Gateway, CloudFront</code></p>
</li>
<li><p>Define Web ACL (Web Access Control List):</p>
<ul>
<li>Rules can include: <code>IP addresses</code>, HTTP headers, HTTP body, or URI strings</li>
<li>Protects from common attack - <code>SQL injection</code> and <code>Cross-Site Scripting (XSS)</code>.</li>
<li>Size constraints, <code>geo-match (block countries)</code></li>
<li><code>Rate-based rules</code> (to count occurrences of events) – <code>for DDoS protection</code></li>
</ul>
</li>
</ul>
<h2 id="Penetration-Testing-on-AWS-Cloud"><a href="#Penetration-Testing-on-AWS-Cloud" class="headerlink" title="Penetration Testing on AWS Cloud"></a>Penetration Testing on AWS Cloud</h2><ul>
<li>AWS customers are welcome to carry out security assessments or penetration tests against their AWS infrastructure <code>without prior approval for 8 services</code>:<ul>
<li>Amazon EC2 instances, NAT Gateways, and Elastic Load Balancers</li>
<li>Amazon RDS</li>
<li>Amazon CloudFront</li>
<li>Amazon Aurora</li>
<li>Amazon API Gateways</li>
<li>AWS Lambda and Lambda Edge functions</li>
<li>Amazon Lightsail resources</li>
<li>Amazon Elastic Beanstalk environments</li>
</ul>
</li>
</ul>
<h3 id="Penetration-Testing-on-your-AWS-Cloud"><a href="#Penetration-Testing-on-your-AWS-Cloud" class="headerlink" title="Penetration Testing on your AWS Cloud"></a>Penetration Testing on your AWS Cloud</h3><ul>
<li><p><code>Prohibited Activities</code></p>
<ul>
<li>DNS zone walking via Amazon Route 53 Hosted Zones</li>
<li>Denial of Service (DoS), Distributed Denial of Service (DDoS), Simulated DoS, Simulated DDoS</li>
<li>Port flooding</li>
<li>Protocol flooding</li>
<li>Request flooding (login request flooding, API request flooding)</li>
</ul>
</li>
<li><p>Read more: <a target="_blank" rel="noopener" href="https://aws.amazon.com/security/penetration-testing/">https://aws.amazon.com/security/penetration-testing/</a></p>
</li>
</ul>
<h2 id="Q-AWS-Inspector"><a href="#Q-AWS-Inspector" class="headerlink" title="*Q AWS Inspector"></a><code>*Q</code> <a target="_blank" rel="noopener" href="https://aws.amazon.com/inspector/">AWS Inspector</a></h2><p>Amazon Inspector is <code>used for security compliance of instances and applications</code>.</p>
<p>Amazon Inspector is an automated security assessment service that helps you <code>test</code> the <code>network accessibility</code> of your Amazon EC2 instances and the <code>security state of your applications running on the instances</code>.</p>
<p>An Amazon Inspector assessment report can be generated for an assessment run once it has been successfully completed. An assessment report is a document that details what is tested in the assessment run, and the results of the assessment. The results of your assessment are formatted into a standard report, which can be generated to share results within your team for remediation actions, to enrich compliance audit data, or to store for future reference.</p>
<p>You can select from two types of report for your assessment, a findings report or a full report. The findings report contains an executive summary of the assessment, the instances targeted, the rules packages tested, the rules that generated findings, and detailed information about each of these rules along with the list of instances that failed the check. The full report contains all the
information in the findings report and additionally provides the list of rules that were checked and passed on all instances in the assessment target.</p>
<ul>
<li><code>Automated Security Assessments</code></li>
<li><code>For EC2 instances</code></li>
<li><code>For Container Images push to Amazon ECR</code></li>
<li><code>For Lambda Functions</code></li>
</ul>
<p>-&gt; <code>Reporting &amp; integration with AWS Security Hub</code></p>
<p>-&gt; <code>Send findings to Amazon Event Bridge</code></p>
<h3 id="What-does-Amazon-Inspector-evaluate"><a href="#What-does-Amazon-Inspector-evaluate" class="headerlink" title="What does Amazon Inspector evaluate?"></a>What does Amazon Inspector evaluate?</h3><ul>
<li><p><code>Only for EC2 instances, Container Images &amp; Lambda functions</code></p>
</li>
<li><p>Continuous scanning of the infrastructure, only when needed</p>
</li>
<li><p>Package vulnerabilities (EC2, ECR &amp; Lambda) – database of CVE</p>
</li>
<li><p>Network reachability (EC2)</p>
</li>
<li><p>A risk score is associated with all vulnerabilities for prioritization</p>
</li>
</ul>
<p>Amazon Inspector discovers potential security issues by using security rules to analyze AWS resources. Amazon Inspector also integrates with AWS Security Hub to provide a view of your security posture across multiple AWS accounts.</p>
<hr>
<h2 id="Amazon-GuardDuty-👀"><a href="#Amazon-GuardDuty-👀" class="headerlink" title="Amazon GuardDuty -  👀"></a>Amazon GuardDuty -  👀</h2><ul>
<li><code>Intelligent Threat discovery to protect your AWS Account</code>.</li>
<li>Uses Machine Learning algorithms, anomaly detection, 3rd party data</li>
<li>One click to enable (30 days trial), no need to install software</li>
<li>Input data includes:<ul>
<li><code>CloudTrail Events Logs</code> – unusual API calls, unauthorized deployments<ul>
<li><code>CloudTrail Management Events</code> – create VPC subnet, create trail, …</li>
<li><code>CloudTrail S3 Data Events</code> – get object, list objects, delete object, …</li>
</ul>
</li>
<li><code>VPC Flow Logs</code> – unusual internal traffic, unusual IP address</li>
<li><code>DNS Logs</code> – compromised EC2 instances sending encoded data within DNS queries</li>
<li><code>Optional Feature</code> – EKS Audit Logs, RDS &amp; Aurora, EBS, Lambda, S3 Data Events…</li>
</ul>
</li>
<li>Can setup <code>EventBridge rules</code> to be notified in case of findings</li>
<li>EventBridge rules can target AWS Lambda or SNS</li>
<li><code>Can protect against CryptoCurrency attacks (has a dedicated “finding” for it)</code> - Exam</li>
</ul>
<hr>
<h2 id="AWS-Macie"><a href="#AWS-Macie" class="headerlink" title="AWS Macie"></a>AWS Macie</h2><ul>
<li>Amazon Macie is a fully managed data security and data privacy service that uses <code>machine learning and pattern matching to discover andprotect your sensitive data in AWS</code>.</li>
<li>Macie helps identify and alert you to <code>sensitive data, such as personally identifiable information (PII)</code>.</li>
<li>Notify to Amazon EventBridge &#x3D;&gt; Integrations</li>
</ul>
<hr>
<h2 id="Q-AWS-Trusted-Advisor"><a href="#Q-AWS-Trusted-Advisor" class="headerlink" title="*Q AWS Trusted Advisor"></a><code>*Q</code> <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/awssupport/latest/user/trusted-advisor.html">AWS Trusted Advisor</a></h2><p>Trusted Advisor provides real-time guidance to help users follow AWS best practices to provision their resources. <code>Hight level account assessment</code>.</p>
<p>E.g. AWS Trusted Advisor checks for service <code>usage</code> that is more than 80% of the <code>service limit</code>.</p>
<h3 id="Check-categories-Exam"><a href="#Check-categories-Exam" class="headerlink" title="Check categories - Exam"></a>Check categories - Exam</h3><ul>
<li><code>Cost optimization</code></li>
<li><code>Performance</code></li>
<li><code>Security</code></li>
<li><code>Fault tolerance</code></li>
<li><code>Service limits</code></li>
</ul>
<h3 id="Trusted-Advisor-–-Support-Plans-Exam"><a href="#Trusted-Advisor-–-Support-Plans-Exam" class="headerlink" title="Trusted Advisor – Support Plans - Exam"></a>Trusted Advisor – Support Plans - Exam</h3><ul>
<li><p>7 CORES CHECKS for Basic &amp; Developer Support plan</p>
<ul>
<li>S3 Bucket Permissions</li>
<li>Security Groups – Specific Ports Unrestricted</li>
<li>IAM Use (one IAM user minimum)</li>
<li>MFA on Root Account</li>
<li>EBS Public Snapshots</li>
<li>RDS Public Snapshots</li>
<li>Service Limits</li>
</ul>
</li>
<li><p>FULL CHECKS</p>
<ul>
<li>Full Checks available on the 5 categories</li>
<li>Ability to set CloudWatch alarms when   reaching limits</li>
<li><code>Programmatic Access using AWS Support API</code> - Exa,</li>
</ul>
</li>
</ul>
<hr>
<h2 id="AWS-KMS-Key-Management-Service"><a href="#AWS-KMS-Key-Management-Service" class="headerlink" title="AWS KMS (Key Management Service)"></a>AWS KMS (Key Management Service)</h2><ul>
<li>Anytime you hear “encryption” for an AWS service, it’s most likely KMS</li>
<li>AWS manages encryption keys for us</li>
<li>Fully integrated with IAM for authorization</li>
<li>Easy way to control access to your data</li>
<li><code>Able to audit KMS Key usage using CloudTrail</code> - Exam</li>
<li>Seamlessly integrated into most AWS services (EBS, S3, RDS, SSM…)</li>
<li>Never ever store your secrets in plaintext, especially in your code!</li>
<li>KMS Key Encryption also available through API calls (SDK, CLI)</li>
<li>Encrypted secrets can be stored in the code &#x2F; environment variables</li>
</ul>
<h3 id="KMS-Automatic-Key-Rotation"><a href="#KMS-Automatic-Key-Rotation" class="headerlink" title="KMS Automatic Key Rotation"></a>KMS Automatic Key Rotation</h3><ul>
<li><code>For Customer-managed CMK</code> (not AWS managed CMK)</li>
<li>If enabled: automatic key rotation happens <code>every 1 year</code>.</li>
<li>Previous key is kept active so you can decrypt old data</li>
<li>New Key has the same CMK ID (only the backing key is changed)</li>
</ul>
<h3 id="KMS-Manual-Key-Rotation"><a href="#KMS-Manual-Key-Rotation" class="headerlink" title="KMS Manual Key Rotation"></a>KMS Manual Key Rotation</h3><ul>
<li><code>When you want to rotate key every 90 days, 180 days, etc...</code></li>
<li>New Key has a different CMK ID</li>
<li>Keep the previous key active so you can decrypt old data</li>
<li>Better to use aliases in this case (to hide the change of key for the application)</li>
<li>Good solution to rotate CMK that are not eligible for automatic rotation (<code>like asymmetric CMK</code>)</li>
</ul>
<h3 id="Changing-The-KMS-Key-For-An-Encrypted-EBS-Volume"><a href="#Changing-The-KMS-Key-For-An-Encrypted-EBS-Volume" class="headerlink" title="Changing The KMS Key For An Encrypted EBS Volume"></a>Changing The KMS Key For An Encrypted EBS Volume</h3><ul>
<li>You <code>can’t</code> change <code>the encryptionkeys used by an EBS volume</code>.</li>
<li><code>Create an EBS snapshot</code> and create a new EBS volume and <code>specify the new KMS key</code>.</li>
</ul>
<h3 id="Sharing-KMS-Encrypted-RDS-DB-Snapshots"><a href="#Sharing-KMS-Encrypted-RDS-DB-Snapshots" class="headerlink" title="Sharing KMS Encrypted RDS DB Snapshots"></a>Sharing KMS Encrypted RDS DB Snapshots</h3><p>You can <code>share RDS DB snapshots encrypted with KMS CMK with other accounts</code>, but must f<code>irst share the KMS CMK</code> with the target account <code>using Key Policy</code>.</p>
<h3 id="KMS-Key-Deletion-Considerations"><a href="#KMS-Key-Deletion-Considerations" class="headerlink" title="KMS Key Deletion Considerations"></a>KMS Key Deletion Considerations</h3><ul>
<li>Schedule CMK for deletion with a waiting period of 7 to 30 days</li>
<li>CMK’s status is “Pending deletion” during the waiting period</li>
<li>During the CMK’s deletion waiting period:</li>
<li>The CMK can’t be used for cryptographic operations (e.g., can’t decrypt KMS- encrypted objects in S3 – SSE-KMS)</li>
<li>The key is not rotated even if planned</li>
<li>You can cancel the key deletion during the waiting period</li>
<li>Consider disabling your key instead of deleting it if you’re not sure!</li>
</ul>
<h2 id="👀"><a href="#👀" class="headerlink" title="👀"></a>👀</h2><p>You can allow IAM users or roles in one AWS account to use a customer master key (CMK) in a different AWS account. You can add these permissions when you create the CMK or change the permissions for an existing CMK.</p>
<p><code>To permit the usage of a CMK to users and roles in another account, you must use two different types of policies</code>:</p>
<ol>
<li>The key policy for the CMK must give the external account (or users and roles in the external account) permission to use the CMK. The key policy is in the account that owns the CMK.</li>
<li>IAM policies in the external account must delegate the key policy permissions to its users and roles. These policies are set in the external account and give permissions to users and roles in that account.</li>
</ol>
<hr>
<h2 id="CloudHSM-Dedicated-Hardware-HSM-x3D-Hardware-Security-Module"><a href="#CloudHSM-Dedicated-Hardware-HSM-x3D-Hardware-Security-Module" class="headerlink" title="CloudHSM - Dedicated Hardware (HSM &#x3D; Hardware Security Module)"></a>CloudHSM - Dedicated Hardware (HSM &#x3D; Hardware Security Module)</h2><ul>
<li>KMS &#x3D;&gt; AWS manages the <code>software</code> for encryption.</li>
<li>CloudHSM &#x3D;&gt; AWS provisions encryption <code>hardware</code>.</li>
<li><code>CUSTOMER MANAGED CMK</code>.</li>
<li>You manage your own encryption keys entirely (not AWS)</li>
<li><code>HSM device is tamper resistant, FIPS 140-2 Level 3 compliance</code>.</li>
<li>Supports both <code>symmetric</code> and <code>asymmetric</code> encryption (SSL&#x2F;TLS keys)</li>
<li>No free tier available</li>
<li>Must use the CloudHSM Client Software</li>
<li>Redshift supports CloudHSM for database encryption and key management</li>
<li><code>Good option to use with SSE-C encryption</code>.</li>
</ul>
<h3 id="CloudHSM-–-High-Availability"><a href="#CloudHSM-–-High-Availability" class="headerlink" title="CloudHSM – High Availability"></a>CloudHSM – High Availability</h3><ul>
<li>CloudHSM clusters are spread across Multi AZ (HA)</li>
<li>Great for availability and durability</li>
</ul>
<hr>
<h2 id="👀-AWS-Artifact-Not-really-a-service-EXAM"><a href="#👀-AWS-Artifact-Not-really-a-service-EXAM" class="headerlink" title="👀 AWS Artifact (Not really a service) - EXAM"></a>👀 <code>AWS Artifact</code> (Not really a service) - EXAM</h2><p>is a service that provides <code>on-demand</code> access to AWS <code>compliance reports</code> and other relevant documents.</p>
<ul>
<li><code>Artifact Reports</code> - Allows you to download AWS security and compliance documents from third-party auditors, like AWS ISO certifications, Payment Card Industry (PCI), and System and Organization Control (SOC) reports</li>
<li><code>Artifact Agreements</code> - Allows you to review, accept, and track the status of AWS agreements such as the Business Associate Addendum (BAA) or the Health Insurance Portability and Accountability Act (HIPAA) for an individual account or in your organization -</li>
</ul>
<p>Can be used to <code>support internal audit or compliance</code>.</p>
<hr>
<h2 id="👀-AWS-Service-Catalog"><a href="#👀-AWS-Service-Catalog" class="headerlink" title="👀 AWS Service Catalog"></a>👀 AWS Service Catalog</h2><p>provides a <code>TagOption library</code>. A TagOption is a key-value pair managed in AWS Service Catalog. It is not an AWS tag but serves as a template for creating an AWS tag based on the TagOption.</p>
<p>The TagOption library makes it easier to enforce the following:</p>
<p>– A consistent <code>taxonomy</code>
– Proper tagging of AWS Service Catalog resources
– Defined, user-selectable options for allowed tags</p>
<hr>
<h2 id="AWS-Secrets-Manager"><a href="#AWS-Secrets-Manager" class="headerlink" title="AWS Secrets Manager"></a>AWS Secrets Manager</h2><ul>
<li>Newer service, meant for storing secrets</li>
<li>Capability to <code>force rotation of secrets</code> every X days</li>
<li>Automate generation of secrets on rotation (uses Lambda)</li>
<li><code>Integration with Amazon RDS</code> (MySQL, PostgreSQL, Aurora)</li>
<li>Secrets are encrypted using KMS</li>
<li>Mostly meant for RDS integration</li>
</ul>
<h3 id="AWS-Secrets-Manager-–-Multi-Region-Secrets"><a href="#AWS-Secrets-Manager-–-Multi-Region-Secrets" class="headerlink" title="AWS Secrets Manager – Multi-Region Secrets"></a>AWS Secrets Manager – Multi-Region Secrets</h3><ul>
<li>Replicate Secrets across multiple AWS Regions</li>
<li>Secrets Manager keeps read replicas in sync with the primary Secret</li>
<li>Ability to promote a read replica Secret to a standalone Secret</li>
<li>Use cases: multi-region apps, disaster recovery strategies, multi-region DB…</li>
</ul>
<h3 id="Secrets-Manager-–-Monitoring"><a href="#Secrets-Manager-–-Monitoring" class="headerlink" title="Secrets Manager – Monitoring"></a>Secrets Manager – Monitoring</h3><ul>
<li><p>CloudTrail captures API calls to the Secrets Manager API</p>
</li>
<li><p>CloudTrail captures other related events that might have a security or compliance impact on your AWS account or might help you troubleshoot operational problems.</p>
</li>
<li><p><code>CloudTrail records these events as non-API service events</code>:</p>
<ul>
<li>RotationStarted event</li>
<li>RotationSucceeded event</li>
<li><code>RotationFailed event</code></li>
<li>RotationAbandoned event – a manual change to a secret instead of automated rotation</li>
<li>StartSecretVersionDelete event</li>
<li>CancelSecretVersionDelete event</li>
</ul>
</li>
<li><p>EndSecretVersionDelete event</p>
</li>
<li><p><code>Combine with CloudWatch Logs and CloudWatch alarms for automations</code>.</p>
</li>
</ul>
<h3 id="SSM-Parameter-Store-vs-Secret-Manager"><a href="#SSM-Parameter-Store-vs-Secret-Manager" class="headerlink" title="SSM Parameter Store vs Secret Manager"></a>SSM Parameter Store vs Secret Manager</h3><ul>
<li>Secret Manager ($$$):<ul>
<li>Automatic rotation</li>
<li>Lambda func is provided for RDS, Redshift, DocumentDB</li>
<li>KMS enc is mandatory.</li>
<li>Integration with CloudFormation</li>
</ul>
</li>
<li>SSM Parameter Store ($)<ul>
<li>Simple API</li>
<li>No secret rotation</li>
<li>KMS enc optional</li>
<li>Integration with CloudFormation</li>
<li>Pull screcte from SSM Parater Store</li>
</ul>
</li>
</ul>
<hr>
<h2 id="EBS"><a href="#EBS" class="headerlink" title="EBS"></a>EBS</h2><p><code>*Q</code></p>
<ul>
<li>Use <code>separate</code> Amazon EBS <code>volumes</code> for the <code>operating system</code> and <code>your data</code>, even though root volume persistence feature is available.</li>
<li>EBS snapshots <code>only capture data that has been written to your Amazon EBS volume</code>, which might exclude any data that has been locally cached by your application or operating system.</li>
<li>By default, <code>data</code> on a <code>non-root EBS volume</code> is <code>preserved</code> even if the instance is <code>shutdown or terminated</code>.
  By default, when you attach a non-root EBS volume to an instance, its <code>DeleteOnTermination</code> attribute is <code>set</code> to <code>false</code>. Therefore, the default is to preserve these volumes. After the instance terminates, you can take a snapshot of the preserved volume or attach it to another instance. You must delete a volume to avoid incurring further charges.</li>
</ul>
<h3 id="Q-To-set-up-a-backup-strategy-for-an-Amazon-Elastic-Block-Store-Amazon-EBS-volume-storing-a-custom-database-on-an-Amazon-EC2-instance-the-following-action-should-be-taken"><a href="#Q-To-set-up-a-backup-strategy-for-an-Amazon-Elastic-Block-Store-Amazon-EBS-volume-storing-a-custom-database-on-an-Amazon-EC2-instance-the-following-action-should-be-taken" class="headerlink" title="*Q. To set up a backup strategy for an Amazon Elastic Block Store (Amazon EBS) volume storing a custom database on an Amazon EC2 instance, the following action should be taken:"></a><code>*Q.</code> To set up a backup strategy for an Amazon Elastic Block Store (Amazon EBS) volume storing a custom database on an Amazon EC2 instance, the following action should be taken:</h3><p>Create an Amazon <code>Data Lifecycle Manager (Amazon DLM)</code> policy to <code>take</code> a <code>snapshot of the EBS</code> volume on a <code>recurring schedule</code>.</p>
<p><code>Explanation</code>: Amazon Data Lifecycle Manager (Amazon DLM) allows you to create automated snapshot lifecycle policies for your Amazon EBS volumes. By creating an Amazon DLM policy, you can define the desired backup schedule and retention period for the EBS volume. The policy will then automatically create snapshots according to the defined schedule. This ensures that regular backups are taken and can be used for data recovery if needed.</p>
<h3 id="EBS-volumes-deleted-with-the-TerminateInstances-API-call-continue-to-show-for-some-time-on-AWS-Config-console"><a href="#EBS-volumes-deleted-with-the-TerminateInstances-API-call-continue-to-show-for-some-time-on-AWS-Config-console" class="headerlink" title="EBS volumes deleted with the TerminateInstances API call continue to show for some time on AWS Config console"></a>EBS volumes deleted with the <code>TerminateInstances</code> API call continue to show for some time on AWS Config console</h3><p>Terminated Amazon EC2 instances use the <code>DeleteOnTermination</code> attribute for each attached EBS volume to determine to delete the volume. Amazon EC2 deletes the Amazon EBS volume that has the <code>DeleteOnTermination</code> attribute set to true, but it does not publish the <code>DeleteVolume</code> API call. This is because AWS Config uses the DeleteVolume API call as a trigger with the rule, and the resource changes aren’t recorded for the EBS volume. The EBS volume still shows as compliant or noncompliant.</p>
<p>AWS Config performs a baseline every six hours to check for new configuration items with the ResourceDeleted status. The AWS Config rule then removes the deleted EBS volumes from the evaluation results.</p>
<h3 id="SSD-backed-volumes-IOPS-intensive"><a href="#SSD-backed-volumes-IOPS-intensive" class="headerlink" title="SSD-backed volumes (IOPS-intensive)"></a>SSD-backed volumes (IOPS-intensive)</h3><h3 id="EBS-Volume-Types"><a href="#EBS-Volume-Types" class="headerlink" title="EBS Volume Types"></a>EBS Volume Types</h3><p>EBS Volumes come in 6 types</p>
<ul>
<li><code>gp2 / gp3</code> (SSD): <code>General purpose</code> SSD volume that balances price and performance for a wide variety of workloads.</li>
<li><code>io1 / io2</code> (SSD): <code>Highest-performance</code> SSD volume for <strong>mission-critical low-latency or high-throughput workloads</strong>. <code>Only multi-attach 16 instances at a time</code></li>
<li><code>st1</code> (HDD): <code>Low cost</code> HDD volume designed for <strong>frequently accessed, throughput- intensive workloads</strong></li>
<li><code>sc1</code> (HDD): <code>Lowest cost</code> HDD volume designed for <strong>less frequently accessed workloads</strong></li>
</ul>
<p>EBS Volumes are characterized in Size | Throughput | IOPS (I&#x2F;O Ops Per Sec).</p>
<p><strong><code>Only gp2/gp3 and io1/io2 can be used as boot volumes</code></strong></p>
<h2 id="Provisioned-IOPS-SSD-io2-Block-Express-io2-amp-io1-volumes"><a href="#Provisioned-IOPS-SSD-io2-Block-Express-io2-amp-io1-volumes" class="headerlink" title="Provisioned IOPS SSD (io2 Block Express, io2 &amp; io1) volumes"></a>Provisioned IOPS SSD (io2 Block Express, io2 &amp; <code>io1</code>) volumes</h2><p>Provisioned IOPS SSD volumes are designed to deliver a maximum of 256,000 IOPS, 4,000 MB&#x2F;s of throughput, and 64 TiB in size per volume1. io2 Block Express is the latest generation of the Provisioned IOPS SSD volumes that delivers 4x higher throughput, IOPS, and capacity than regular io2 volumes, along with sub-millisecond latency – at the same price as io2. io2 Block Express provides highest block storage performance for the largest, most I&#x2F;O- intensive, mission-critical deployments of Oracle, Microsoft SQL Server, SAP HANA, and SAS Analytics</p>
<h4 id="General-purpose-SSD-gp3-and-gp2-volumes"><a href="#General-purpose-SSD-gp3-and-gp2-volumes" class="headerlink" title="General purpose SSD (gp3 and gp2) volumes"></a>General purpose SSD (<code>gp3 and gp2</code>) volumes</h4><p>General-purpose volumes are backed by solid-state drives (SSDs) and are suitable for a broad range of transactional workloads, virtual desktops, medium sized single instance databases, latency sensitive interactive applications, dev&#x2F;test environments, and boot volumes.</p>
<h3 id="HDD-backed-volumes-MB-x2F-s-intensive"><a href="#HDD-backed-volumes-MB-x2F-s-intensive" class="headerlink" title="HDD-backed volumes (MB&#x2F;s-intensive)"></a>HDD-backed volumes (MB&#x2F;s-intensive)</h3><h3 id="Throughput-optimized-HDD-st1-volumes"><a href="#Throughput-optimized-HDD-st1-volumes" class="headerlink" title="Throughput optimized HDD (st1) volumes"></a>Throughput optimized HDD (<code>st1</code>) volumes</h3><p>ST1 is backed by hard disk drives (HDDs) and is ideal for frequently accessed, throughput intensive workloads with large datasets and large I&#x2F;O sizes, such as MapReduce, Kafka, log processing, data warehouse, and ETL workloads.</p>
<h3 id="Cold-HDD-sc1-volumes"><a href="#Cold-HDD-sc1-volumes" class="headerlink" title="Cold HDD (sc1) volumes"></a>Cold HDD (<code>sc1</code>) volumes</h3><p>SC1 is backed by hard disk drives (HDDs) and provides the lowest cost per GB of all EBS volume types. It is ideal for less frequently accessed workloads with large, cold datasets.</p>
<h3 id="Changing-the-instance-type"><a href="#Changing-the-instance-type" class="headerlink" title="Changing the instance type"></a>Changing the instance type</h3><ol>
<li>The possibility to resize an instance depends on whether the root device is an EBS volume. If it is, you can easily change the instance size by modifying its instance type, also known as resizing. However, if the root device is an instance store volume, you need to migrate your application to a new instance with the desired instance type.</li>
<li>Before changing the instance type of your Amazon EBS-backed instance, you must stop it. AWS will then move the instance to new hardware, but the instance ID will remain the same.</li>
<li>If your instance belongs to an Auto Scaling group, the Amazon EC2 Auto Scaling service considers the stopped instance as unhealthy and may terminate it, launching a replacement instance instead. To avoid this, you can temporarily suspend the scaling processes for the group while resizing your instance.</li>
</ol>
<p>Reference: <a target="_blank" rel="noopener" href="https://aws.amazon.com/ebs/features/">Amazon EBS features</a></p>
<h2 id="S3"><a href="#S3" class="headerlink" title="S3"></a>S3</h2><h3 id="Q-S3-inventory"><a href="#Q-S3-inventory" class="headerlink" title="*Q S3 inventory"></a><code>*Q</code> <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-inventory.html">S3 inventory</a></h3><p>Is one of the tools Amazon S3 provides to help manage your storage. You can use it <code>to audit and report on the replication and encryption status of your objects</code> for business, compliance, and regulatory needs.
You can also simplify and speed up business workflows and big data jobs using Amazon S3 inventory, which provides a scheduled alternative to the Amazon S3 synchronous List API operation.</p>
<h3 id="Retain-Until-Date"><a href="#Retain-Until-Date" class="headerlink" title="Retain Until Date"></a>Retain Until Date</h3><p>A retention period safeguards an object version for a specified duration. When a retention period is assigned to an object version, Amazon S3 records a timestamp in the object version’s metadata, indicating when the retention period concludes. Once the retention period ends, the object version can be overwritten or deleted, unless a legal hold has also been placed on it.</p>
<p>You can assign a retention period to an object version either explicitly or through a default setting at the bucket level. Explicitly applying a retention period involves specifying a “Retain Until Date” for the object version. Amazon S3 stores this setting in the object version’s metadata and ensures the protection of the object version until the retention period expires.</p>
<h4 id="References"><a href="#References" class="headerlink" title="References"></a>References</h4><ol>
<li><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock.html</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock-overview.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock-overview.html</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html</a></li>
</ol>
<h3 id="S3-RTC-S3-Replication-Time-Control"><a href="#S3-RTC-S3-Replication-Time-Control" class="headerlink" title="S3 RTC S3 (Replication Time Control)"></a>S3 RTC S3 (Replication Time Control)</h3><p>S3 Replication Time Control (S3 RTC) helps you meet compliance or business requirements for data replication and provides visibility into Amazon S3 replication times. S3 RTC replicates most objects that you upload to Amazon S3 in seconds, and 99.99 percent of those objects within 15 minutes.</p>
<p>Reference: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-time-control.html#using-s3-events-to-track-rtc">Using S3 Replication Time Control</a></p>
<h3 id="Amazon-S3-Access-Points"><a href="#Amazon-S3-Access-Points" class="headerlink" title="Amazon S3 Access Points"></a>Amazon S3 Access Points</h3><p>Allow you to <code>create unique entry points for accessing your S3 buckets</code>. Each access point can have its own access policy, allowing you to control access at a granular level. By using access points, you can assign specific permissions to each application or team accessing the shared bucket without affecting other applications. This helps in maintaining access control and minimizing the risk of unintended changes to the bucket policy.</p>
<p>Reference: <a target="_blank" rel="noopener" href="https://aws.amazon.com/s3/features/access-points/">Amazon S3 Access Points</a></p>
<h3 id="Logs"><a href="#Logs" class="headerlink" title="Logs"></a>Logs</h3><h4 id="S3-Server-Access-Logging"><a href="#S3-Server-Access-Logging" class="headerlink" title="S3 Server Access Logging"></a>S3 Server Access Logging</h4><p>To <code>track requests for access to your bucket</code>, you can enable server access logging. Each access log record provides details about a single access request, such as the requester, bucket name, request time, request action, response status, and an error code, if relevant.</p>
<p>There is no extra charge for enabling server access logging on an Amazon S3 bucket, and you are not charged when the logs are PUT to your bucket. However, any log files that the system delivers to your bucket accrue the usual charges for storage. You can delete these log files at any time. Subsequent reads and other requests to these log files are charged normally, as for any other object, including data transfer charges.</p>
<p>By default, logging is disabled. When logging is enabled, logs are saved to a bucket in the same AWS Region as the source bucket.</p>
<p>Reference: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-s3-access-logs-to-identify-requests.html">Using Amazon S3 access logs to identify requests</a></p>
<h3 id="As-a-Website"><a href="#As-a-Website" class="headerlink" title="As a Website"></a>As a Website</h3><p>If you use an Amazon S3 bucket configured as a website endpoint, <code>you must set it up with CloudFront as a custom origin</code>. You can’t use the origin access identity feature. However, you can restrict access to content on a custom origin by setting up custom headers and configuring your origin to require them.</p>
<h3 id="Enable-MFA-Delete"><a href="#Enable-MFA-Delete" class="headerlink" title="Enable MFA-Delete"></a><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html#MultiFactorAuthenticationDelete">Enable MFA-Delete</a></h3><p>You should note that <code>only</code> the bucket owner (<code>root account</code>) <code>can enable MFA Delete</code> only <code>via</code> the AWS <code>CLI</code>. However, the bucket owner, the AWS account that created the bucket (root account), and all authorized IAM users can enable versioning.</p>
<h3 id="Vault-Lock-Policy"><a href="#Vault-Lock-Policy" class="headerlink" title="Vault Lock Policy"></a>Vault Lock Policy</h3><p>S3 Glacier Vault Lock <code>allows</code> you to easily deploy and <code>enforce compliance controls for individual S3 Glacier vaults</code> with a <code>vault lock policy</code>. You <code>can specify controls</code> such as <code>“write once read many” (WORM)</code> in a vault lock policy and lock the policy from future edits. Once locked, the policy can no longer be changed.</p>
<h3 id="Q-Snowball-Edge"><a href="#Q-Snowball-Edge" class="headerlink" title="*Q Snowball Edge"></a><code>*Q</code> Snowball Edge</h3><p>AWS Snowball is a service designed for large-scale data transfers. Snowball Edge appliances are rugged, petabyte-scale data transfer devices that can be used for offline data migration. By using multiple instances of the AWS Snowball client and multiple Snowball Edge Appliances, the company can achieve fast and cost-effective data migration.</p>
<p>Using multiple instances of the AWS Snowball client and Snowball Edge Appliances <code>allows</code> for <code>parallel data transfers</code>, significantly <code>reducing the migration time</code>. It also <code>avoids</code> the need for <code>network-based transfers</code>, which could be slower and potentially costly due to data transfer charges.</p>
<h3 id="Amazon-S3-Storage-Classes"><a href="#Amazon-S3-Storage-Classes" class="headerlink" title="Amazon S3 Storage Classes"></a><a target="_blank" rel="noopener" href="https://aws.amazon.com/s3/storage-classes/">Amazon S3 Storage Classes</a></h3><h4 id="Amazon-S3-Standard-Infrequent-Access-S3-Standard-IA"><a href="#Amazon-S3-Standard-Infrequent-Access-S3-Standard-IA" class="headerlink" title="Amazon S3 Standard-Infrequent Access (S3 Standard-IA)"></a>Amazon S3 Standard-Infrequent Access (S3 Standard-IA)</h4><p>S3 Standard-IA is for data that is accessed less frequently, but requires rapid access when needed. S3 Standard-IA offers the high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval charge.</p>
<h3 id="Q-Access-to-Amazon-S3-via-Direct-Connect"><a href="#Q-Access-to-Amazon-S3-via-Direct-Connect" class="headerlink" title="*Q Access to Amazon S3 via Direct Connect"></a><code>*Q</code> Access to Amazon S3 via Direct Connect</h3><p>It’s not possible to directly access an S3 bucket through a private virtual interface (VIF) using Direct Connect. This is true even if you have an Amazon Virtual Private Cloud (Amazon VPC) endpoint for Amazon S3 in your VPC because <code>VPC endpoint connections can&#39;t extend outside of a VPC</code>. Additionally, Amazon S3 resolves to public IP addresses, even if you enable a VPC endpoint for Amazon S3.</p>
<p>However, you can establish access to Amazon S3 using <code>Direct Connect</code> by following these steps (This configuration doesn’t require a VPC endpoint for Amazon S3, because traffic doesn’t traverse the VPC):</p>
<ol>
<li>Create a <code>connection</code>. You can request a <code>dedicated connection or a hosted connection</code>.</li>
<li>Establish a cross-network connection with the help of your network provider, and then create a public virtual interface for your connection.</li>
<li>Configure an end router for use with the public virtual interface.</li>
</ol>
<p>After the BGP is up and established, the Direct Connect router advertises all global public IP prefixes, including Amazon S3 prefixes. Traffic heading to Amazon S3 is routed through the Direct Connect public virtual interface through a private network connection between AWS and your data center or corporate network.</p>
<h3 id="Amazon-S3-–-Security-👀"><a href="#Amazon-S3-–-Security-👀" class="headerlink" title="Amazon S3 – Security 👀"></a>Amazon S3 – Security 👀</h3><ul>
<li><p><strong><code>User-Based</code></strong></p>
<ul>
<li><strong>IAM Policies</strong> – which API calls should be allowed for a specific user from IAM</li>
</ul>
</li>
<li><p><strong><code>Resource-Based</code></strong></p>
<ul>
<li><strong>Bucket Policies</strong> – bucket wide rules from the S3 console - <code>allows cross account</code></li>
<li><strong>Object Access Control List (ACL)</strong> – finer grain (can be disabled)</li>
<li><strong>Bucket Access Control List (ACL)</strong> – less common (can be disabled)</li>
</ul>
</li>
<li><p><strong><code>Note</code></strong>: an IAM principal can access an S3 object if</p>
<ul>
<li>The user IAM permissions ALLOW it OR the resource policy ALLOWS it</li>
<li>AND there’s no explicit DENY</li>
</ul>
</li>
<li><p><strong><code>Encryption</code></strong>: encrypt objects in Amazon S3 using encryption keys</p>
</li>
</ul>
<h3 id="S3-Bucket-Policies-👀-OJO"><a href="#S3-Bucket-Policies-👀-OJO" class="headerlink" title="S3 Bucket Policies 👀 OJO"></a>S3 Bucket Policies 👀 OJO</h3><ul>
<li><p>Use S3 bucket for policy to:</p>
<ul>
<li>Grant public access to the bucket</li>
<li>Force objects to be encrypted at upload</li>
<li><strong><code>Grant access to another account (Cross Account)</code></strong></li>
</ul>
</li>
<li><p>Optional Conditions on:</p>
<ul>
<li>Public IP or Elastic IP (not on Private IP)</li>
<li>Source VPC or Source VPC Endpoint – only works with VPC Endpoints</li>
<li>CloudFront Origin Identity</li>
<li>MFA</li>
</ul>
</li>
<li><p>Examples here: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html</a></p>
</li>
</ul>
<h3 id="S3-Performance-👀-OJO"><a href="#S3-Performance-👀-OJO" class="headerlink" title="S3 Performance 👀 OJO"></a>S3 Performance 👀 OJO</h3><ul>
<li><p><strong><code>Multi-Part upload</code></strong>:</p>
<ul>
<li>recommended for files &gt; 100MB, must use for files &gt; 5GB</li>
<li>Can help parallelize uploads (speed up transfers)</li>
</ul>
</li>
<li><p><strong><code>S3 Transfer Acceleration</code></strong></p>
<ul>
<li>Increase transfer speed by transferring  file to an AWS edge location which will  forward the data to the S3 bucket in the target region</li>
<li>Compatible with multi-part upload</li>
</ul>
</li>
</ul>
<h3 id="S3-Batch-Operations"><a href="#S3-Batch-Operations" class="headerlink" title="S3 Batch Operations"></a>S3 Batch Operations</h3><p>Eg Encrypt un-encrypted objects.</p>
<p><strong><code>You can use S3 Inventory to get object list and use S3 Select to filter your objects.</code></strong></p>
<h3 id="S3-Inventory-👀-OJO"><a href="#S3-Inventory-👀-OJO" class="headerlink" title="S3 Inventory 👀 OJO"></a>S3 Inventory 👀 OJO</h3><ul>
<li>List objects and their corresponding metadata (alternative to S3 List API operation)</li>
<li>Usage examples:<ul>
<li><code>Audit and report on the replication and encryption status of your objects</code></li>
<li><code>Get the number of objects in an S3 bucket</code></li>
<li><code>Identify the total storage of previous object versions</code></li>
</ul>
</li>
<li><strong>Generate</strong> daily or weekly <strong>reports</strong></li>
<li><strong>Output files</strong>: CSV, ORC, or Apache Parquet</li>
<li><code>You can query all the data using Amazon Athena, Redshift, Presto, Hive, Spark...</code></li>
<li>You can filter generated report using S3 Select</li>
<li>Use cases: <code>Business</code>, <code>Compliance</code>, <code>Regulatory</code> needs, …</li>
</ul>
<h3 id="Amazon-S3-Analytics-–-Storage-Class-Analysis-👀-OJO"><a href="#Amazon-S3-Analytics-–-Storage-Class-Analysis-👀-OJO" class="headerlink" title="Amazon S3 Analytics – Storage Class Analysis 👀 OJO"></a>Amazon S3 Analytics – Storage Class Analysis 👀 OJO</h3><ul>
<li><p>Help you <strong><code>decide when to transition objects to the right storage class</code></strong></p>
</li>
<li><p>Recommendations for <code>Standard</code> and <code>Standard IA</code></p>
</li>
<li><p>Does NOT work for One-Zone IA or Glacier</p>
</li>
<li><p>Report is updated daily</p>
</li>
<li><p>24 to 48 hours to start seeing data analysis</p>
<pre><code>Good first step to put together Lifecycle Rules
</code></pre>
</li>
</ul>
<h3 id="Amazon-S3-Glacier-Vault-Policies-amp-Vault-Lock-👀-OJO"><a href="#Amazon-S3-Glacier-Vault-Policies-amp-Vault-Lock-👀-OJO" class="headerlink" title="Amazon S3 Glacier - Vault Policies &amp; Vault Lock 👀 OJO"></a>Amazon S3 Glacier - Vault Policies &amp; Vault Lock 👀 OJO</h3><ul>
<li>Each Vault has:<ul>
<li>ONE vault access policy</li>
<li>ONE vault lock policy</li>
</ul>
</li>
<li>Vault Policies are written in JSON</li>
<li>Vault Access Policy is like a bucket policy (restrict user &#x2F; account permissions)</li>
<li>Vault Lock Policy is a policy you lock, for regulatory and compliance requirements.<ul>
<li>The policy is immutable, <strong><code>it can never be changed (that’s why it’s call LOCK)</code></strong></li>
<li>Example 1: forbid deleting an archive if less than 1 year old</li>
<li>Example 2: implement WORM policy (write once read many)</li>
</ul>
</li>
</ul>
<h3 id="Glacier-–-Notifications-for-Restore-Operations"><a href="#Glacier-–-Notifications-for-Restore-Operations" class="headerlink" title="Glacier – Notifications for Restore Operations"></a>Glacier – Notifications for Restore Operations</h3><p>S3 Event Notifications</p>
<ul>
<li>S3 supports the restoration of objects archivedto S3 Glacier storage classes</li>
<li><strong><code>s3:ObjectRestore:Post</code></strong> &#x3D;&gt; notify when object restoration initiated</li>
<li><strong><code>s3:ObjectRestore:Completed</code></strong> &#x3D;&gt; notify whenobject restoration completed</li>
</ul>
<h3 id="Amazon-S3-–-Object-Encryption"><a href="#Amazon-S3-–-Object-Encryption" class="headerlink" title="Amazon S3 – Object Encryption"></a>Amazon S3 – Object Encryption</h3><p>You can encrypt objects in S3 buckets using one of 4 methods</p>
<p>Server-Side Encryption (SSE)</p>
<ol>
<li><strong><code>Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)</code></strong> – Enabled by Default<ul>
<li>Encrypts S3 objects using keys handled, managed, and owned by AWS</li>
<li>Must set header <code>&quot;x-amz-server-side-encryption&quot;: &quot;AES256&quot;</code></li>
</ul>
</li>
<li><strong><code>Server-Side Encryption with KMS Keys stored in AWS KMS (SSE-KMS)</code></strong><ul>
<li>Leverage AWS Key Management Service (AWS KMS) to manage encryption keys</li>
<li>ust set header  <code>&quot;x-amz-server-side-encryption&quot;: &quot;aws:kms&quot;</code></li>
</ul>
</li>
<li><strong><code>Server-Side Encryption with Customer-Provided Keys (SSE-C)</code></strong><ul>
<li>When you want to manage your own encryption keys</li>
<li><code>HTTPS must be used</code></li>
</ul>
</li>
<li>Client-Side Encryption</li>
</ol>
<p>Amazon S3 – Force Encryption in Transit
aws:SecureTransport</p>
<h2 id="CORS"><a href="#CORS" class="headerlink" title="CORS"></a>CORS</h2><ul>
<li><strong><code>Cross-Origin Resource Sharing (CORS)</code></strong></li>
<li>Origin &#x3D; scheme (protocol) + host (domain) + port<ul>
<li>example: <a target="_blank" rel="noopener" href="https://www.example.com/">https://www.example.com</a> (implied port is 443 for HTTPS, 80 for HTTP)</li>
</ul>
</li>
<li>Web Browser based mechanism to allow requests to other origins while visiting the main origin</li>
<li>Same origin: <a target="_blank" rel="noopener" href="http://example.com/app1">http://example.com/app1</a> &amp; <a target="_blank" rel="noopener" href="http://example.com/app2">http://example.com/app2</a></li>
<li>Different origins: <a target="_blank" rel="noopener" href="http://www.example.com/">http://www.example.com</a> &amp; <a target="_blank" rel="noopener" href="http://other.example.com/">http://other.example.com</a></li>
<li>The requests won’t be fulfilled unless the other origin allows for the requests, using CORS Headers (example: <code>**Access-Control-Allow-Origin**</code>)</li>
</ul>
<h3 id="👀-OJO-Exam-question"><a href="#👀-OJO-Exam-question" class="headerlink" title="👀 OJO Exam question"></a>👀 OJO Exam question</h3><ul>
<li>If a client makes a cross-origin request on our S3 bucket, we need to enable the correct CORS headers.
You can allow for a specific origin or for * (all origins)</li>
</ul>
<h3 id="Amazon-S3-–-MFA-Delete"><a href="#Amazon-S3-–-MFA-Delete" class="headerlink" title="Amazon S3 – MFA Delete"></a>Amazon S3 – MFA Delete</h3><ul>
<li><p><strong>MFA (Multi-Factor Authentication)</strong> – force users to generate a code on a
device (usually a mobile phone or hardware) before doing important operations on S3</p>
</li>
<li><p>MFA will be required to:</p>
<ul>
<li>Permanently delete an object version  Google Authenticator</li>
<li>Suspend Versioning on the bucket</li>
</ul>
</li>
<li><p>MFA won’t be required to:</p>
<ul>
<li>Enable Versioning</li>
<li>List deleted versions</li>
</ul>
</li>
<li><p>To use MFA Delete, <strong><code>Versioning must be enabled</code></strong> on the bucket</p>
</li>
<li><p><strong><code>Only the bucket owner (root account) can enable/disable MFA Delete</code></strong></p>
</li>
</ul>
<h3 id="S3-Access-Logs"><a href="#S3-Access-Logs" class="headerlink" title="S3 Access Logs"></a>S3 Access Logs</h3><ul>
<li>For audit purpose, you may want to log all access to S3 buckets</li>
<li>Any request made to S3, from any account, authorized or denied, will be logged into another S3 bucket</li>
<li>That data can be analyzed using data analysis tools…</li>
<li>The target logging bucket must be in the same AWS region</li>
</ul>
<h2 id="Amazon-Athena"><a href="#Amazon-Athena" class="headerlink" title="Amazon Athena"></a>Amazon Athena</h2><ul>
<li><p>Serverless query service to analyze data stored in Amazon S3</p>
</li>
<li><p>Uses standard SQL language to query the files (built on Presto)</p>
</li>
<li><p>Supports CSV, JSON, ORC, Avro, and Parquet</p>
</li>
<li><p>Pricing: $5.00 per TB of data scanned</p>
</li>
<li><p>Commonly used with Amazon Quicksight for reporting&#x2F;dashboards</p>
</li>
<li><p><strong><code>Use cases</code></strong>: Business intelligence &#x2F; analytics &#x2F; reporting, analyze &amp; query VPC Flow Logs, ELB Logs, CloudTrail trails, etc…</p>
</li>
<li><p><strong><code>Exam Tip</code></strong>: analyze data in S3 using serverless SQL, use Athena</p>
</li>
</ul>
<h3 id="Amazon-Athena-–-Performance-Improvement"><a href="#Amazon-Athena-–-Performance-Improvement" class="headerlink" title="Amazon Athena – Performance Improvement"></a>Amazon Athena – Performance Improvement</h3><ul>
<li><strong><code>Use columnar</code></strong> data for cost-savings (less scan).</li>
<li><strong><code>Compress data</code></strong> for smaller retrievals (bzip2, gzip, lz4, snappy, zlip, zstd…).</li>
<li><strong><code>Partition</code></strong> datasets in S3 for easy querying on virtual columns.</li>
<li><strong><code>Use larger files</code></strong> (&gt; 128 MB) to minimize overhead.</li>
</ul>
<h3 id="Amazon-Athena-–-Federated-Query"><a href="#Amazon-Athena-–-Federated-Query" class="headerlink" title="Amazon Athena – Federated Query"></a>Amazon Athena – Federated Query</h3><p>Allows you to run SQL queries across data stored in relational, non-relational, object, and custom data sources (AWS or on-premises)</p>
<p>Uses Data Source Connectors that run on AWS Lambda to run Federated Queries (e.g., CloudWatch Logs, DynamoDB, RDS, …)</p>
<p>Store the results back in Amazon S3</p>
<h2 id="AWS-OpsHub"><a href="#AWS-OpsHub" class="headerlink" title="AWS OpsHub"></a><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/snowball/latest/developer-guide/aws-opshub.html">AWS OpsHub</a></h2><p>AWS OpsHub for <code>Snow Family</code>, that you can use to <code>manage your devices and local AWS services</code>. You use AWS OpsHub on a client computer to perform tasks such as unlocking and configuring single or clustered devices, transferring files, and launching and managing instances running on Snow Family Devices. You can use AWS OpsHub to manage both the Storage Optimized and Compute Optimized device types and the Snow device. The AWS OpsHub application is available at no additional cost to you.</p>
<p>AWS OpsHub takes all the existing operations available in the Snowball API and presents them as a graphical user interface. This interface helps you quickly migrate data to the AWS Cloud and deploy edge computing applications on Snow Family Devices.</p>
<p>When your Snow device arrives at your site, you download, install, and launch the AWS OpsHub application on a client machine, such as a laptop. After installation, you can unlock the device and start managing it and using supported AWS services locally. AWS OpsHub provides a dashboard that summarizes key metrics such as storage capacity and active instances on your device. It also provides a selection of AWS services that are supported on the Snow Family Devices. Within minutes, you can begin transferring files to the device.</p>
<hr>
<h2 id="Amazon-FSx-–-Overview"><a href="#Amazon-FSx-–-Overview" class="headerlink" title="Amazon FSx – Overview"></a>Amazon FSx – Overview</h2><p><strong><code>Launch 3rd party high-performance file systems on AWS</code></strong></p>
<h3 id="FSx-for-Lustre"><a href="#FSx-for-Lustre" class="headerlink" title="FSx for Lustre"></a>FSx for Lustre</h3><p>Lustre is a type of <code>parallel distributed</code> file system, for large-scale computing. The name Lustre is derived from “Linux” and “cluster.</p>
<ul>
<li><strong><code>Machine Learning, High Performance Computing (HPC)</code></strong></li>
<li>Video Processing, Financial Modeling, Electronic Design Automation</li>
<li><strong><code>Seamless integration with S3</code></strong><ul>
<li>Can “read S3” as a file system (through FSx)</li>
<li>Can write the output of the computations back to S3 (through FSx)</li>
</ul>
</li>
<li><strong><code>Can be used from on-premises servers (VPN or Direct Connect)</code></strong></li>
</ul>
<h4 id="FSx-Lustre-File-System-Deployment-Options"><a href="#FSx-Lustre-File-System-Deployment-Options" class="headerlink" title="FSx Lustre - File System Deployment Options"></a>FSx Lustre - File System Deployment Options</h4><ol>
<li><strong><code>Scratch File System</code></strong>: Temporary storage</li>
<li><strong><code>Persistent File System</code></strong></li>
</ol>
<ul>
<li>Long-term storage</li>
<li>Data is replicated within same AZ</li>
</ul>
<h3 id="FSx-for-Windows-File-Server"><a href="#FSx-for-Windows-File-Server" class="headerlink" title="FSx for Windows File Server"></a>FSx for Windows File Server</h3><ul>
<li><strong><code>FSx for Windows</code></strong> is a fully managed Windows file system share drive</li>
<li>Supports <strong>SMB protocol &amp; Windows NTFS</strong></li>
<li>Microsoft Active Directory integration, ACLs, user quotas</li>
<li><strong><code>Can be mounted on Linux EC2 instances</code></strong></li>
<li>Supports <strong><code>Microsoft&#39;s Distributed File System (DFS)</code></strong> Namespaces (group files across multiple FS)</li>
</ul>
<h3 id="FSx-for-NetAppONTAP"><a href="#FSx-for-NetAppONTAP" class="headerlink" title="FSx for NetAppONTAP"></a>FSx for NetAppONTAP</h3><ul>
<li>Managed NetApp ONTAP on AWS</li>
<li>File System compatible with <strong><code>NFS, SMB, iSCSI protocols</code></strong></li>
<li><strong><code>Point-in-time instantaneous cloning (helpful for testingnew workloads)</code></strong></li>
</ul>
<h3 id="FSx-for-OpenZFS"><a href="#FSx-for-OpenZFS" class="headerlink" title="FSx for OpenZFS"></a>FSx for OpenZFS</h3><ul>
<li>Managed OpenZFS file system on AWS</li>
<li>File System compatible with <strong><code>NFS</code></strong> (v3, v4, v4.1, v4.2)</li>
<li><strong><code>Point-in-time instantaneous cloning (helpful for testing new workloads)</code></strong></li>
</ul>
<hr>
<h2 id="AWS-Storage-Gateway"><a href="#AWS-Storage-Gateway" class="headerlink" title="AWS Storage Gateway"></a>AWS Storage Gateway</h2><p>Bridge between on-premises data and cloud data</p>
<ul>
<li><p><code>File Gateway is POSIX compliant (Linux file system)</code></p>
<ul>
<li>POSIX metadata ownership, permissions, and timestamps stored in the object’s metadata in S3</li>
</ul>
</li>
<li><p>Reboot Storage Gateway VM: (e.g., maintenance)</p>
<ul>
<li><code>File Gateway</code>: simply restart the Storage Gateway VM</li>
<li><code>Volume and Tape Gateway</code>:<ul>
<li>Stop Storage Gateway Service (AWS Console, VM local Console, Storage Gateway API)</li>
<li>Reboot the Storage Gateway VM</li>
<li>Start Storage Gateway Service (AWS Console, VM local Console, Storage Gateway API)</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Types of Storage Gateway:</p>
<ol>
<li><code>S3 File Gateway</code></li>
</ol>
<ul>
<li>Configured S3 buckets are accessible using the <strong><code>NFS and SMB protocol</code></strong></li>
<li><strong><code>Most recently used data is cached in the file gateway</code></strong></li>
<li>Supports S3 Standard, S3 Standard IA, S3 One Zone A, S3 Intelligent Tiering</li>
<li>Transition to S3 Glacier using a Lifecycle Policy</li>
</ul>
<ol start="2">
<li><code>FSx File Gateway</code></li>
</ol>
<ul>
<li>Native access to Amazon FSx for Windows File Server</li>
<li><strong><code>Local cache for frequently accessed data</code></strong></li>
<li><strong><code>Windows native compatibility (SMB, NTFS, Active Directory...)</code></strong></li>
<li>Useful for group file shares and home directories</li>
</ul>
<ol start="3">
<li><code>Volume Gateway</code></li>
</ol>
<ul>
<li>Block storage using <strong><code>iSCSI protocol backed by S3</code></strong></li>
<li><code>Backed by EBS snapshots</code> which can help restore on-premises volumes!</li>
<li><code>Cached volumes</code>: low latency access to most recent data</li>
<li><code>Stored volumes</code>: entire dataset is on premise, scheduled backups to S3</li>
</ul>
<ol start="4">
<li><code>Tape Gateway</code></li>
</ol>
<ul>
<li>Some companies have backup processes using physical tapes (!)</li>
<li>With Tape Gateway, companies use the same processes but, in the cloud</li>
<li><code>Virtual Tape Library (VTL)</code> backed by Amazon S3 and Glacier</li>
<li>Back up data using existing tape-based processes (and iSCSI interface)</li>
<li>Works with leading backup software vendors</li>
</ul>
<h3 id="Storage-Gateway-–-Activations"><a href="#Storage-Gateway-–-Activations" class="headerlink" title="Storage Gateway – Activations"></a>Storage Gateway – Activations</h3><p>Two ways to get Activation Key:</p>
<ul>
<li>Using the Gateway VM CLI</li>
<li>Make a web request to the Gateway VM (Port 80) old way</li>
</ul>
<p><code>Troubleshooting Activation Failures</code> - 👀 exam</p>
<ul>
<li>Make sure the Gateway VM has <code>port 80 opened</code></li>
<li>Check that the Gateway VM has the correct time and synchronizing its time automatically to a <code>Network Time Protocol (NTP)</code> server</li>
</ul>
<hr>
<h2 id="Amazon-CloudFront"><a href="#Amazon-CloudFront" class="headerlink" title="Amazon CloudFront"></a>Amazon CloudFront</h2><p>Content Delivery Network (CDN)</p>
<ul>
<li><strong><code>Improves read performance, content is cached at the edge</code></strong></li>
<li>Improves users experience</li>
<li>216 Point of Presence globally (edge locations)</li>
<li><strong><code>DDoS protection (because worldwide), integration with Shield, AWS Web Application Firewall</code></strong></li>
</ul>
<h3 id="CloudFront-–-Origins"><a href="#CloudFront-–-Origins" class="headerlink" title="CloudFront – Origins"></a>CloudFront – Origins</h3><p><code>S3 bucket</code></p>
<ul>
<li>For distributing files and caching them at the edge</li>
<li>Enhanced security with CloudFront <strong><code>Origin Access Control (OAC)</code></strong></li>
<li>OAC is replacing Origin Access Identity (OAI)</li>
<li>CloudFront can be used as an ingress (to upload files to S3)</li>
</ul>
<p> <code>Custom Origin (HTTP)</code></p>
<ul>
<li>Application Load Balancer</li>
<li>EC2 instance</li>
<li>S3 website (must first enable the bucket as a static S3 website)</li>
<li>Any HTTP backend you want</li>
</ul>
<h2 id="Q-AwS-Origin-Shield"><a href="#Q-AwS-Origin-Shield" class="headerlink" title="*Q AwS Origin Shield"></a><code>*Q</code> AwS Origin Shield</h2><p>Enabling the Origin Shield <code>feature</code> in <code>CloudFront</code> helps reduce the load on the origin server by <code>adding</code> an additional <code>caching layer</code> <em>between</em> <code>CloudFront edge locations</code> and &#96;the origin. It improves cache hit ratios and reduces the number of requests hitting the origin by serving content from the Origin Shield cache.</p>
<h2 id="AWS-WAF-Web-Application-Firewall"><a href="#AWS-WAF-Web-Application-Firewall" class="headerlink" title="AWS WAF (Web Application Firewall)"></a>AWS WAF (Web Application Firewall)</h2><p>AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to an Amazon <code>CloudFront</code> distribution, an Amazon <code>API Gateway</code> REST API, an <code>Application Load Balancer</code>, or an <code>AWS AppSync GraphQL API</code>.</p>
<h3 id="Q-Change-AWS-Firewall-Manager-administration-account"><a href="#Q-Change-AWS-Firewall-Manager-administration-account" class="headerlink" title="*Q Change AWS Firewall Manager administration account"></a><code>*Q</code> <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/waf/latest/developerguide/fms-change-administrator.html">Change AWS Firewall Manager administration account</a></h3><p>You can designate <code>only one account in</code> an <code>organization as a Firewall Manager administrator account</code>. To create a new Firewall Manager administrator account, you must revoke the original administrator account first.</p>
<h3 id="CloudFront-Origin-Headers-vs-Cache-Behavior"><a href="#CloudFront-Origin-Headers-vs-Cache-Behavior" class="headerlink" title="CloudFront Origin Headers vs Cache Behavior"></a>CloudFront Origin Headers vs Cache Behavior</h3><p><code>Origin Custom Headers</code>:</p>
<ul>
<li>Origin-level setting</li>
<li>Set a constant header &#x2F; header value for all requests to origin</li>
</ul>
<p><code>Behavior setting</code>:</p>
<ul>
<li>Cache-related settings</li>
<li>Contains the whitelist of headers to forward</li>
</ul>
<h3 id="CloudFront-Caching-TTL"><a href="#CloudFront-Caching-TTL" class="headerlink" title="CloudFront Caching TTL"></a>CloudFront Caching TTL</h3><p><code>**“Cache-Control: max-age”**</code> is preferred to “Expires” header</p>
<h3 id="CloudFront-–-Increasing-Cache-Ratio"><a href="#CloudFront-–-Increasing-Cache-Ratio" class="headerlink" title="CloudFront – Increasing Cache Ratio"></a>CloudFront – Increasing Cache Ratio</h3><p>Monitor the CloudWatch metric <code>CacheHitRate</code></p>
<ul>
<li>Specify how long to cache your objects: <code>Cache-Control max-age</code> header</li>
<li>Specify none or the minimally required <code>headers</code></li>
<li>Specify none or the minimally required <code>cookies</code></li>
<li>Specify none or the minimally required <code>query string parameters</code></li>
<li>Separate static and dynamic distributions (two origins)</li>
</ul>
<h2 id="CloudFront-with-ALB-sticky-sessions"><a href="#CloudFront-with-ALB-sticky-sessions" class="headerlink" title="CloudFront with ALB sticky sessions"></a>CloudFront with ALB sticky sessions</h2><ul>
<li>Must forward &#x2F; whitelist the cookie that controls the session affinity to the origin to allow the session affinity to work</li>
<li>Set a TTL to a value lesser than when the authentication cookie expire</li>
</ul>
<p><code>Cookie: AWSALB=...</code></p>
<hr>
<h3 id="AWS-Health-Dashboard-Service-History"><a href="#AWS-Health-Dashboard-Service-History" class="headerlink" title="AWS Health Dashboard - Service History"></a>AWS Health Dashboard - Service History</h3><ul>
<li><p>Shows all regions, all services health</p>
</li>
<li><p>Shows historical information for each day</p>
</li>
<li><p>Has an RSS feed you can subscribe to</p>
</li>
<li><p>Previously called AWS Service Health Dashboard</p>
</li>
</ul>
<h3 id="AWS-Health-Dashboard-–-Your-Account"><a href="#AWS-Health-Dashboard-–-Your-Account" class="headerlink" title="AWS Health Dashboard – Your Account"></a>AWS Health Dashboard – Your Account</h3><ul>
<li><p>Previously called <code>AWS Personal Health Dashboard</code> (PHD).</p>
</li>
<li><p>AWS Account Health Dashboard provides <code>alerts and remediation guidance</code> when AWS is experiencing <code>events that may impact you</code>.</p>
</li>
<li><p>While the Service Health Dashboard displays the general status of AWS services, Account Health Dashboard gives you a <code>personalized view into the performance and availability of the AWS services underlying your AWS resources</code>.</p>
</li>
<li><p>The dashboard displays <code>relevant and timely information</code> to help you manage events in progress and <code>provides proactive</code> notification to help
you plan for <code>scheduled activities</code>.</p>
</li>
<li><p><code>Can aggregate data from an entire AWS Organization</code></p>
</li>
<li><p>Global service</p>
</li>
<li><p>Shows how AWS outages directly impact you &amp; your AWS resources</p>
</li>
<li><p>Alert, remediation, proactive, scheduled activitie</p>
</li>
</ul>
<h3 id="Health-Event-Notifications"><a href="#Health-Event-Notifications" class="headerlink" title="Health Event Notifications"></a>Health Event Notifications</h3><ul>
<li>Use <code>EventBridge to react to changes for AWS Health events</code> in your AWS account</li>
<li>Example: receive email notifications when EC2 instances in your AWS account are scheduled for updates</li>
<li>This is possible for Account events (resources that are affected in your account) and Public Events (Regional availability of a service)</li>
<li>Use cases: send notifications, capture event information, take corrective action…</li>
</ul>
<h2 id="👀-Q-AWS-Personal-Health-Dashboard"><a href="#👀-Q-AWS-Personal-Health-Dashboard" class="headerlink" title="👀 *Q AWS Personal Health Dashboard"></a>👀 <code>*Q</code> AWS Personal Health Dashboard</h2><p>AWS Personal Health Dashboard provides <code>alerts</code> and <em><code>remediation guidance</code></em> when AWS is experiencing <code>events that may impact you</code>. While the Service Health Dashboard displays the general status of AWS services, Personal Health Dashboard gives you a personalized view into the <em>performance and availability</em> of the AWS services underlying your AWS resources.</p>
<p>What’s more, Personal Health Dashboard proactively notifies you when AWS experiences any events that may affect you, helping provide quick visibility and guidance to help you minimize the impact of events in progress, and plan for any scheduled changes, such as AWS hardware maintenance.AWS Inspector</p>
<p>The <code>AWS Health API provides</code> programmatic <code>access</code> to the <code>AWS Health information</code> that appears in the AWS Personal Health Dashboard. You can use the API operations to get information about events that might affect your AWS services and resources.</p>
<hr>
<h2 id="AWS-Organizations"><a href="#AWS-Organizations" class="headerlink" title="AWS Organizations"></a>AWS Organizations</h2><p>If you have created an organization in AWS Organizations, you <code>can</code> also <code>create a trail</code> that will <code>log all events for all AWS accounts in that organization</code>. This is referred to as an organization trail.</p>
<p>Offers <code>policy-based</code> management for multiple AWS accounts. With Organizations, you can create groups of accounts, automate account creation, and apply and manage policies for those groups. Organizations enable you to centrally manage policies across multiple accounts without requiring custom scripts and manual processes. It allows you to create Service Control Policies (SCPs) that centrally control AWS service use across multiple AWS accounts.</p>
<ul>
<li>Global service.</li>
<li>Allows to manage multiple AWS accounts.</li>
<li>The main account is the <code>management account</code>.</li>
<li>Other accounts are <code>member accounts</code>.</li>
<li>Member accounts can only be part of one organization.</li>
<li>Consolidated Billing across all accounts - single payment method.</li>
<li>Pricing benefits from <code>aggregated usage</code> (volume discount for EC2, S3…).</li>
<li><code>Shared reserved instances and Savings Plans discounts across accounts</code>.</li>
<li>API is available to automate AWS account creation.</li>
</ul>
<h3 id="Advantages"><a href="#Advantages" class="headerlink" title="Advantages"></a>Advantages</h3><ul>
<li>Multi Account vs One Account Multi VPC.</li>
<li>Use tagging standards for billing purposes.</li>
<li>Enable CloudTrail on all accounts, send logs to central S3 account.</li>
<li>Send CloudWatch Logs to central logging account.</li>
<li>Establish Cross Account Roles for Admin purposes.</li>
</ul>
<h3 id="Security-Service-Control-Policies-SCP"><a href="#Security-Service-Control-Policies-SCP" class="headerlink" title="Security: Service Control Policies (SCP)"></a>Security: Service Control Policies (SCP)</h3><ul>
<li>IAM policies applied to OU or Accounts to restrict Users and Roles.</li>
<li>They do not apply to the management account (full admin power).</li>
<li>Must have an explicit allow (does not allow anything by default – like IAM).</li>
</ul>
<h3 id="AWS-Organizations-–-Reserved-Instances"><a href="#AWS-Organizations-–-Reserved-Instances" class="headerlink" title="AWS Organizations – Reserved Instances"></a>AWS Organizations – Reserved Instances</h3><ul>
<li>For billing purposes, the consolidated billing feature of AWS Organizations treats all the accounts in the organization as one account.</li>
<li>This means that <code>all accounts</code> in the organization can receive the hourly cost benefit of Reserved Instances that are purchased <code>by any other account</code>.</li>
<li><code>The payer account (master account) of an organization</code> can turn off Reserved Instance (RI) discount and Savings Plans discount sharing for any accounts in that organization, including the payer account</li>
<li>This means that RIs and Savings Plans discounts aren’t shared between any accounts that have sharing turned off.</li>
<li>To share an RI or Savings Plans discount with an account, <code>both accounts must have sharing turned on</code>.</li>
</ul>
<h3 id="AWS-Organizations-–-IAM-Policies"><a href="#AWS-Organizations-–-IAM-Policies" class="headerlink" title="AWS Organizations – IAM Policies"></a>AWS Organizations – IAM Policies</h3><p>Use <code>aws:PrincipalOrgID</code> condition key in your resource-based policies to restrict access to IAM principals from accounts in an AWS Organizat</p>
<h2 id="AWS-Organizations-–-Tag-Policies"><a href="#AWS-Organizations-–-Tag-Policies" class="headerlink" title="AWS Organizations – Tag Policies"></a>AWS Organizations – Tag Policies</h2><ul>
<li>Helps you standardize tags across resources in an AWS Organization</li>
<li>Ensure consistent tags, audit tagged resources, maintain proper resources categorization, …</li>
<li>You define tag keys and their allowed values</li>
<li>Helps with AWS Cost Allocation Tags and Attribute-based Access Control</li>
<li>Prevent any non-compliant tagging operations on specified services and resources (has no effect on resources without tags)</li>
<li>Generate a report that lists all tagged&#x2F;non-compliant resources</li>
<li>Use CloudWatch Events to monitor non-compliant tags</li>
</ul>
<hr>
<h2 id="AWS-Control-Tower"><a href="#AWS-Control-Tower" class="headerlink" title="AWS Control Tower"></a>AWS Control Tower</h2><p>Offers the easiest way to <code>set up and govern a secure, multi-account AWS environment</code>. It establishes a landing zone that is <code>based on the best-practices</code> blueprints and enables governance using guardrails you can choose from a pre-packaged list. The landing zone is a well-architected, multi-account baseline that follows AWS best practices. Guardrails implement governance rules for security, compliance, and operations.</p>
<ul>
<li>Benefits:<ul>
<li>Automate the set up of your environment in a few clicks</li>
<li>Automate ongoing policy management using guardrails</li>
<li>Detect policy violations and remediate them</li>
<li>Monitor compliance through an interactive dashboard</li>
</ul>
</li>
<li><code>AWS Control Tower runs on top of AWS Organizations</code>:<ul>
<li>It automatically sets up AWS Organizations to organize accounts and implement SCPs (Service Control Policies)</li>
</ul>
</li>
</ul>
<hr>
<h2 id="AWS-Service-Catalog"><a href="#AWS-Service-Catalog" class="headerlink" title="AWS Service Catalog"></a>AWS Service Catalog</h2><p>AWS Service Catalog allows <code>organizations to create and manage catalogs of IT services that are approved for use on AWS</code>. These IT services can include everything from virtual machine images, servers, software, and databases to complete multi-tier application architectures. AWS Service Catalog allows you to centrally manage deployed IT services and your applications, resources, and metadata. This helps you achieve <code>consistent governance</code> and <code>meet your compliance requirements</code> while enabling users to quickly deploy only the approved IT services they need.</p>
<h4 id="Sharing-and-Importing-Portfolios"><a href="#Sharing-and-Importing-Portfolios" class="headerlink" title="Sharing and Importing Portfolios"></a><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/servicecatalog/latest/adminguide/catalogs_portfolios_sharing.html">Sharing and Importing Portfolios</a></h4><p>To make your Service Catalog products available to users who are not in your AWS accounts, such as users who belong to other organizations or to other AWS accounts in your organization, you share your portfolios with them. You can share in several ways, including <code>account-to-account</code> sharing, <code>organizational</code> sharing, and <code>deploying catalogs using stack sets</code>.</p>
<p>Before you share your products and portfolios to other accounts, you must decide whether you want to share a reference of the catalog or to deploy a copy of the catalog into each recipient account. Note that if you deploy a copy, you must redeploy if there are updates you want to propagate to the recipient accounts.</p>
<ul>
<li><p>Users that are new to AWS have too many options, and may create stacks that are not compliant &#x2F; in line with the rest of the organization</p>
</li>
<li><p>Some users just want a quick <code>self-service portal</code> to launch a set of <code>authorized products</code> pre-defined <code>by admins</code></p>
</li>
<li><p>Includes: virtual machines, databases, storage options, etc…</p>
</li>
</ul>
<h3 id="AWS-Service-Catalog-–-Sharing-Catalogs"><a href="#AWS-Service-Catalog-–-Sharing-Catalogs" class="headerlink" title="AWS Service Catalog – Sharing Catalogs"></a>AWS Service Catalog – Sharing Catalogs</h3><p>Share portfolios with individual AWS accounts or AWS Organizations.</p>
<h3 id="AWS-Service-Catalog-–-TagOptions-Library"><a href="#AWS-Service-Catalog-–-TagOptions-Library" class="headerlink" title="AWS Service Catalog – TagOptions Library"></a>AWS Service Catalog – TagOptions Library</h3><ul>
<li>Easily manage tags on provisioned products</li>
<li><code>TagOption</code>:<ul>
<li>Key-value pair managed in AWS Service Catalog</li>
<li>Used to create an AWS Tag</li>
</ul>
</li>
<li>Can be associated with Portfolios and Products.</li>
<li>Use cases: proper resources tagging, defined allowed tags, …</li>
<li>Can be shared with other AWS accounts and AWS Organizations.</li>
</ul>
<hr>
<h2 id="Cost-Explorer"><a href="#Cost-Explorer" class="headerlink" title="Cost Explorer"></a>Cost Explorer</h2><ul>
<li>Visualize, understand, and manage your AWS costs and usage over time</li>
<li>Create custom reports that analyze cost and usage data.</li>
<li>Analyze your data at a high level: total costs and usage across all accounts</li>
<li>Or Monthly, hourly, resource level granularity</li>
<li>Choose an optimal <code>Savings Plan</code> (to lower prices on your bill)</li>
<li><code>Forecast usage up to 12 months based on previous usage</code></li>
</ul>
<h2 id="👀-Q-Collect-information-about-the-service-costs-of-each-developer"><a href="#👀-Q-Collect-information-about-the-service-costs-of-each-developer" class="headerlink" title="👀 *Q Collect information about the service costs of each developer"></a><code>👀</code> <code>*Q</code> Collect information about the service costs of each developer</h2><ul>
<li>The AWS-generated <em>tag</em> <code>createdBy</code> defines and applies to supported AWS resources for cost allocation purposes. To use the AWS-generated tags, a management account owner must activate it in the Billing and Cost Management console. When a management account owner activates the tag, it is also activated for all member accounts.</li>
<li><code>Cost Explorer</code> is a tool that enables you to view and <code>analyze your costs and usage</code>. You can explore your usage and costs using the main graph, the Cost Explorer cost and usage reports, or the Cost Explorer RI reports.</li>
</ul>
<p>AWS Cost Explorer provides the following prebuilt reports:</p>
<p>– EC2 <code>RI Utilization %</code> offers relevant data to identify and act on opportunities to increase your Reserved Instance usage efficiency. It’s calculated by dividing Reserved Instance used hours by total Reserved Instance purchased hours.</p>
<p>– EC2 <code>RI Coverage %</code> shows how much of your overall instance usage is covered by Reserved Instances. This lets you make informed decisions about when to purchase or modify a Reserved Instance to ensure</p>
<hr>
<h2 id="AWS-Budgets"><a href="#AWS-Budgets" class="headerlink" title="AWS Budgets"></a>AWS Budgets</h2><ul>
<li><code>Create budget and send alarms when costs exceeds the budget</code>.</li>
<li>4 types of budgets: <code>Usage, Cost, Reservation, Savings Plans</code>.</li>
<li>For Reserved Instances (RI)<ul>
<li>Track utilization</li>
<li>Supports <code>EC2, ElastiCache, RDS, Redshift</code></li>
</ul>
</li>
<li>Up to 5 SNS notifications per budget</li>
<li>Can filter by: Service, Linked Account, Tag, Purchase Option, Instance Type, Region, Availability Zone, API Operation, etc…</li>
<li><code>Same options as AWS Cost Explorer!</code></li>
<li>2 budgets are free, then $0.02&#x2F;day&#x2F;budget</li>
</ul>
<h3 id="Cost-Allocation-Tags"><a href="#Cost-Allocation-Tags" class="headerlink" title="Cost Allocation Tags"></a>Cost Allocation Tags</h3><ul>
<li>Use <code>cost allocation tags</code> to track your AWS costs on a detailed level</li>
<li><code>AWS generated tags</code><ul>
<li>Automatically applied to the resource you create</li>
<li>Starts with Prefix <code>aws: (e.g. aws: createdBy)</code></li>
</ul>
</li>
<li><code>User-defined tags</code><ul>
<li>Defined by the user</li>
<li>Starts with Prefix <code>user:</code></li>
</ul>
</li>
</ul>
<h3 id="Cost-and-Usage-Reports"><a href="#Cost-and-Usage-Reports" class="headerlink" title="Cost and Usage Reports"></a>Cost and Usage Reports</h3><ul>
<li>Dive deeper into your AWS costs and usage</li>
<li>The AWS Cost &amp; Usage Report contains <code>the most comprehensive set of AWS cost and usage data available</code></li>
<li>Includes additional metadata about AWS services, pricing, and reservations (<code>e.g., Amazon EC2 Reserved Instances (RIs)</code>)</li>
<li>The AWS Cost &amp; Usage Report lists AWS usage for each:<ul>
<li>service category used by an account</li>
<li>in hourly or daily line items</li>
<li>any tags that you have activated for cost allocation purposes</li>
</ul>
</li>
<li>Can be configured for daily exports to S3</li>
<li>Can be integrated with Athena, Redshift or QuickSight</li>
</ul>
<h3 id="AWS-Compute-Optimizer"><a href="#AWS-Compute-Optimizer" class="headerlink" title="AWS Compute Optimizer"></a>AWS Compute Optimizer</h3><ul>
<li><code>Reduce costs and improve performance by</code> recommending optimal AWS resources for your workloads</li>
<li>Helps you choose optimal configurations and right-size your workloads (over&#x2F;under provisioned)</li>
<li>Uses Machine Learning to analyze your <code>resources configurations and their utilization CloudWatch metrics</code></li>
<li>Supported resources<ul>
<li>EC2 instances</li>
<li>EC2 Auto Scaling Groups</li>
<li>EBS volumes</li>
<li>Lambda functions</li>
</ul>
</li>
<li>Lower your costs by up to 25%</li>
<li>Recommendations can be exported to S3</li>
</ul>
<hr>
<h2 id="IAM"><a href="#IAM" class="headerlink" title="IAM"></a>IAM</h2><h3 id="If-an-IAM-user-with-full-access-to-IAM-and-Amazon-S3-assigns-a-bucket-policy-to-an-Amazon-S3-bucket-and-doesn’t-specify-the-AWS-account-root-user-as-a-principal-the-root-user-is-denied-access-to-that-bucket"><a href="#If-an-IAM-user-with-full-access-to-IAM-and-Amazon-S3-assigns-a-bucket-policy-to-an-Amazon-S3-bucket-and-doesn’t-specify-the-AWS-account-root-user-as-a-principal-the-root-user-is-denied-access-to-that-bucket" class="headerlink" title="If an IAM user, with full access to IAM and Amazon S3, assigns a bucket policy to an Amazon S3 bucket and doesn’t specify the AWS account root user as a principal, the root user is denied access to that bucket."></a>If an IAM user, with full access to IAM and Amazon S3, assigns a bucket policy to an Amazon S3 bucket and doesn’t specify the AWS account root user as a principal, the root user is denied access to that bucket.</h3><p>To fix this issue, the CTO needs to ensure that an IAM user with full access to both IAM and Amazon S3 explicitly includes the AWS account root user as a principal in the bucket policy of the S3 bucket. By adding the root user as a principal, access will be granted to the CTO and they will be able to access the S3 bucket in their AWS account.</p>
<h3 id="Reference-1"><a href="#Reference-1" class="headerlink" title="Reference:"></a>Reference:</h3><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_iam-s3.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_iam-s3.html</a></p>
<h3 id="IAM-Security-Tools"><a href="#IAM-Security-Tools" class="headerlink" title="IAM Security Tools"></a>IAM Security Tools</h3><ul>
<li><p><code>IAM Credentials Report (account-level)</code></p>
<ul>
<li>a report that lists all your account’s users and the status of their various credentials</li>
</ul>
</li>
<li><p><code>IAM Access Advisor (user-level)</code></p>
<ul>
<li>Access advisor shows the service permissions granted to a user and when those services were last accessed.</li>
<li>You can use this in1formation to revise your policies.</li>
</ul>
</li>
</ul>
<h3 id="IAM-Access-Analyzer"><a href="#IAM-Access-Analyzer" class="headerlink" title="IAM Access Analyzer"></a>IAM Access Analyzer</h3><p>helps you <code>identify the resources in your organization and accounts</code>, such as <code>Amazon S3 buckets</code> or <code>IAM roles</code>, <code>shared with an external entity</code>. This lets you identify unintended access to your resources and data, which is a security risk. Access Analyzer identifies resources shared with external principals by using logic-based reasoning to analyze the resource-based policies in your AWS environment. For each instance of a resource shared outside of your account, Access Analyzer generates a finding.</p>
<ul>
<li><code>Find out which resources are shared externally</code><ul>
<li>S3 Buckets</li>
<li>IAM Roles</li>
<li>KMS Keys</li>
<li>Lambda Functions and Layers</li>
<li>SQS queues</li>
<li>Secrets Manager Secrets</li>
</ul>
</li>
<li>Define <code>Zone of Trust</code> &#x3D; AWS Account or AWS Organization</li>
<li>Access outside zone of trusts &#x3D;&gt; findings</li>
</ul>
<h3 id="IAM-Policy-Types"><a href="#IAM-Policy-Types" class="headerlink" title="IAM Policy Types"></a>IAM Policy Types</h3><p>You manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. Resource-based policies are JSON policy documents that you attach to a resource such as an Amazon S3 bucket. These policies grant the specified principal permission to perform specific actions on that resource and define under what conditions this applies.</p>
<ul>
<li><code>Identity-based</code> policies are attached to an IAM user, group, or role. These policies let you specify what that identity can do (its permissions).</li>
<li><code>Resource-based</code> policies are attached to a resource. For example, you can attach resource-based policies to Amazon S3 buckets, Amazon SQS queues, and AWS Key Management Service encryption keys.</li>
<li><code>Identity-based</code> policies and <code>resource-based</code> policies are both permissions policies and are evaluated together. For a request to which only permissions policies apply, AWS first checks all policies for a Deny. If one exists, then the request is denied. Then AWS checks for each Allow. If at least one policy statement allows the action in the request, the request is allowed. It doesn’t matter whether the Allow is in the identity-based policy or the resource-based policy.</li>
</ul>
<h4 id="Trust-policy"><a href="#Trust-policy" class="headerlink" title="Trust policy"></a>Trust policy</h4><p>Defines which principal entities (accounts, users, roles, and federated users) can assume the role. An IAM role is both an identity and a resource that supports resource-based policies. For this reason, you <code>must attach both a trust policy and an identity-based policy to an IAM role</code>. The <code>IAM service</code> <code>supports only one</code> type of <code>resource-based</code> policy called a <code>role trust policy</code>, which is attached to an IAM role.</p>
<p>References: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-resource.html">Identity-based policies and resource-based policies</a></p>
<hr>
<h2 id="Identity-Federation"><a href="#Identity-Federation" class="headerlink" title="Identity Federation"></a>Identity Federation</h2><ul>
<li>Federation <code>lets users outside of AWS to assume temporary role for accessing AWS resources</code>.</li>
<li>These users assume identity provided access role.</li>
<li><code>Federation assumes a form of 3rd party authentication</code><ul>
<li>LDAP</li>
<li>Microsoft Active Directory (~&#x3D; SAML) - EXAM</li>
<li>Single Sign On - EXAM</li>
<li>Open ID</li>
<li>Cognito - EXAM</li>
</ul>
</li>
<li><code>Using federation, you don’t need to create IAM users (user management is outside of AWS)</code>.</li>
</ul>
<h3 id="Custom-Identity-Broker-Application"><a href="#Custom-Identity-Broker-Application" class="headerlink" title="Custom Identity Broker Application"></a>Custom Identity Broker Application</h3><p><code>For Enterprises</code></p>
<ul>
<li>Use only if identity provider is <code>not compatible with SAML 2.0</code>.</li>
<li><code>The identity broker must determine the appropriate IAM policy</code>.</li>
</ul>
<hr>
<h2 id="AWS-DataSync"><a href="#AWS-DataSync" class="headerlink" title="AWS DataSync"></a>AWS DataSync</h2><ul>
<li><code>Move large amount of data</code> to and from<ul>
<li><code>On-premises</code> &#x2F; other cloud to AWS (NFS, SMB, HDFS, S3 API…) – needs agent</li>
<li><code>AWS to AWS</code> (different storage services) – no agent needed</li>
</ul>
</li>
<li>Can synchronize to:<ul>
<li>Amazon S3 (any storage classes – including Glacier)</li>
<li>Amazon EFS</li>
<li>Amazon FSx (Windows, Lustre, NetApp, OpenZFS…)</li>
</ul>
</li>
<li>Replication <code>tasks can be scheduled hourly, daily, weekly</code>.</li>
<li><code>File permissions and metadata are preserved (NFS POSIX, SMB...)</code>. - Exam</li>
<li>One agent task can use 10 Gbps, can setup a bandwidth limit</li>
</ul>
<p>Maybe for <code>large quantity of data</code> you can use <code>AWS Snowcone</code> with has the <code>DataSync agent pre-installed</code>.</p>
<hr>
<h2 id="AWS-STS-–-Security-Token-Service"><a href="#AWS-STS-–-Security-Token-Service" class="headerlink" title="AWS STS – Security Token Service"></a>AWS STS – Security Token Service</h2><ul>
<li><code>Allows to grant limited and temporary access to AWS resources.</code></li>
<li>Token is valid for up to one hour (must be refreshed)</li>
<li><code>AssumeRole</code><ul>
<li>Within your own account: for enhanced security</li>
<li>Cross Account Access: assume role in target account to perform actions there</li>
</ul>
</li>
<li><code>AssumeRoleWithSAML</code><ul>
<li>return credentials for users logged with SAML</li>
</ul>
</li>
<li><code>AssumeRoleWithWebIdentity</code><ul>
<li>return creds for users logged with an IdP (Facebook Login, Google Login, OIDC compatible…)</li>
<li>AWS recommends against using this, and using <code>Cognito</code> instead</li>
</ul>
</li>
<li><code>GetSessionToken</code><ul>
<li>for MFA, from a user or AWS account root user</li>
</ul>
</li>
</ul>
<hr>
<h2 id="Cognito-Identity-Pools-–-IAM-Roles"><a href="#Cognito-Identity-Pools-–-IAM-Roles" class="headerlink" title="Cognito Identity Pools – IAM Roles"></a>Cognito Identity Pools – IAM Roles</h2><ul>
<li><p>Default IAM roles for authenticated and guest users</p>
</li>
<li><p>Define rules to choose the role for each user based on the user’s ID</p>
</li>
<li><p>You can partition your users’ access using <code>policy variables</code>.</p>
</li>
<li><p>IAM credentials are obtained by Cognito Identity Pools through STS</p>
</li>
<li><p>The roles must have a “trust” policy of Cognito Identity Pools</p>
</li>
</ul>
<h3 id="Cognito-User-Pools-vs-Identity-Pools"><a href="#Cognito-User-Pools-vs-Identity-Pools" class="headerlink" title="Cognito User Pools vs Identity Pools"></a>Cognito User Pools vs Identity Pools</h3><ul>
<li><code>Cognito User Pools (for authentication = identity verification)</code>:<ul>
<li>Database of users for your web and mobile application</li>
<li>Allows to federate logins through Public Social, OIDC, SAML…</li>
<li>Can customize the hosted UI for authentication (including the logo)]</li>
<li>Has triggers with AWS Lambda during the authentication flow</li>
<li>Adapt the sign-in experience to different risk levels (MFA, etc…)</li>
</ul>
</li>
<li><code>Cognito Identity Pools (for authorization = access control)</code>:</li>
<li>Obtain AWS credentials for your users</li>
<li>Users can login through Public Social, OIDC, SAML &amp; Cognito User Pools</li>
<li>Users can be unauthenticated (guests)</li>
<li>Users are mapped to IAM roles &amp; policies, can leverage policy variables</li>
<li><code>CUP + CIP = manage user / password + access AWS services</code></li>
</ul>
<hr>
<h2 id="Amazon-Route-53"><a href="#Amazon-Route-53" class="headerlink" title="Amazon Route 53"></a>Amazon Route 53</h2><ul>
<li>A highly available, scalable, fully managed and Authoritative DNS<ul>
<li>Authoritative &#x3D; the customer (you) can update the DNS records</li>
</ul>
</li>
<li>Route 53 is also a Domain Registrar</li>
<li>Ability to check the health of your resources</li>
<li>The only AWS service which provides 100% availability SLA</li>
<li>Why Route 53? 53 is a reference to the traditional DNS port</li>
</ul>
<h3 id="Route-53-–-Records"><a href="#Route-53-–-Records" class="headerlink" title="Route 53 – Records"></a>Route 53 – Records</h3><ul>
<li>How you want to route traffic for a domain</li>
<li>Each record contains:<ul>
<li><code>Domain/subdomain Name</code> – e.g., example.com</li>
<li><code>Record Type</code> – e.g., A or AAAA</li>
<li><code>Value</code> – e.g., 12.34.56.78</li>
<li><code>Routing Policy</code> – how Route 53 responds to queries</li>
<li><code>TTL</code> – amount of time the record cached at DNS Resolvers</li>
</ul>
</li>
<li>Route 53 supports the following DNS record types:</li>
<li>(must know) A &#x2F; AAAA &#x2F; CNAME &#x2F; NS</li>
<li>(advanced) CAA &#x2F; DS &#x2F; MX &#x2F; NAPTR &#x2F; PTR &#x2F; SOA &#x2F; TXT &#x2F; SPF &#x2F; SRV</li>
</ul>
<h3 id="Route-53-–-Record-Types"><a href="#Route-53-–-Record-Types" class="headerlink" title="Route 53 – Record Types"></a>Route 53 – Record Types</h3><ul>
<li><code>A</code> – maps a hostname to IPv4</li>
<li><code>AAAA</code> – maps a hostname to IPv6</li>
<li><code>CNAME</code> – maps a hostname to another hostname<ul>
<li>The target is a domain name which must have an A or AAAA record</li>
<li>Can’t create a CNAME record for the top node of a DNS namespace (Zone Apex)</li>
<li>Example: you can’t create for example.com, but you can create for <a target="_blank" rel="noopener" href="http://www.example.com/">www.example.com</a></li>
</ul>
</li>
<li><code>NS</code> – Name Servers for the Hosted Zone</li>
<li>Control how traffic is routed for a domain</li>
</ul>
<h3 id="Route-53-–-Hosted-Zones"><a href="#Route-53-–-Hosted-Zones" class="headerlink" title="Route 53 – Hosted Zones"></a>Route 53 – Hosted Zones</h3><ul>
<li>A container for records that define how to route traffic to a domain and its subdomains</li>
<li><strong><code>Public Hosted Zones</code></strong> – contains records that specify how to route traffic on the Internet (public domain names)
<em>application1.mypublicdomain.com</em></li>
<li><strong><code>Private Hosted Zones</code></strong> – contain records that specify how you route traffic within one or more VPCs (private domain names)
<em>application1.company.internal</em></li>
<li>You pay $0.50 per month per hosted zone</li>
</ul>
<h3 id="Route-53-–-Records-TTL-Time-To-Live"><a href="#Route-53-–-Records-TTL-Time-To-Live" class="headerlink" title="Route 53 – Records TTL (Time To Live)"></a>Route 53 – Records TTL (Time To Live)</h3><ul>
<li><code>High TTL – e.g., 24 hr</code><ul>
<li>Less traffic on Route 53</li>
<li>Possibly outdated records</li>
</ul>
</li>
<li><code>Low TTL – e.g., 60 sec.</code><ul>
<li>More traffic on Route 53 ($$)</li>
<li>Records are outdated for less time</li>
<li>Easy to change records</li>
</ul>
</li>
<li><code>Except for Alias records, TTL is mandatory for each DNS record</code></li>
</ul>
<h3 id="CNAME-vs-Alias"><a href="#CNAME-vs-Alias" class="headerlink" title="CNAME vs Alias"></a>CNAME vs Alias</h3><ul>
<li><p>AWS Resources (Load Balancer, CloudFront…) expose an AWS hostname:</p>
<ul>
<li><span style="color:blue">lb1-1234.us-east-2.elb.amazonaws.com</span> and you want <span style="color:blue">myapp.mydomain.com</span></li>
</ul>
</li>
<li><p>CNAME:</p>
<ul>
<li>Points a hostname to any other hostname. (app.mydomain.com &#x3D;&gt; blabla.anything.com)</li>
<li><u><strong>ONLY FOR NON ROOT DOMAIN (aka. something.mydomain.com)</strong></u></li>
</ul>
</li>
<li><p>Alias:</p>
<ul>
<li>Points a hostname to an AWS Resource (app.mydomain.com &#x3D;&gt; blabla.amazonaws.com)</li>
<li><u><strong>Works for ROOT DOMAIN and NON ROOT DOMAIN (aka mydomain.com)</strong></u></li>
<li>Free of charge.</li>
<li>Native health check.</li>
</ul>
</li>
</ul>
<h3 id="Route-53-–-Alias-Records"><a href="#Route-53-–-Alias-Records" class="headerlink" title="Route 53 – Alias Records"></a>Route 53 – Alias Records</h3><ul>
<li>Maps a hostname to an AWS resource</li>
<li>An extension to DNS functionality</li>
<li>Automatically recognizes changes in the resource’s IP addresses</li>
<li>Unlike CNAME, it can be used for the top node of a DNS namespace (Zone Apex), e.g.: example.com</li>
<li>Alias Record is always of type A&#x2F;AAAA for AWS resources (IPv4 &#x2F; IPv6)</li>
<li><code>You can’t set the TTL</code></li>
</ul>
<h2 id="Route-53-–-Alias-Records-Targets"><a href="#Route-53-–-Alias-Records-Targets" class="headerlink" title="Route 53 – Alias Records Targets"></a>Route 53 – Alias Records Targets</h2><p><strong>You cannot set an ALIAS record for an EC2 DNS name</strong> - EXAM</p>
<h2 id="Route-53-–-Routing-Policies"><a href="#Route-53-–-Routing-Policies" class="headerlink" title="Route 53 – Routing Policies"></a>Route 53 – Routing Policies</h2><ul>
<li>Define how Route 53 responds to DNS queries</li>
<li>Don’t get confused by the word “Routing”<ul>
<li>It’s not the same as Load balancer routing which routes the traffic</li>
<li>DNS does not route any traffic, it only responds to the DNS queries</li>
</ul>
</li>
<li>Route 53 Supports the following Routing Policies<ul>
<li>Simple</li>
<li>Weighted</li>
<li>Failover</li>
<li>Latency based</li>
<li>Geolocation</li>
<li>Multi-Value Answer</li>
<li>Geoproximity (using Route 53 Traffic Flow feature)</li>
</ul>
</li>
</ul>
<h3 id="Types-of-health-checks"><a href="#Types-of-health-checks" class="headerlink" title="Types of health checks:"></a>Types of health checks:</h3><ol>
<li><code>Health checks that monitor an endpoint</code> – You can configure a health check that monitors an endpoint that you specify either by IP address or by domain name. At regular intervals that you specify, Route 53 submits automated requests over the internet to your application, server, or other resources to verify that it’s reachable, available, and functional. Optionally, you can configure the health check to make requests similar to those that your users make, such as requesting a web page from a specific URL.</li>
<li><code>Health checks that monitor other health checks</code> (calculated health checks) – You can create a health check that monitors whether Route 53 considers other health checks healthy or unhealthy. One situation where this might be useful is when you have multiple resources that perform the same function, such as multiple web servers, and your chief concern is whether some minimum number of your resources are healthy. You can create a  health check for each resource without configuring notifications for those health checks. Then you can create a health check that monitors the status of the other health checks, and that notifies you only when the number of available web resources drops below a specified threshold.</li>
<li><code>Health checks that monitor CloudWatch alarms</code> – You can create CloudWatch alarms that monitor the status of CloudWatch metrics, such as the number of throttled read events for an Amazon DynamoDB database or the number of Elastic Load Balancing hosts that are considered healthy. After you create an alarm, you can create a health check that monitors the same data stream that CloudWatch monitors for the alarm.</li>
</ol>
<h3 id="“Evaluate-Target-Health”"><a href="#“Evaluate-Target-Health”" class="headerlink" title="“Evaluate Target Health”"></a>“Evaluate Target Health”</h3><p>You need to set the “Evaluate Target Health” <code>flag</code> to <code>true</code> on Route 53. This way, Route 53 will check both ALB entry to ensure that your ALBs are responding.</p>
<h3 id="Routing-Policies-–-Weighted"><a href="#Routing-Policies-–-Weighted" class="headerlink" title="Routing Policies – Weighted"></a>Routing Policies – Weighted</h3><ul>
<li>Control the % of the requests that go to each specific resource</li>
<li>Assign each record a relative weight:<ul>
<li>traffic (%) &#x3D; weight for a specific record &#x2F;sum of all weight records</li>
<li>Weights don’t need to sum up to 100</li>
</ul>
</li>
<li>DNS records must have the same name and type</li>
<li><code>Can be associated with Health Checks</code>.</li>
<li><code>Use cases: load balancing between regions, testing new application versions</code>…</li>
<li><code>Assign a weight of 0 to a record to stop sending traffic to a resource</code>.</li>
<li><code>If all records have weight of 0, then all records will be returned equally</code>.</li>
</ul>
<h3 id="Routing-Policies-–-Latency-based"><a href="#Routing-Policies-–-Latency-based" class="headerlink" title="Routing Policies – Latency-based"></a>Routing Policies – Latency-based</h3><ul>
<li>Redirect to the resource that has the least latency close to us</li>
<li>Super helpful when latency for users is a priority</li>
<li>Latency is based on traffic between users and AWS Regions</li>
<li>Germany users may be directed to the US (if that’s the lowest latency)</li>
<li>Can be associated with Health Checks (has a failover capability)</li>
</ul>
<h3 id="Route-53-–-Health-Checks"><a href="#Route-53-–-Health-Checks" class="headerlink" title="Route 53 – Health Checks"></a>Route 53 – Health Checks</h3><ul>
<li><p>HTTP Health Checks are only for <code>public resources</code></p>
</li>
<li><p>Health Check &#x3D;&gt; Automated DNS Failover:</p>
<ol>
<li>Health checks that monitor an endpoint (application, server, other AWS resource)</li>
<li>Health checks that monitor other health checks (Calculated Health Checks)</li>
<li>Health checks that monitor CloudWatch Alarms (full control !!) – e.g., throttles of DynamoDB, alarms on RDS, custom metrics,
… (helpful for private resources)</li>
</ol>
</li>
<li><p>Health Checks are integrated with CW metrics</p>
</li>
</ul>
<h3 id="Health-Checks-–-Private-Hosted-Zones"><a href="#Health-Checks-–-Private-Hosted-Zones" class="headerlink" title="Health Checks – Private Hosted Zones"></a>Health Checks – Private Hosted Zones</h3><ul>
<li>Route 53 health checkers are outside the VPC.</li>
<li>They <code>can’t access private</code> endpoints (private VPC or on-premises resource)</li>
<li>You can create a <code>CloudWatch Metric</code> and associate a <code>CloudWatch Alarm</code>, then create a Health Check that checks the alarm itself.</li>
</ul>
<h3 id="Routing-Policies-–-Geolocation"><a href="#Routing-Policies-–-Geolocation" class="headerlink" title="Routing Policies – Geolocation"></a>Routing Policies – Geolocation</h3><ul>
<li>Different from Latency-based!</li>
<li>This routing is <code>based on user location</code></li>
<li>Specify location by Continent, Country or by US State (if there’s overlapping, most precise location selected)</li>
<li>Should create a “<code>Default”</code> record (in case there’s no match on location)</li>
<li>Use cases: website localization, restrict content distribution, load balancing, …</li>
<li>Can be associated with Health Checks</li>
</ul>
<h3 id="Routing-Policies-–-Geoproximity"><a href="#Routing-Policies-–-Geoproximity" class="headerlink" title="Routing Policies – Geoproximity"></a>Routing Policies – Geoproximity</h3><ul>
<li><p>Route traffic to your resources based on the geographic location of users and resources</p>
</li>
<li><p>Ability <code>to shift more traffic to resources based</code> on the defined <code>bias</code></p>
</li>
<li><p>To change the size of the geographic region, specify <code>bias</code> values:</p>
<ul>
<li>To expand (1 to 99) – more traffic to the resource</li>
<li>To shrink (-1 to -99) – less traffic to the resource</li>
</ul>
</li>
<li><p>Resources can be:</p>
<ul>
<li>AWS resources (specify AWS region)</li>
<li>Non-AWS resources (specify Latitude and Longitude)</li>
</ul>
</li>
<li><p>You must use Route 53 <code>Traffic Flow</code> to use this feature</p>
</li>
</ul>
<h3 id="Route-53-–-Traffic-flow"><a href="#Route-53-–-Traffic-flow" class="headerlink" title="Route 53 – Traffic flow"></a>Route 53 – Traffic flow</h3><ul>
<li>Simplify the process of creating and maintaining records in large and complex configurations</li>
<li>Visual editor to manage complex routing decision trees</li>
<li>Configurations can be saved as <code>Traffic Flow Policy</code><ul>
<li>Can be applied to different Route 53 Hosted Zones (different domain names)</li>
<li>Supports versioning</li>
</ul>
</li>
</ul>
<h3 id="Route-53-–-Hybrid-DNS-OJO"><a href="#Route-53-–-Hybrid-DNS-OJO" class="headerlink" title="Route 53 – Hybrid DNS - OJO"></a>Route 53 – Hybrid DNS - OJO</h3><ul>
<li>By default, Route 53 Resolver automatically answers DNS queries for:<ul>
<li>Local domain names for EC2 instances</li>
<li>Records in Private Hosted Zones</li>
<li>Records in public Name Servers</li>
</ul>
</li>
<li><code>Hybrid DNS</code> – resolving DNS queries between VPC (Route 53 Resolver) and your networks (other DNS Resolvers)</li>
<li>Networks can be:<ul>
<li>VPC itself &#x2F; Peered VPC</li>
<li>On-premises Network (connected through Direct Connect or AWS VPN)</li>
</ul>
</li>
</ul>
<h3 id="Route-53-–-Resolver-Endpoints"><a href="#Route-53-–-Resolver-Endpoints" class="headerlink" title="Route 53 – Resolver Endpoints"></a>Route 53 – Resolver Endpoints</h3><ul>
<li><p><code>Inbound Endpoint</code></p>
<ul>
<li>DNS Resolvers on your network can forward DNS queries to Route 53 Resolver</li>
<li>Allows your DNS Resolvers to resolve domain names for AWS resources (e.g., EC2 instances) and records in Route 53 Private Hosted Zones</li>
</ul>
</li>
<li><p><code>Outbound Endpoint</code></p>
<ul>
<li>Route 53 Resolver conditionally forwards DNS queries to your DNS Resolvers</li>
<li>Use <code>Resolver</code> Rules to forward DNS queries to your DNS Resolvers</li>
</ul>
</li>
<li><p>Associated with one or more VPCs in the same AWS Region</p>
</li>
<li><p>Create in two AZs for high availability</p>
</li>
<li><p>Each Endpoint supports 10,000 queries per second per IP address</p>
</li>
</ul>
<h3 id="Route-53-–-Resolver-Rules"><a href="#Route-53-–-Resolver-Rules" class="headerlink" title="Route 53 – Resolver Rules"></a>Route 53 – Resolver Rules</h3><ul>
<li>Control which DNS queries are forwarded to DNS Resolvers on your network</li>
<li><code>Conditional Forwarding Rules (Forwarding Rules)</code><ul>
<li>Forward DNS queries for a specified domain and all its subdomains <code>to target IP addresses</code></li>
</ul>
</li>
<li><code>System Rules</code><ul>
<li>Selectively overriding the behavior defined in Forwarding Rules (e.g., don’t forward DNS queries for a subdomain acme.example.com)</li>
</ul>
</li>
<li><code>Auto-defined System Rules</code><ul>
<li>Defines how DNS queries for selected domains are resolved (e.g., AWS internal domain names, Privated Hosted Zones)</li>
</ul>
</li>
<li>If multiple rules matched, Route 53 Resolver chooses the most specific match</li>
<li><code>Resolver Rules can be shared across accounts using AWS RAM</code><ul>
<li>Manage them centrally in one account</li>
<li>Send DNS queries from multiple VPC to the target IP defined in the rule</li>
</ul>
</li>
</ul>
<hr>
<h2 id="ELB"><a href="#ELB" class="headerlink" title="ELB"></a>ELB</h2><h3 id="Elastic-Load-Balancing-and-AWS-X-Ray"><a href="#Elastic-Load-Balancing-and-AWS-X-Ray" class="headerlink" title="Elastic Load Balancing and AWS X-Ray"></a><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/xray/latest/devguide/xray-services-elb.html">Elastic Load Balancing and AWS X-Ray</a></h3><p>Elastic Load Balancing application load balancers add a trace ID to incoming HTTP requests in a header named X-Amzn-Trace-Id.</p>
<pre><code>  X-Amzn-Trace-Id: Root=1-5759e988-bd862e3fe1be46a994272793
</code></pre>
<p><code>Load balancers do not send data to X-Ray</code>, and do not appear as a node on your service map.</p>
<h3 id="ELB-access-logs"><a href="#ELB-access-logs" class="headerlink" title="ELB access logs"></a>ELB access logs</h3><p>ELB access logs is an optional feature of Elastic Load Balancing that is disabled by default. The access logs capture detailed information about requests sent to your load balancer. Each log contains information such as the time the request was received, the client’s IP address, latencies, request paths, and server responses. You can use these access logs to analyze traffic patterns and troubleshoot issues. Each access log file is automatically encrypted using SSE-S3 before it is stored in your S3 bucket and decrypted when you access it. You do not need to take any action; the encryption and decryption is performed transparently</p>
<h4 id="VPC-Flow-Logs-only-captures-information-about-the-IP-traffic-going-to-and-from-network-interfaces-in-a-VPC"><a href="#VPC-Flow-Logs-only-captures-information-about-the-IP-traffic-going-to-and-from-network-interfaces-in-a-VPC" class="headerlink" title="VPC Flow Logs only captures information about the IP traffic going to and from network interfaces in a VPC"></a><code>VPC Flow Logs</code> only captures information about the <code>IP traffic</code> going to and from network interfaces in a VPC</h4><p>Reference: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html">Access logs for your Application Load Balancer</a></p>
<h3 id="CloudTrail-logs"><a href="#CloudTrail-logs" class="headerlink" title="CloudTrail logs"></a>CloudTrail logs</h3><p><code>Elastic Load Balancing</code> is <code>integrated</code> with <code>AWS CloudTrail</code>, a service that provides a record of actions taken by a user, role, or an AWS service in Elastic Load Balancing. <code>CloudTrail captures</code> all <code>API calls for Elastic Load Balancing as events</code>. The calls captured include calls from the AWS Management Console and code calls to the Elastic Load Balancing API operations.</p>
<h2 id="Amazon-Machine-Images-AMIs"><a href="#Amazon-Machine-Images-AMIs" class="headerlink" title="Amazon Machine Images (AMIs)"></a>Amazon Machine Images (AMIs)</h2><h3 id="Sharing"><a href="#Sharing" class="headerlink" title="Sharing"></a>Sharing</h3><p>The key points to consider before planning the expansion and sharing of Amazon Machine Images (AMIs) are:</p>
<ol>
<li>AMIs are regional resources and can be shared across Regions: AMIs are specific to a particular AWS Region. If you want to use an AMI in a different Region, you need to copy the AMI to that Region. Sharing an AMI across Regions requires creating a new copy in each desired Region.
2&#x2F; You need to share any CMKs used to encrypt snapshots and any Amazon EBS snapshots that the AMI references: If the AMI references Amazon Elastic Block Store (EBS) snapshots, you must also share those snapshots. Additionally, if the snapshots are encrypted using customer-managed customer master keys (CMKs), you need to share the CMKs as well.</li>
</ol>
<h2 id="EC2"><a href="#EC2" class="headerlink" title="EC2"></a>EC2</h2><h3 id="Errors"><a href="#Errors" class="headerlink" title="Errors"></a>Errors</h3><h4 id="Client-InternalError-Client-error-on-launch"><a href="#Client-InternalError-Client-error-on-launch" class="headerlink" title="Client.InternalError: Client error on launch"></a><code>Client.InternalError: Client error on launch</code></h4><ol>
<li>error is caused when an Auto Scaling group attempts to launch an instance that has an encrypted EBS volume, but the service-linked role does not have access to the customer-managed CMK used to encrypt it. Additional setup is required to allow the Auto Scaling group to launch instances.</li>
</ol>
<h3 id="Termination-Policy"><a href="#Termination-Policy" class="headerlink" title="Termination Policy"></a>Termination Policy</h3><ol>
<li>You <code>can&#39;t enable termination protection for Spot Instances</code>, a Spot Instance is terminated when the Spot price exceeds the amount you’re willing to pay for Spot Instances. However, you can prepare your application to handle Spot Instance interruptions.</li>
<li>To prevent instances that are part of an Auto Scaling group from terminating on scale in, use instance protection. The <code>DisableApiTermination</code> attribute does not prevent Amazon EC2 Auto Scaling from terminating an instance.</li>
</ol>
<h3 id="Spot-Instances-Interruptions"><a href="#Spot-Instances-Interruptions" class="headerlink" title="Spot Instances Interruptions"></a>Spot Instances Interruptions</h3><p>You can specify that Amazon EC2 should do one of the following when it interrupts a Spot Instance:</p>
<ol>
<li><code>Stop</code> the Spot Instance</li>
<li><code>Hibernate</code> the Spot Instance</li>
<li><code>Terminate</code> the Spot Instance</li>
</ol>
<p>The default is to <code>terminate</code> Spot Instances when they are interrupted.</p>
<h3 id="Q-Spot-Fleet-Config-Cost-Optimization"><a href="#Q-Spot-Fleet-Config-Cost-Optimization" class="headerlink" title="*Q Spot Fleet Config Cost Optimization"></a><code>*Q</code> <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/how-spot-fleet-works.html">Spot Fleet Config Cost Optimization</a></h3><p>Using <code>lowestPrice</code> allocation <code>strategy</code> a Spot Fleet automatically deploys the lowest price combination of instance types and Availability Zones based on the current Spot price across the number of Spot pools specified. You can use this combination to avoid the most expensive Spot Instances.</p>
<p><u>Spot Fleets allow us to automatically request Spot Instances with the lowest price</u></p>
<h3 id="Q-Get-public-IP-address"><a href="#Q-Get-public-IP-address" class="headerlink" title="*Q Get public IP address"></a><code>*Q</code> Get public IP address</h3><p>EC2 instances in AWS have <code>metadata</code> associated with them that can be accessed from within the instance. This metadata includes information about the instance, such as its IP address, instance type, security groups, and more.</p>
<p>Can make an HTTP GET request to a specific URL provided by the instance metadata service. The URL is <a target="_blank" rel="noopener" href="http://169.254.169.254/latest/meta-data/public-ipv4">http://169.254.169.254/latest/meta-data/public-ipv4</a>.</p>
<h3 id="👀-EC2-Detailed-monitoring-👀"><a href="#👀-EC2-Detailed-monitoring-👀" class="headerlink" title="👀 EC2 Detailed monitoring 👀"></a>👀 EC2 Detailed monitoring 👀</h3><p><code>Metrics are the fundamental concept in CloudWatch</code>. A metric represents a time-ordered set of data points that are published to CloudWatch. Think of a metric as a variable to monitor, and the data points as representing the values of that variable over time.</p>
<p><code>By default, your instance is enabled for basic monitoring</code>. You can optionally enable detailed monitoring. After you enable <code>detailed monitoring</code>, the Amazon EC2 console displays monitoring graphs with a <code>1-minute period</code> for the instance. .In <code>Basic monitoring</code>, data is available automatically in <code>5-minute periods</code> at no charge</p>
<p>Reference: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch-new.html">Enable or turn off detailed monitoring for your instances</a></p>
<h3 id="👀-EC2-Launch-Troubleshooting-👀"><a href="#👀-EC2-Launch-Troubleshooting-👀" class="headerlink" title="👀 EC2 Launch Troubleshooting 👀"></a>👀 EC2 Launch Troubleshooting 👀</h3><p><code>InstanceLimitExceeded</code>: if you get this error, it means that you have reached your limit of <code>max number of vCPUs</code> per <code>region</code>.</p>
<p><code>InsufficientInstanceCapacity</code> : if you get this error, it means &#96;AWS does not have that enough On-Demand capacity&#96;&#96; in the particular AZ where
the instance is launched</p>
<p><code>Instance Terminates Immediately</code> <strong>(goes from pending to terminated)</strong></p>
<ol>
<li>You’ve reached your EBS volume limit.</li>
<li>An EBS snapshot is corrupt.</li>
<li>The root EBS volume is encrypted and you do not have permissions to access the KMS key for decryption.</li>
<li>The instance store-backed AMI that you used to launch the instance is missing a required part (an image.part.xx file).</li>
</ol>
<h3 id="👀-EC2-SSH-Troubleshooting-👀"><a href="#👀-EC2-SSH-Troubleshooting-👀" class="headerlink" title="👀 EC2 SSH Troubleshooting 👀"></a>👀 EC2 SSH Troubleshooting 👀</h3><ol>
<li>SG is not configured correctly</li>
<li>NACL is not configured correctly</li>
<li>Check the route table for the subnet (routes traffic destined outside VPC to IGW)</li>
<li>Instance doesn’t have a public IPv4</li>
<li>CPU load of the instance is high</li>
</ol>
<h3 id="EC2-Instances-Purchasing-Options"><a href="#EC2-Instances-Purchasing-Options" class="headerlink" title="EC2 Instances Purchasing Options"></a>EC2 Instances Purchasing Options</h3><ol>
<li><code>On-Demand</code> Instances – short workload, predictable pricing, pay by second</li>
<li><code>Reserved</code> (1 &amp; 3 years)</li>
<li><code>Reserved Instances</code> – long workloads</li>
<li><code>Convertible Reserved Instances</code> – long workloads with flexible instances</li>
<li><code>Savings Plans (1 &amp; 3 years)</code> –commitment to an amount of usage, long workload</li>
<li><code>Spot Instances</code> – short workloads, cheap, can lose instances (less reliable)</li>
<li><code>Dedicated Hosts</code> – book an entire physical server, control instance placement</li>
<li><code>Dedicated Instances</code> – no other customers will share your hardware</li>
<li><code>Capacity Reservations</code> – reserve capacity in a specific AZ for any duration</li>
</ol>
<h2 id="AWS-Storage-Gateway-1"><a href="#AWS-Storage-Gateway-1" class="headerlink" title="AWS Storage Gateway"></a>AWS Storage Gateway</h2><p>AWS Storage Gateway is a set of hybrid cloud storage services that provide on-premises access to virtually unlimited cloud storage.</p>
<p>AWS Storage Gateway <code>uses SSL/TLS</code> (Secure Socket Layers&#x2F;Transport Layer Security) <code>to encrypt data</code> that is transferred <code>between your gateway appliance and AWS storage</code>. By default, Storage Gateway uses Amazon S3-Managed Encryption Keys (SSE-S3) to server-side encrypt all data it stores in Amazon S3. You have an option to use the Storage Gateway API to configure your gateway to encrypt data stored in the cloud using server-side encryption with AWS Key Management Service (SSE-KMS) customer master keys (CMKs).</p>
<p>File, Volume and Tape Gateway data is stored in Amazon S3 buckets by AWS Storage Gateway. Tape Gateway supports backing data to Amazon S3 Glacier apart from the standard storage.</p>
<p>Encrypting a file share: For a file share, you can configure your gateway to encrypt your objects with AWS KMS–managed keys by using SSE-KMS.</p>
<p>Encrypting a volume: For cached and stored volumes, you can configure your gateway to encrypt volume data stored in the cloud with AWS KMS–managed keys by using the Storage Gateway API.</p>
<p>Encrypting a tape: For a virtual tape, you can configure your gateway to encrypt tape data stored in the cloud with AWS KMS–managed keys by using the Storage Gateway API.</p>
<h3 id="Tape-Gateway"><a href="#Tape-Gateway" class="headerlink" title="Tape Gateway"></a>Tape Gateway</h3><p>Tape Gateway enables you to replace using physical tapes on-premises with virtual tapes in AWS without changing existing backup workflows. Tape Gateway supports all leading backup applications and caches virtual tapes on-premises for low-latency data access. Tape Gateway encrypts data between the gateway and AWS for secure data transfer and compresses data and transitions virtual tapes between Amazon S3 and Amazon S3 Glacier, or Amazon S3 Glacier Deep Archive, to minimize storage costs.</p>
<h3 id="File-Gateway"><a href="#File-Gateway" class="headerlink" title="File Gateway"></a>File Gateway</h3><p>File Gateway provides a seamless way to connect to the cloud in order to store application data files and backup images as durable objects in Amazon S3 cloud storage. File Gateway offers <code>SMB or NFS-based</code> access to data in Amazon S3 with local caching.</p>
<h3 id="Volume-Gateway"><a href="#Volume-Gateway" class="headerlink" title="Volume Gateway"></a>Volume Gateway</h3><p>You can configure the AWS Storage Gateway service as a Volume Gateway to present <code>cloud-based iSCSI block</code> storage volumes to your <code>on-premises</code> applications. The Volume Gateway provides either a local cache or full volumes on-premises while also storing full copies of your volumes in the AWS cloud. Volume Gateway also provides Amazon EBS Snapshots of your data for backup, disaster recovery, and migration. It’s easy to get started with the Volume Gateway: Deploy it as a virtual machine or hardware appliance, give it local disk resources, connect it to your applications, and start using your hybrid cloud storage for block data.</p>
<p>Reference: <a target="_blank" rel="noopener" href="https://aws.amazon.com/storagegateway/">AWS Storage Gateway</a></p>
<h2 id="Posts"><a href="#Posts" class="headerlink" title="Posts"></a>Posts</h2><ol>
<li><code>RDP traffic</code>: Port 3389, TCP protocol.</li>
</ol>
<h2 id="👀-AWS-Directory-Services"><a href="#👀-AWS-Directory-Services" class="headerlink" title="👀 AWS Directory Services"></a>👀 AWS Directory Services</h2><p>service that automatically <code>creates</code> an AWS <code>security group</code> in your VPC with network rules for traffic in and out of AWS managed domain controllers. The default inbound rules <code>allow traffic from any source (0.0.0.0/0) to ports required by Active Directory</code>. These rules do not introduce security vulnerabilities, as traffic to the domain controllers is limited to traffic from your VPC, other peered VPCs, or networks connected using AWS Direct Connect, AWS Transit Gateway or Virtual Private Network.</p>
<p><code>By default</code>, AWS Directory Services creates security groups that <em><code>allow unrestricted access</code></em>, which can be &#96;flagged as a security concern. To address this issue, you need to review the security group rules and make necessary adjustments to restrict access based on your specific requirements and security best practices.</p>
<p>Using <code>AWS Trusted Advisor</code> can provide additional insights into security best practices and potential misconfigurations, but it may not specifically highlight the security group issue related to AWS Directory Services.</p>
<h2 id="Enhanced-networking"><a href="#Enhanced-networking" class="headerlink" title="Enhanced networking"></a>Enhanced networking</h2><p><code>*Q</code> Consider using enhanced networking for the following scenarios of <code>network performance issues</code>:</p>
<ol>
<li>If your packets-per-second rate reaches its ceiling, consider moving to enhanced networking. If your rate reaches its ceiling, you’ve likely reached the upper thresholds of the virtual network interface driver.</li>
<li>If your throughput is near or exceeding 20K packets per second (PPS) on the VIF driver, it’s a best practice to use enhanced networking.</li>
</ol>
<p>All current generation instance types support enhanced networking, except for T2 instances.</p>
<h2 id="Cost-Allocation-Tags-1"><a href="#Cost-Allocation-Tags-1" class="headerlink" title="Cost Allocation Tags"></a>Cost Allocation Tags</h2><p>A tag is a label that you or AWS assigns to an AWS resource. Each tag consists of a key and a value. For each resource, each tag key must be unique, and each tag key can have only one value. You can use tags to organize your resources, and cost allocation tags to track your AWS costs on a detailed level. After you activate cost allocation tags, AWS uses the cost allocation tags to organize your resource costs on your cost allocation report, to make it easier for you to categorize and track your AWS costs.</p>
<h2 id="👀-Q-AWS-Resource-Groups-Tag-Editor"><a href="#👀-Q-AWS-Resource-Groups-Tag-Editor" class="headerlink" title="👀 *Q AWS Resource Groups Tag Editor"></a>👀 <code>*Q</code> <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/ARG/latest/userguide/tag-editor.html">AWS Resource Groups Tag Editor</a></h2><p>With Resource Groups, you can create, maintain, and view a collection of resources that share common tags. Tag Editor manages <code>tags across services and AWS Regions</code>. Tag Editor can perform a global search and can edit a large number of tags at one time.</p>
<h2 id="OpsWorks"><a href="#OpsWorks" class="headerlink" title="OpsWorks"></a>OpsWorks</h2><p>AWS OpsWorks is a <code>configuration management service</code> that provides managed instances of <code>Chef</code> and <code>Puppet</code>.</p>
<h3 id="👀-Chef-Server"><a href="#👀-Chef-Server" class="headerlink" title="👀 Chef Server"></a>👀 Chef Server</h3><p>You can <code>add nodes</code> automatically to your Chef Server using the <code>unattended method</code>. The recommended method of unattended (or automatic) association of new nodes is to <code>configure</code> the <code>Chef Client Cookbook</code>.</p>
<h2 id="Q-AWS-Service-Health-Dashboard"><a href="#Q-AWS-Service-Health-Dashboard" class="headerlink" title="*Q AWS Service Health Dashboard"></a><code>*Q</code> AWS Service Health Dashboard</h2><p>Publishes the most up-to-the-minute information on the status and availability of all AWS services in tabular form for all Regions that AWS is present in. You can check on this page <a target="_blank" rel="noopener" href="https://status.aws.amazon.com/">https://status.aws.amazon.com/</a> to get current status information.</p>
<h2 id="👀-Cost-Allocation-Tags-Account-Level"><a href="#👀-Cost-Allocation-Tags-Account-Level" class="headerlink" title="👀 Cost Allocation Tags - Account Level"></a>👀 Cost Allocation Tags - Account Level</h2><p>A tag is a label that you or AWS assigns to an AWS resource. Each tag consists of a key and a value. For each resource, each tag key must be unique, and each tag key can have only one value. You can use tags to organize your resources, and cost allocation tags to track your AWS costs on a detailed level. After you activate cost allocation tags, AWS uses the cost allocation tags to organize your resource costs on your cost allocation report, to make it easier for you to categorize and track your AWS costs.</p>
<h2 id="Protecting-data-using-encryption"><a href="#Protecting-data-using-encryption" class="headerlink" title="Protecting data using encryption"></a><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html">Protecting data using encryption</a></h2><h3 id="SSE-S3-Server-Side-Encryption-with-Amazon-S3-Managed-Keys-SSE-S3"><a href="#SSE-S3-Server-Side-Encryption-with-Amazon-S3-Managed-Keys-SSE-S3" class="headerlink" title="SSE-S3 - Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)"></a>SSE-S3 - Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)</h3><p>Using SSE-S3 each object is encrypted with a unique key employing strong multi-factor encryption. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data.</p>
<h3 id="SSE-KMS"><a href="#SSE-KMS" class="headerlink" title="SSE-KMS"></a>SSE-KMS</h3><p>Similar to SSE-S3 and also <code>provides</code> you with an <code>audit trail</code> of when your key was used and by whom. Additionally, you have the <code>option</code> to <code>create</code> and <code>manage encryption keys yourself</code>.</p>
<h3 id="SSE-C"><a href="#SSE-C" class="headerlink" title="SSE-C"></a>SSE-C</h3><p><code>You manage the encryption keys</code> and <code>Amazon S3 manages the encryption</code> as it writes to disks and decryption when you access your objects.</p>
<h3 id="Client-Side-Encryption"><a href="#Client-Side-Encryption" class="headerlink" title="Client-Side Encryption"></a>Client-Side Encryption</h3><p>You can encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p>
<h2 id="AWS-Elastic-Beanstalk"><a href="#AWS-Elastic-Beanstalk" class="headerlink" title="AWS Elastic Beanstalk"></a>AWS Elastic Beanstalk</h2><p>With deployment policies such as ‘All at once’, AWS Elastic Beanstalk performs an in-place update when you update your application versions and your application can become unavailable to users for a short period of time. You can avoid this downtime by performing a blue&#x2F;green deployment, where you deploy the new version to a separate environment, and then swap CNAMEs (via Route 53) of the two environments to redirect traffic to the new version instantly. In case of any deployment issues, the rollback process is very quick via swapping the URLs for the two environments.</p>
<p>Reference: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html">Deploying applications to Elastic Beanstalk environments</a></p>
<h2 id="Dedicated-Hosts-and-Dedicated-Instances"><a href="#Dedicated-Hosts-and-Dedicated-Instances" class="headerlink" title="Dedicated Hosts and Dedicated Instances"></a>Dedicated Hosts and Dedicated Instances</h2><h2 id="Dedicated-Instances"><a href="#Dedicated-Instances" class="headerlink" title="Dedicated Instances"></a><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html">Dedicated Instances</a></h2><p>Are Amazon EC2 instances that run in a virtual private cloud (VPC) on <code>hardware</code> that’s <code>dedicated</code> to a <code>single customer</code>. Dedicated Instances that belong to different AWS accounts are physically isolated at a hardware level, even if those accounts are linked to a single-payer account. <code>Note</code> that Dedicated Instances may share hardware with other instances from the same AWS account that are not Dedicated Instances.</p>
<p>Dedicated Hosts allow you to use your existing per-socket, per-core, or per-VM software licenses, including Windows Server, Microsoft SQL Server, SUSE, and Linux Enterprise Server</p>
<h2 id="Dedicated-Host"><a href="#Dedicated-Host" class="headerlink" title="Dedicated Host"></a><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html">Dedicated Host</a></h2><p>Is a <code>physical server</code> with EC2 instance capacity fully dedicated to your use</p>
<ul>
<li>Dedicated Hosts <code>allow</code> you to <code>use</code> your <code>existing software licenses</code> on EC2 instances. With a Dedicated Host, you have <code>visibility and control</code> over how <code>instances</code> are placed on the server.</li>
<li>Dedicated Hosts allow you to use your existing <code>per-socket</code>, <code>per-core</code>, or <code>per-VM software licenses</code>, including <code>Windows Server</code>, <code>Microsoft SQL Server</code>, <code>SUSE</code>, and <code>Linux Enterprise Server</code>.</li>
</ul>
<p>Reference: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html#dedicated-hosts-dedicated-instances">Dedicated Hosts</a></p>
<h2 id="AWS-CloudHSM-Hardware-Security-Module"><a href="#AWS-CloudHSM-Hardware-Security-Module" class="headerlink" title="AWS CloudHSM (Hardware Security Module)"></a><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/cloudhsm/latest/userguide/clusters.html">AWS CloudHSM (Hardware Security Module)</a></h2><p>CloudHSM provides <code>tamper-resistant hardware</code> that is <code>available</code> in <code>multiple Availability Zones</code> (AZs), ensuring <code>high availability</code> and <code>durability of the keys</code>.</p>
<p>AWS CloudHSM provides dedicated <code>hardware security</code> modules to store and <code>manage cryptographic KEYS securely</code>. It offers <code>FIPS 140-2 Level 3</code> validated HSMs, which are ideal for meeting compliance requirements. With CloudHSM, you have full control over the key lifecycle and can perform key operations within the HSM, ensuring strong security and compliance for your keys.</p>
<p>You can use stack sets to deploy your catalog to many accounts at the same time. If you want to share a reference (an imported version of your portfolio that stays in sync with the original), you can use account-to-account sharing or you can share using AWS Organizations.</p>
<h2 id="Amazon-EFS-–-Elastic-File-System"><a href="#Amazon-EFS-–-Elastic-File-System" class="headerlink" title="Amazon EFS – Elastic File System"></a>Amazon EFS – Elastic File System</h2><ul>
<li><p>Use cases: content management, web serving, data sharing, Wordpress</p>
</li>
<li><p>Uses NFSv4.1 protocol</p>
</li>
<li><p>Uses security group to control access to EFS</p>
</li>
<li><p><strong>Compatible with Linux based AMI (not Windows)</strong></p>
</li>
<li><p>Encryption at rest using KMS</p>
</li>
<li><p>POSIX file system (~Linux) that has a standard file API</p>
</li>
<li><p>File system scales automatically, pay-per-use, no capacity planning!</p>
</li>
</ul>
<h3 id="EFS-vs-EBS"><a href="#EFS-vs-EBS" class="headerlink" title="EFS vs EBS"></a>EFS vs EBS</h3><h3 id="EFS-Access-Points"><a href="#EFS-Access-Points" class="headerlink" title="EFS - Access Points"></a>EFS - Access Points</h3><ul>
<li>Easily manage applications access to NFS environments</li>
<li>Enforce a POSIX user and group to use when accessing the file system</li>
<li>Restrict access to a directory within the file system and optionally specify a different root directory</li>
<li>Can restrict access from NFS clients using IAM policies</li>
</ul>
<h3 id="EFS-Operations"><a href="#EFS-Operations" class="headerlink" title="EFS - Operations"></a>EFS - Operations</h3><ul>
<li>Operations that can be done in place:<ul>
<li>Lifecycle Policy (enable IA or change IA settings)</li>
<li>Throughput Mode and Provisioned Throughput Numbers</li>
<li>EFS Access Points</li>
</ul>
</li>
<li>Operations that require a migration using DataSync (replicates all file attributes and metadata)<ul>
<li><code>Migration to encrypted EFS</code></li>
<li><code>Performance Mode (e.g. Max IO)</code></li>
</ul>
</li>
</ul>
<h3 id="Amazon-Data-Lifecycle-Manager-OJO"><a href="#Amazon-Data-Lifecycle-Manager-OJO" class="headerlink" title="Amazon Data Lifecycle Manager - OJO"></a>Amazon Data Lifecycle Manager - OJO</h3><ul>
<li>Automate the creation, retention, and deletion of EBS snapshots and EBS-backed AMIs.</li>
<li>Schedule backups, cross-account snapshot copies, delete outdated backups, …</li>
<li>Uses resource tags to identify the resources (EC2 instances, EBS volumes).</li>
<li>Can’t be used to manage snapshots&#x2F;AMIs created outside DLM.</li>
<li>Can’t be used to manage instance-store backed AMIs</li>
</ul>
<h3 id="EFS-–-Storage-Classes"><a href="#EFS-–-Storage-Classes" class="headerlink" title="EFS – Storage Classes"></a>EFS – Storage Classes</h3><p><strong><code>Storage Tiers (lifecycle management feature – move file after N days)</code></strong></p>
<ul>
<li>Standard: for frequently accessed files</li>
<li>Infrequent access (EFS-IA): cost to retrieve files, lower price to store. Enable EFS-IA with a Lifecycle Policy</li>
</ul>
<p><strong><code>Availability and durability</code></strong></p>
<ul>
<li><p>Standard: Multi-AZ, great for prod</p>
</li>
<li><p>One Zone: One AZ, great for dev, backup enabled by default, compatible with IA (EFS One Zone-IA)</p>
</li>
<li><p>Over 90% in cost savings</p>
</li>
</ul>
<h3 id="EFS-–-CloudWatch-Metrics"><a href="#EFS-–-CloudWatch-Metrics" class="headerlink" title="EFS – CloudWatch Metrics"></a>EFS – CloudWatch Metrics</h3><p><strong><code>PercentIOLimit</code></strong></p>
<ul>
<li>How close the file system reaching the I&#x2F;O limit (General Purpose)</li>
<li>If at 100%, move to Max I&#x2F;O (migration)</li>
</ul>
<p><strong><code>BurstCreditBalance</code></strong></p>
<ul>
<li>The number of burst credits the file system can use to achieve higher throughput levels</li>
</ul>
<p><strong><code>StorageBytes</code></strong></p>
<ul>
<li>File system’s size in bytes (15 minutes interval)</li>
<li>Dimensions: Standard, IA, Total (Standard + IA)</li>
</ul>
<h3 id="Enforce-creation-that-is-encrypted-at-rest"><a href="#Enforce-creation-that-is-encrypted-at-rest" class="headerlink" title="Enforce creation that is encrypted at rest"></a>Enforce creation that is encrypted at rest</h3><ul>
<li>Use the <code>elasticfilesystem:Encrypted</code> IAM condition key in AWS IAM identity-based policies to mandate users for creating only encrypted-at-rest Amazon EFS file systems</li>
</ul>
<p>You can create an AWS Identity and Access Management (IAM) identity-based policy to control whether users can create Amazon EFS file systems that are encrypted at rest. The Boolean condition key elasticfilesystem:Encrypted specifies the type of file system, encrypted or unencrypted, that the policy applies to. You use the condition key with the elasticfilesystem:CreateFileSystem action and the policy effect, allow or deny, to create a policy for creating encrypted or unencrypted file systems.</p>
<ul>
<li>Define <code>Service Control Policies (SCPs)</code> inside <code>AWS Organizations</code> to enforce EFS encryption for all AWS accounts in your organization.</li>
</ul>
<p>Service control policies (SCPs) are a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization.</p>
<p>An SCP restricts permissions for IAM users and roles in member accounts, including the member account’s root user. If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable SCPs, the user or role can’t perform that action. You can also define service control policies (SCPs) inside AWS Organizations to enforce EFS encryption for all AWS accounts in your organization.</p>
<h3 id="EC2-instances-are-unable-to-mount-the-file-system"><a href="#EC2-instances-are-unable-to-mount-the-file-system" class="headerlink" title="EC2 instances are unable to mount the file system."></a>EC2 instances are unable to mount the file system.</h3><p>The <code>security groups</code> that you associate with a mount target must allow inbound access for the <code>TCP protocol</code> on the <code>NFS port</code> <code>from</code> the <code>security group</code> used by the instances.
To connect your Amazon EFS file system to your Amazon EC2 instance, you <code>must create two security groups</code>: <code>one</code> for your Amazon <code>EC2 instance</code> and <code>another</code> for your Amazon <code>EFS mount target</code>.</p>
<p>Reference: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/efs/latest/ug/accessing-fs-create-security-groups.html">Creating security groups</a></p>
<h2 id="👀-Amazon-Inspector"><a href="#👀-Amazon-Inspector" class="headerlink" title="👀 Amazon Inspector"></a>👀 Amazon Inspector</h2><p>It is an automated security assessment <code>service</code> that <code>helps improve the security and compliance</code> of <code>applications deployed on AWS</code>.
Amazon Inspector <code>checks</code> for <code>unintended network accessibility</code> of your Amazon EC2 instances and <em><code>vulnerabilities</code></em> on those EC2 instances.</p>
<p>Amazon Inspector also <code>integrates</code> with <code>AWS Security Hub</code> to provide a <code>view</code> of your <code>security posture across multiple AWS accounts</code>.</p>
<h2 id="AWS-Control-Tower-1"><a href="#AWS-Control-Tower-1" class="headerlink" title="AWS Control Tower"></a>AWS Control Tower</h2><p>Offers the easiest way to <code>set up</code> and <code>govern</code> a <code>secure, multi-account AWS environment</code>. It establishes a landing zone that is based on the best-practices blueprints and enables governance using guardrails you can choose from a pre-packaged list. The landing zone is a well-architected, multi-account baseline that follows AWS best practices. Guardrails implement governance rules for security, compliance, and operations.</p>
<h2 id="AWS-X-Ray"><a href="#AWS-X-Ray" class="headerlink" title="AWS X-Ray"></a>AWS X-Ray</h2><ol>
<li><code>S3</code> - AWS X-Ray integrates with Amazon S3 to trace upstream requests to update your application’s S3 buckets.</li>
<li><code>Lambda functions</code> - Lambda runs the X-Ray daemon and records a segment with details about the function invocation and execution.</li>
<li><code>API Gateway APIs</code> - You can use X-Ray to trace and analyze user requests as they travel through your Amazon API Gateway APIs to the underlying services.</li>
</ol>
<h2 id="DNS-Resolution"><a href="#DNS-Resolution" class="headerlink" title="DNS Resolution"></a>DNS Resolution</h2><p>DNS Resolution is used to enable resolution of public DNS hostnames to private IP addresses when queried from the <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html">peered VPC</a>.</p>
<h2 id="👀-Traffic-Mirroring"><a href="#👀-Traffic-Mirroring" class="headerlink" title="👀 Traffic Mirroring"></a><code>👀</code> <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/vpc/latest/mirroring/what-is-traffic-mirroring.html">Traffic Mirroring</a></h2><p><code>Traffic Mirroring provides</code> the ability to create a copy of a packet flow to examine the contents of a packet. This feature is useful for <code>threat monitoring</code>, <code>content inspection</code>, and <code>troubleshooting</code>.</p>
<p>A packet is truncated to the <code>MTU</code> value when both of the following are true:</p>
<ul>
<li>The traffic <code>mirror target is a standalone instance</code>.</li>
<li>The traffic <code>packet size from the mirror source is greater</code> than the MTU size for the traffic mirror <code>target</code>.</li>
</ul>
<h2 id="Q-SAML-federation"><a href="#Q-SAML-federation" class="headerlink" title="*Q SAML federation"></a><code>*Q</code> <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html">SAML federation</a></h2><ul>
<li><code>SAML federation between AWS and</code> the corporate <code>Active Directory and mapping Active Directory groups to IAM groups</code> is the recommended way to make access more secure and streamlined.</li>
</ul>
<hr>
<h2 id="👀-IMPORTANT-NOTES-👀"><a href="#👀-IMPORTANT-NOTES-👀" class="headerlink" title="👀 IMPORTANT NOTES 👀"></a>👀 IMPORTANT NOTES 👀</h2><h3 id="By-default-Amazon-EC2-and-Amazon-VPC-use-the-IPv4-addressing-protocol"><a href="#By-default-Amazon-EC2-and-Amazon-VPC-use-the-IPv4-addressing-protocol" class="headerlink" title="By default, Amazon EC2 and Amazon VPC use the IPv4 addressing protocol"></a>By default, Amazon EC2 and Amazon VPC use the IPv4 addressing protocol</h3><p>Amazon EC2 and Amazon VPC support both the IPv4 and IPv6 addressing protocols. By default, Amazon EC2 and Amazon VPC use the IPv4 addressing protocol; you can’t disable this behavior. When you create a VPC, you must specify an IPv4 CIDR block (a range of private IPv4 addresses). You can optionally assign an IPv6 CIDR block to your VPC and subnets, and assign IPv6 addresses from that block to instances in your subnet.</p>
<h2 id="Dynamo-DB"><a href="#Dynamo-DB" class="headerlink" title="Dynamo DB"></a>Dynamo DB</h2><h3 id="Cross-Account-access"><a href="#Cross-Account-access" class="headerlink" title="Cross Account access"></a><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/glue/latest/dg/cross-account-access.html">Cross Account access</a></h3><p>When you export your DynamoDB tables from Account A to an S3 bucket in Account B, the objects are still owned by Account A. The AWS Identify Access Management (IAM) users in Account B can’t access the objects by default. The export function doesn’t write data with the access control list (ACL) bucket-owner-full-control. As a workaround to this object ownership issue, include the <code>PutObjectAcl</code> permission on all exported objects after the export is complete. This workaround grants access to all exported objects for the bucket owners in Account B.</p>
<h2 id="ClientConnections-Metric"><a href="#ClientConnections-Metric" class="headerlink" title="ClientConnections Metric"></a><code>ClientConnections</code> Metric</h2><p>To track the number of Amazon EC2 instances that are connected to a file system, you can monitor the Sum statistic of the ClientConnections metric. To calculate the average ClientConnections for periods greater than one minute, divide the sum by the number of minutes in the period.</p>
<h2 id="👀-AWS-Budgets"><a href="#👀-AWS-Budgets" class="headerlink" title="👀 AWS Budgets"></a>👀 AWS Budgets</h2><p>Give you the ability to <code>set custom budgets that alert you</code> when your costs or <code>usage exceed</code> (or are <code>forecasted</code> to exceed) your budgeted amount.</p>
<p>You can also use AWS Budgets to <code>set reservation utilization or coverage targets and receive alerts</code> when your utilization drops below the threshold you define. Reservation alerts are supported for <code>Amazon EC2, Amazon RDS, Amazon Redshift, Amazon ElastiCache, and Amazon Elasticsearch reservations</code>.</p>
<h2 id="👀-Q-AWS-Task-Orchestrator-and-Executor-AWSTOE"><a href="#👀-Q-AWS-Task-Orchestrator-and-Executor-AWSTOE" class="headerlink" title="👀 Q AWS Task Orchestrator and Executor (AWSTOE)"></a>👀 Q AWS Task Orchestrator and Executor (AWSTOE)</h2><p>Use the AWS Task Orchestrator and Executor (AWSTOE) application <code>to orchestrate complex workflows</code>, <code>modify system configurations</code>, and <code>test your systems without writing code</code>. This application uses a declarative document schema. Because it is a standalone application, it does not require additional server setup.</p>
<h2 id="👀-AWS-Cost"><a href="#👀-AWS-Cost" class="headerlink" title="👀 AWS Cost"></a>👀 AWS Cost</h2><p>In <code>AWS Cost</code> and <code>Usage Reports</code>, you can choose to have AWS <code>publish billing reports</code> to an <code>Amazon Simple Storage</code> Service (Amazon S3) bucket that you own. You can receive reports that break down your costs by the hour or month, by product or product resource, or by tags that you define yourself. AWS updates the report in your bucket once a day in a comma-separated value (CSV) format. You can view the reports using spreadsheet software such as Microsoft Excel or Apache OpenOffice Calc or access them from an application using the Amazon S3 API.</p>
<h2 id="AWS-Database-Migration-Service-DMS"><a href="#AWS-Database-Migration-Service-DMS" class="headerlink" title="AWS Database Migration Service (DMS)"></a>AWS Database Migration Service (DMS)</h2><h2 id="Amazon-Macie"><a href="#Amazon-Macie" class="headerlink" title="Amazon Macie"></a>Amazon Macie</h2><p>is a fully managed data security and data privacy service that uses <code>machine learning</code> and pattern matching to help you discover, monitor, and protect sensitive data in your AWS environment. Macie automates the discovery of sensitive data, such as <code>personally identifiable information (PII)</code> and financial data, to provide you with a better understanding of the data that your organization stores in Amazon S3.
Amazon Macie <code>only supports S3 as a data source</code>.</p>
<h2 id="👀-Run-Command"><a href="#👀-Run-Command" class="headerlink" title="👀 Run Command"></a>👀 Run Command</h2><h2 id="👀EC2Rescue"><a href="#👀EC2Rescue" class="headerlink" title="👀EC2Rescue"></a><code>👀</code><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-ec2rescue.html">EC2Rescue</a></h2><p>EC2Rescue can help you diagnose and troubleshoot problems on Amazon EC2 Linux and Windows Server instances. You can run the tool manually, as described in Using EC2Rescue for Linux Server and Using EC2Rescue for Windows Server. Or, you can run the tool automatically by using Systems Manager Automation and the AWSSupport-ExecuteEC2Rescue document. The AWSSupport-ExecuteEC2Rescue document is designed to perform a combination of Systems Manager actions, AWS CloudFormation actions, and Lambda functions that automate the steps normally required to use EC2Rescue.</p>
<p>EC2Rescue for EC2 Windows is a convenient, straightforward, GUI-based troubleshooting tool that can be run on your Amazon EC2 Windows Server instances to troubleshoot operating system-level issues and collect advanced logs and configuration files for further analysis. EC2Rescue simplifies and expedites the troubleshooting of EC2 Windows instances.</p>
<h2 id="Service-Control-Policies"><a href="#Service-Control-Policies" class="headerlink" title="Service Control Policies"></a>Service Control Policies</h2><h3 id="Groups"><a href="#Groups" class="headerlink" title="Groups"></a>Groups</h3><p>Can be granted permissions using access control policies - Groups can be granted permissions using access control policies. This makes it easier to manage permissions for a collection of users, rather than having to manage permissions for each user.<a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_groups.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_groups.html</a></p>
<hr>
<p>Activate a <code>cost allocation</code> tag that is named Department in the <code>AWS Billing</code> and <code>Cost Management console in the Organizations management account</code>. Use a tag policy to mandate a Department tag on new resources.</p>
<p>Correct. You must activate a tag in the Billing and Cost Management console before viewing the expense by cost allocation tag. You should mandate the use of tags to ensure that the resources are tagged correctly.</p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_tag-policies.html">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_tag-policies.html</a></p>
<hr>
<p>Use the <code>AWS Resource Groups</code> Tag Editor to identify resources that are not tagged in each account. Apply a tag that is named Department to any untagged resources.</p>
<p>With Resource Groups, you can create, maintain, and view a collection of resources that share common tags. Tag Editor manages tags across services and AWS Regions. Tag Editor can perform a global search and can edit a large number of tags at one time.</p>
<p>For more information about resource groups and tagging, see Tag Editor.</p>
<h2 id="👀-Billing-Preferences"><a href="#👀-Billing-Preferences" class="headerlink" title="👀 Billing Preferences"></a>👀 Billing Preferences</h2><p>The management account of an organization can change this setting by turning off <code>RI</code> (Reserved Instances) sharing for <code>an individual member account</code> the more suitable service is AWS WAF</p>
<h2 id="👀-AWS-Shield-Advanced"><a href="#👀-AWS-Shield-Advanced" class="headerlink" title="👀 AWS Shield Advanced"></a>👀 AWS Shield Advanced</h2><p>is more suitable to be used against <code>distributed denial of service (DDoS</code>) attacks but not for common web exploits such as <code>cross-site scripting</code>, <code>SQL injection</code>, and <code>brute-force HTTP flood attacks</code>.</p>
<h2 id="👀-A-placement-group"><a href="#👀-A-placement-group" class="headerlink" title="👀 A placement group"></a>👀 A placement group</h2><p>is a logical <code>grouping</code> of <code>instances</code> <code>within</code> a <code>single Availability Zone</code>. By placing the EC2 instances in a placement group, you can ensure that the instances are physically located close to each other, which can significantly <code>reduce network latency between them</code>. This can <code>improve the performance</code> of inter-instance communication and reduce the overall latency in data transfer.</p>
<p>Sometimes you want control over the EC2 Instance placement strategy, When you create a placement group, you specify one of the following strategies for the group:</p>
<ul>
<li><code>Cluster—clusters</code> instances into a <code>low-latency</code> group in a <code>single Availability Zone</code>.<ul>
<li><code>Pros</code>: Great network (10 Gbps bandwidth between instances with Enhanced Networking enabled - recommended)</li>
<li><code>Cons</code>: If the rack fails, all instances fails at the same time</li>
<li><code>Use</code> case: Big Data job that needs to complete fast</li>
</ul>
</li>
<li><code>Spread—spreads</code> instances across underlying hardware (max 7 instances per group per AZ) – critical applications<ul>
<li><code>Pros</code>:<ul>
<li>Can span across Availability Zones (AZ)</li>
<li>Reduced risk is simultaneous failure</li>
<li>EC2 Instances are on different  physical hardware</li>
</ul>
</li>
<li><code>Cons</code>:<ul>
<li>Limited to 7 instances per AZ per placement group</li>
</ul>
</li>
<li><code>Use case</code>:<ul>
<li>Application that needs to maximize high availability</li>
<li>Critical Applications where each instance must be isolated from failure from each other</li>
</ul>
</li>
</ul>
</li>
<li><code>Partition—spreads</code> instances across many different partitions (which rely on different sets of racks) within an AZ. Scales to 100s of EC2 instances per group (Hadoop, Cassandra, Kafka)<ul>
<li>Up to 7 partitions per AZ  Can span across multiple AZs in the same region</li>
<li>Up to 100s of EC2 instances</li>
<li>The instances in a partition do not share racks with the instances in the other partitions</li>
<li>A partition failure can affect many EC2 but won’t affect other partitions</li>
<li>EC2 instances get access to the partition information as metadata</li>
<li>Use cases: HDFS, HBase, Cassandra, Kafka</li>
</ul>
</li>
</ul>
<h2 id="👀-Network-Firewall"><a href="#👀-Network-Firewall" class="headerlink" title="👀 Network Firewall"></a>👀 Network Firewall</h2><p>You can use Network Firewall to monitor and protect your Amazon VPC traffic in a number of ways, including the following:
– Pass traffic through only from known AWS service domains or IP address endpoints, such as Amazon S3.
– Use custom <code>lists of known bad domains to limit the types of domain names that your applications can access</code>.
– Perform <code>deep packet inspection</code> DPI on traffic entering or leaving your VPC.
– Use stateful protocol detection to filter protocols like HTTPS, independent of the port used.</p>
<h2 id="👀-Access-Analyzer"><a href="#👀-Access-Analyzer" class="headerlink" title="👀 Access Analyzer"></a>👀 Access Analyzer</h2><p>helps you identify the resources in your organization and accounts, such as Amazon S3 buckets or IAM roles, <code>shared with an external entity</code>. This lets you identify unintended access to your resources and data, which is a security risk. Access Analyzer <code>identifies resources shared</code> with <code>external principals</code> by using logic-based reasoning to analyze the resource-based policies in your AWS environment. For each instance of a resource shared outside of your account, Access Analyzer generates a finding.</p>
<h2 id="👀-Tags-with-AWS-Organizations"><a href="#👀-Tags-with-AWS-Organizations" class="headerlink" title="👀 Tags with AWS Organizations"></a>👀 Tags with AWS Organizations</h2><ul>
<li><p>Use <code>AWS Service Catalog</code> to tag the provisioned resources with corresponding unique identifiers for portfolio, product, and users:
AWS <code>Service Catalog</code> allows you to create and manage catalogs of IT services that are approved for use within your <code>organization</code>. When provisioning resources through AWS Service Catalog, you can define <code>tags</code> that are automatically applied to the provisioned resources. By leveraging this feature, you can ensure consistent tagging across different accounts and resources.</p>
</li>
<li><p>Use <code>AWS Systems Manager</code> <strong><code>Automation</code></strong> to automatically add <code>tags</code> to your <code>provisioned</code> resources:
AWS &#96;&#96;Systems Manager Automation provides a way to automate common operational tasks across AWS resources. You can create automation workflows that include tagging resources with the desired set of tags. By utilizing Systems Manager Automation, you can enforce consistent tagging practices during resource creation.</p>
</li>
</ul>
<h2 id="AWS-System-Manager"><a href="#AWS-System-Manager" class="headerlink" title="AWS System Manager"></a>AWS System Manager</h2><p>👀 AWS Systems Manager provides a unified, centralized way to manage both your Amazon EC2 instances and on-premises servers (including <code>Raspbian</code> systems, devices such as <code>Raspberry Pi</code> through a single interface). It offers a wide range of capabilities, including <code>inventory management</code>, <code>patch management</code>, <code>automation</code>, and <code>configuration management</code>, allowing you to efficiently manage your hybrid infrastructure from a single console. With Systems Manager, you can maintain consistent configurations, apply patches, and automate administrative tasks for your on-premises servers, just like you would for your EC2 instances.</p>
<p>TODO
gives you visibility and control of your infrastructure on AWS. Systems Manager provides a unified user interface so you can view operational data from multiple AWS services and allows you to automate operational tasks across your AWS resources. With Systems Manager, you can group resources, like Amazon EC2 instances, Amazon S3 buckets, or Amazon RDS instances, by application, view operational data for monitoring and troubleshooting, and take action on your groups of resources. Systems Manager simplifies resource and application management, shortens the time to detect and resolve operational problems, and makes it easy to operate and manage your infrastructure securely at scale.</p>
<ul>
<li>Helps you manage your <strong>EC2</strong> and <strong>On-Premises</strong> systems at scale.</li>
<li>Get operational insights about the state of your infrastructure.</li>
<li>Easily detect problems.</li>
<li><strong>Patching automation for enhanced compliance</strong>.</li>
<li>Works for both Windows and Linux OS.</li>
<li><code>Integrated with CloudWatch metrics / dashboards</code>.</li>
<li><code>Integrated with AWS Config</code>.</li>
<li>Free service.</li>
</ul>
<h3 id="Main-Features-for-the-Examen"><a href="#Main-Features-for-the-Examen" class="headerlink" title="Main Features for the Examen"></a>Main Features for the Examen</h3><ul>
<li>Resource Groups</li>
<li>Shared Resources
Documents</li>
<li>Change Management<ul>
<li>Automation</li>
<li>Maintenance Windows</li>
</ul>
</li>
<li>Application Management<ul>
<li>Parameter Store</li>
</ul>
</li>
<li>Node Management<ul>
<li>Inventory</li>
<li>Session Manager</li>
<li>Run Command</li>
<li>State Manager</li>
<li>Patch Manager</li>
</ul>
</li>
</ul>
<h3 id="👀-Fleet-Manager"><a href="#👀-Fleet-Manager" class="headerlink" title="👀 Fleet Manager"></a>👀 <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/systems-manager/latest/userguide/fleet.html">Fleet Manager</a></h3><p>Helps you remotely <code>manage</code> your <code>server fleet</code> that runs on <code>AWS</code> or on <code>premises</code>. With Fleet Manager, you can gather data from individual instances to perform common troubleshooting and management tasks from a single console. However, you cannot use Fleet Manager to upload a script to start or stop instances.</p>
<h3 id="Recover-impaired-instances"><a href="#Recover-impaired-instances" class="headerlink" title="Recover impaired instances"></a>Recover impaired instances</h3><p>A Systems Manager <code>Automation document</code> defines the <code>Automation workflow</code> (the actions that Systems Manager performs on your managed instances and AWS resources). Automation includes several pre-defined Automation documents that you can use to perform common tasks like restarting one or more EC2 instances or creating an Amazon Machine Image (AMI).</p>
<p>Use the <code>AWSSupport-ExecuteEC2Rescue</code> document to recover impaired instances.</p>
<h3 id="👀-AWS-Systems-Manager-Inventory"><a href="#👀-AWS-Systems-Manager-Inventory" class="headerlink" title="👀 AWS Systems Manager Inventory"></a>👀 AWS Systems Manager Inventory</h3><p>AWS Systems Manager Inventory provides visibility into your <code>Amazon EC2</code> and <em><code>on-premises</code></em> computing environment. You can use Inventory to <code>collect metadata</code> from your <code>managed instances</code>. You can store this metadata in a central Amazon Simple Storage Service (Amazon S3) bucket, and then use built-in tools to query the data and quickly determine which instances are running the software and configurations required by your software policy, and which instances need to be updated. You can configure Inventory on all of your managed instances by using a one-click procedure. You can also configure and view inventory data from multiple AWS Regions and AWS accounts.</p>
<p>If the pre-configured metadata types collected by Systems Manager Inventory don’t meet your needs, then you can create custom inventory. <code>Custom inventory</code> is simply a JSON file with information that you provide and add to the managed instance in a specific directory. When Systems Manager Inventory collects data, it captures this custom inventory data.</p>
<p>Systems Manager Inventory collects only metadata from your managed instances. Inventory doesn’t access proprietary information or data.</p>
<h3 id="AWS-Tags"><a href="#AWS-Tags" class="headerlink" title="AWS Tags"></a>AWS Tags</h3><ul>
<li>You can add text key-value pairs called Tags to many AWS resources</li>
<li>Commonly used in EC2</li>
<li>Free naming, common tags are Name, Environment, Team …</li>
<li>They’re used for<ul>
<li>Resource grouping</li>
<li>Automation</li>
<li>Cost allocation</li>
</ul>
</li>
</ul>
<h3 id="Resource-Groups"><a href="#Resource-Groups" class="headerlink" title="Resource Groups"></a>Resource Groups</h3><ul>
<li>Create, view or manage logical group of resources thanks to <strong>tags</strong>.</li>
<li>Allows creation of logical groups of resources such as<ul>
<li>Applications</li>
<li>Different layers of an application stack</li>
<li>Production versus development environments</li>
</ul>
</li>
<li>Regional service</li>
<li>Works with EC2, S3, DynamoDB, Lambda, etc…</li>
</ul>
<h3 id="SSM-–-Inventory"><a href="#SSM-–-Inventory" class="headerlink" title="SSM – Inventory"></a>SSM – Inventory</h3><ol>
<li>Collect metadata from your managed instances (EC2&#x2F;On-premises)</li>
<li>Metadata includes installed software, OS drivers, configurations, installed updates, running services …</li>
<li>View data in AWS Console or store in S3 and query and analyze using Athena and QuickSight</li>
<li>Specify metadata collection interval (minutes, hours, days)</li>
<li>Query data from multiple AWS accounts and regions</li>
<li>Create Custom Inventory for your custom metadata (e.g., rack location of each managed instance)</li>
</ol>
<h2 id="Scalability-amp-High-Availability"><a href="#Scalability-amp-High-Availability" class="headerlink" title="Scalability &amp; High Availability"></a>Scalability &amp; High Availability</h2><p>Scalability means that an application &#x2F; system can handle greater loads by adapting.</p>
<p><strong>Scalability is linked but different to High Availability</strong></p>
<h3 id="Vertical-Scalability"><a href="#Vertical-Scalability" class="headerlink" title="Vertical Scalability"></a>Vertical Scalability</h3><p>Vertically scalability means increasing the size of the resource (instance)</p>
<h3 id="Horizontal-Scalability"><a href="#Horizontal-Scalability" class="headerlink" title="Horizontal Scalability"></a>Horizontal Scalability</h3><p>Horizontal Scalability means increasing the number of instances &#x2F; systems for your application</p>
<h3 id="High-Availability-amp-Scalability-For-EC2"><a href="#High-Availability-amp-Scalability-For-EC2" class="headerlink" title="High Availability &amp; Scalability For EC2"></a>High Availability &amp; Scalability For EC2</h3><ul>
<li>Vertical Scaling: Increase instance size (&#x3D; scale up &#x2F; down)<ul>
<li>From: t2.nano - 0.5G of RAM, 1 vCPU</li>
<li>To: u-12tb1.metal – 12.3 TB of RAM, 448 vCPUs</li>
</ul>
</li>
<li>Horizontal Scaling: Increase number of instances (&#x3D; scale out &#x2F; in)<ul>
<li>Auto Scaling Group</li>
<li>Load Balancer</li>
</ul>
</li>
<li>High Availability: Run instances for the same application across multi-AZ<ul>
<li>Auto Scaling Group multi-AZ</li>
<li>Load Balancer multi-AZ</li>
</ul>
</li>
</ul>
<h2 id="Gateway-Load-Balancer"><a href="#Gateway-Load-Balancer" class="headerlink" title="Gateway Load Balancer"></a>Gateway Load Balancer</h2><p>Uses the <strong>GENEVE</strong> protocol on port <strong>6081</strong></p>
<h2 id="Application-Load-Balancers"><a href="#Application-Load-Balancers" class="headerlink" title="Application Load Balancers"></a>Application Load Balancers</h2><h3 id="Monitoring"><a href="#Monitoring" class="headerlink" title="Monitoring"></a>Monitoring</h3><ul>
<li><code>**RequestCountPerTarget**</code></li>
<li><code>**SurgeQueueLength**</code>: The total of requests (HTTP listener) or connections (TCP listener) that are pending routing to a healthy instance. Help to scale out ASG. Max value is 1024</li>
<li><code>👀</code> <code>SpilloverCount</code> represents the total <code>number of requests</code> that were <code>rejected</code> <code>because</code> the surge <code>queue</code> is <code>full</code>.
To solve this use-case, you need to configure the Auto Scaling groups to scale your instances based on the <code>SurgeQueueLength</code> metric.</li>
</ul>
<h3 id="Target-Groups-Settings"><a href="#Target-Groups-Settings" class="headerlink" title="Target Groups Settings"></a>Target Groups Settings</h3><ul>
<li><code>deregisteration_delay.timeout_seconds</code>: time the load balancer waits before deregistering a target.</li>
<li><code>slow_start.duration_seconds</code>: (see next slide).</li>
<li><code>load_balancing.algorithm.type</code>: how the load balancer selects targets when routing requests (Round Robin, Least Outstanding Requests).</li>
<li><code>stickiness.enabled</code>.</li>
<li><code>stickiness.type</code>: application-based or duration-based cookie.</li>
<li><code>stickiness.app_cookie.cookie_name</code>: name of the application cookie.</li>
<li><code>stickiness.app_cookie.duration_seconds</code>: application-based cookie expiration period.</li>
<li><code>stickiness.lb_cookie.duration_seconds</code>: duration-based cookie expiration period.</li>
</ul>
<h2 id="ASG"><a href="#ASG" class="headerlink" title="ASG"></a>ASG</h2><h3 id="Good-metrics-to-scale-on"><a href="#Good-metrics-to-scale-on" class="headerlink" title="Good metrics to scale on"></a>Good metrics to scale on</h3><ul>
<li>CPUUtilization: Average CPU utilization across your instances</li>
<li>RequestCountPerTarget: to make sure the number of requests per EC2 instances is stable</li>
<li>Average Network In &#x2F; Out (if you’re application is network bound)</li>
<li>Any custom metric (that you push using CloudWatch)</li>
</ul>
<p>advice
exam</p>
<p>ApproximateNumberOfMessages</p>
<h2 id="AWS-Auto-Scaling"><a href="#AWS-Auto-Scaling" class="headerlink" title="AWS Auto Scaling"></a>AWS Auto Scaling</h2><p>Backbone service of auto scaling for scalable resources in AWS:</p>
<ul>
<li><code>Amazon EC2 Auto Scaling groups</code>: Launch or terminate EC2 instances</li>
<li><code>Amazon EC2 Spot Fleet requests</code>: Launch or terminate instances from a Spot Fleet request, or automatically replace instances that get interrupted for price or capacity reasons.</li>
<li><code>Amazon ECS</code>: Adjust the ECS service desired count up or down</li>
<li><code>Amazon DynamoDB</code> (table or global secondary index):WCU &amp; RCU</li>
<li><code>Amazon Aurora</code>: Dynamic Read Replicas Auto Scaling</li>
</ul>
<p>Application Load Balancer (v2)
Target Groups</p>
<ul>
<li><p>EC2 instances (can be managed by an Auto Scaling Group) – HTTP</p>
</li>
<li><p>ECS tasks (managed by ECS itself) – HTTP</p>
</li>
<li><p>Lambda functions – HTTP request is translated into a JSON event</p>
</li>
<li><p>IP Addresses – must be private IPs</p>
</li>
<li><p>ALB can route to multiple target groups</p>
</li>
<li><p>Health checks are at the target group level</p>
</li>
</ul>
<h2 id="UpdatePolicy-Attribute"><a href="#UpdatePolicy-Attribute" class="headerlink" title="UpdatePolicy Attribute"></a>UpdatePolicy Attribute</h2><p>Use it to handle updates for below resources</p>
<ul>
<li><code>AWS::AppStream::Fleet</code></li>
<li><code>AWS::AutoScaling::AutoScalingGroup</code></li>
<li><code>AWS::ElastiCache::ReplicationGroup</code></li>
<li><code>AWS::OpenSearchService::Domain</code></li>
<li><code>AWS::Elasticsearch::Domain</code></li>
<li><code>AWS::Lambda::Alias</code></li>
</ul>
<h3 id="AutoScalingReplacingUpdate-policy-EXAM"><a href="#AutoScalingReplacingUpdate-policy-EXAM" class="headerlink" title="AutoScalingReplacingUpdate policy - *EXAM"></a>AutoScalingReplacingUpdate policy - <code>*EXAM</code></h3><h3 id="AutoScalingRollingUpdate-policy"><a href="#AutoScalingRollingUpdate-policy" class="headerlink" title="AutoScalingRollingUpdate policy"></a><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html">AutoScalingRollingUpdate policy</a></h3><p>With rolling updates, you can specify whether CloudFormation performs updates in batches or all at once for instances that are in an Auto Scaling group. The <code>AutoScalingRollingUpdate</code> policy is the only CloudFormation feature that provides such an incremental update throughout the Auto Scaling group.</p>
<h3 id="AutoScalingScheduledAction-policy"><a href="#AutoScalingScheduledAction-policy" class="headerlink" title="AutoScalingScheduledAction policy"></a>AutoScalingScheduledAction policy</h3><p>Applies when you update a stack that includes an Auto Scalling group with an associated scheduled action.</p>
<h3 id="CloudFormation-StackSets"><a href="#CloudFormation-StackSets" class="headerlink" title="CloudFormation StackSets"></a>CloudFormation StackSets</h3><ul>
<li>Create, update, or delete stacks across multiple accounts and regions with a single operation</li>
<li>Administrator account to create StackSets</li>
<li>Trusted accounts to create, update, delete stack instances from StackSets</li>
</ul>
<h3 id="Lambda-Tracing-with-X-Ray"><a href="#Lambda-Tracing-with-X-Ray" class="headerlink" title="Lambda Tracing with X-Ray"></a>Lambda Tracing with X-Ray</h3><ul>
<li>Enable in Lambda configuration (<code>Active Tracing</code>)</li>
<li>Environment variables to communicate with X-Ray<ul>
<li>_X_AMZN_TRACE_ID: contains the tracing header</li>
<li>AWS_XRAY_CONTEXT_MISSING: by default, LOG_ERROR</li>
<li><code>AWS_XRAY_DAEMON_ADDRESS</code>: the X-Ray Daemon IP_ADDRESS:PORT</li>
</ul>
</li>
</ul>
<h2 id="Lambda-Function-Configuration"><a href="#Lambda-Function-Configuration" class="headerlink" title="Lambda Function Configuration"></a>Lambda Function Configuration</h2><ul>
<li>RAM
-The more RAM you add, the more vCPU credits you get<ul>
<li>At 1,792 MB, a function has the equivalent of one full vCPU</li>
</ul>
</li>
</ul>
<ul>
<li><code>If your application is CPU-bound (computation heavy), increase RAM</code> - <code>*EXAM</code></li>
<li><code>Timeout</code>: default 3 seconds, maximum is 900 seconds (15 minutes)</li>
</ul>
<p><em>Cold Starts &amp; Provisioned Concurrency</em></p>
<ul>
<li><code>Cold Start</code>:<ul>
<li>New instance &#x3D;&gt; code is loaded and code outside the handler run (init)</li>
<li>If the init is large (code, dependencies, SDK…) this process can take some time.</li>
<li>First request served by new instances has higher latency than the rest</li>
</ul>
</li>
<li><code>Provisioned Concurrency</code>:<ul>
<li>Concurrency is allocated before the function is invoked (in advance)</li>
<li>So the cold start never happens and all invocations have low latency</li>
<li>Application Auto Scaling can manage concurrency (schedule or target utilization)</li>
</ul>
</li>
<li>Note:<ul>
<li>Note: cold starts in VPC have been dramatically reduced in Oct &amp; Nov 2019</li>
<li><a target="_blank" rel="noopener" href="https://aws.amazon.com/blogs/compute/announcing-improved-vpc-networking-for-aws-lambda-functions/">https://aws.amazon.com/blogs/compute/announcing-improved-vpc-networking-for-aws-lambda-functions/</a></li>
</ul>
</li>
</ul>
<p>Lambda Monitoring – CloudWatch Metrics</p>
<ul>
<li><code>Invocations</code> – number of times your function is invoked (success&#x2F;failure)</li>
<li><code>Duration</code> – amount of time your function spends processing an event</li>
<li><code>Errors</code> – number of invocations that result in a function error</li>
<li><code>Throttles</code> – number of invocation requests that are throttled (no concurrency available)</li>
<li><code>DeadLetterErrors</code> – number of times Lambda failed to send an event to a DLQ (async invocations)</li>
<li><code>IteratorAge</code> – time between when a Stream receives a record and when the Event Source Mapping sends the event to the function (for Event Source Mapping that reads from Stream)</li>
<li><code>ConcurrentExecutions</code> – number of function instances that are processing events</li>
</ul>
<hr>
<h2 id="Review"><a href="#Review" class="headerlink" title="Review"></a>Review</h2><p>The cfn-signal helper script signals AWS CloudFormation to indicate whether Amazon EC2 instances have been successfully created or updated. If you install and configure software applications on instances, you can signal AWS CloudFormation when those software applications are ready.</p>
<p>You use the cfn-signal script in conjunction with a CreationPolicy or an Auto Scaling group with a WaitOnResourceSignals update policy. When AWS CloudFormation creates or updates resources with those policies, it suspends work on the stack until the resource receives the requisite number of signals or until the timeout period is exceeded. For each valid signal that AWS CloudFormation receives, AWS CloudFormation publishes the signals to the stack events so that you track each signal.</p>
<p> maximum coverage. It’s calculated by dividing Reserved Instance used hours by total EC2 On-Demand and Reserved Instance hours.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://blog.pablo-magro-gaspar.site/2023/05/15/AWS-Certified-SysOps-Administrator-Associate-SOA-C02/" data-id="cljtprqzo0004fbop93j06gye" class="article-share-link">Share</a>
      
        <a href="https://blog.pablo-magro-gaspar.site/2023/05/15/AWS-Certified-SysOps-Administrator-Associate-SOA-C02/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Certifications/" rel="tag">Certifications</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/SOA-C02/" rel="tag">SOA-C02</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/SysOps/" rel="tag">SysOps</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-AWS-Certified-Solutions-Architect-Certification-SAA-C02" class="article article-type-post"
  itemscope itemtype="http://schema.org/Blog" id="post-hexo-tags-structureddata-keywords" class="article article-type-post" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/05/13/AWS-Certified-Solutions-Architect-Certification-SAA-C02/" class="article-date">
  <time datetime="2020-05-13T10:55:28.000Z" itemprop="datePublished">13 May 2020</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/AWS/">AWS</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/05/13/AWS-Certified-Solutions-Architect-Certification-SAA-C02/">AWS Certified Solutions Architect Certification SAA-C02</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="AWS-Account-Activation-Troubleshooting"><a href="#AWS-Account-Activation-Troubleshooting" class="headerlink" title="AWS Account Activation Troubleshooting"></a>AWS Account Activation Troubleshooting</h2><p>Ensuring your AWS Account is activated (please read)</p>
<p>Before proceeding with the course, you need to make sure your account is activated</p>
<p>The activation email looks like this (the content might be slightly different for you):</p>
<p>Note: The text might be slightly different for you, but as long as AWS says thank you, or welcome, and tells you to start using your account, it should mean it’s activated.</p>
<p>Example: A student received “Thank you for creating an Amazon Web Services (AWS) account. For the next 12 months, you will have free access to compute, storage, database, and application services. Learn more by visiting our Free Tier page. To access your account, click Access Account”</p>
<hr>
<p>To have your account activated, make sure to:</p>
<p>Add a Payment Method</p>
<p>Verify your phone number</p>
<p>Choose an AWS Support Plan (Free)</p>
<p>If your account is not activated yet, you will see this kind of error messages in the next lecture:</p>
<p>How to get an account activation email? (can take up to 24 hours, usually few minutes)</p>
<p>After you choose a Support plan, a confirmation page indicates that your account is being activated. Accounts are usually activated within a few minutes, but the process might take up to 24 hours.</p>
<p>You can sign in to your AWS account during this time. The AWS home page might display a button that shows “Complete Sign Up” during this time, even if you’ve completed all the steps in the sign-up process.</p>
<p>When your account is fully activated, you’ll receive a confirmation email. After you receive this email, you have full access to all AWS services.</p>
<p>Troubleshooting delays in account activation</p>
<p>Account activation can sometimes be delayed. If the process takes more than 24 hours, check the following:</p>
<p>Finish the account activation process. You might have accidentally closed the window for the sign-up process before you’ve added all the necessary information. To finish the sign-up process, open <a target="_blank" rel="noopener" href="https://aws-portal.amazon.com/gp/aws/developer/registration/index.html">https://aws-portal.amazon.com/gp/aws/developer/registration/index.html</a> and sign in using the email address and password you chose for the account.</p>
<p>Check the information associated with your payment method. Check Payment Methods in the AWS Billing and Cost Management console. Fix any errors in the information.</p>
<p>Contact your financial institution. Financial institutions occasionally reject authorization requests from AWS for various reasons. Contact your payment method’s issuing institution and ask that they approve authorization requests from AWS.
Note: AWS cancels the authorization request as soon as it’s approved by your financial institution. You aren’t charged for authorization requests from AWS. Authorization requests might still appear as a small charge (usually 1 USD) on statements from your financial institution.</p>
<p>Check your email for requests for additional information. Check your email to see if AWS needs any information from you to complete the activation process.</p>
<p>Try a different browser.</p>
<p>Contact AWS Support. Contact AWS Support for help. Be sure to mention any troubleshooting steps that you already tried.
Note: Don’t provide sensitive information, such as credit card numbers, in any correspondence with AWS.</p>
<p>How to Contact AWS Support?</p>
<p>The top right corner will have Support &gt; Support Center.</p>
<p>Ask them to activate the account, that can take up to 24 hours</p>
<p>Where to find more details?</p>
<p>Details can be found here: <a target="_blank" rel="noopener" href="https://aws.amazon.com/premiumsupport/knowledge-center/create-and-activate-aws-account/">https://aws.amazon.com/premiumsupport/knowledge-center/create-and-activate-aws-account/</a></p>
<h2 id="Section-3-AWS-Fundamentals-IAM-amp-EC2"><a href="#Section-3-AWS-Fundamentals-IAM-amp-EC2" class="headerlink" title="Section 3: AWS Fundamentals: IAM &amp; EC2"></a>Section 3: AWS Fundamentals: IAM &amp; EC2</h2><h3 id="11-AWS-Regions-and-AZs"><a href="#11-AWS-Regions-and-AZs" class="headerlink" title="11. AWS Regions and AZs"></a>11. AWS Regions and AZs</h3><ul>
<li>AWS has Regions all around the world.</li>
<li>Names can be: us-east-1, eu-west-3…</li>
<li>A region is a cluster of data centers.</li>
<li>Most AWS services are region-scoped.</li>
</ul>
<p><strong>AWS Availability Zones</strong></p>
<ul>
<li><p>Each region has many availability zones (usually 3, min is 2, max is 6). Example:</p>
<ul>
<li>ap-southeast-2a</li>
<li>ap-southeast-2b</li>
<li>ap-southeast-2c</li>
</ul>
</li>
<li><p>Each availability zone (AZ) is one or more discrete data centers with redundant power, networking, and connectivity.</p>
</li>
<li><p>They’re separate from each other, so that they’re isolated from disasters.</p>
</li>
<li><p>They’re connected with high bandwidth, ultra-low latency networking.</p>
<p>This helps guarantee that multi AZ won’t all fail at once (due to a meteorological disaster for example). Read more here: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html</a></p>
</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/about-aws/global-infrastructure/">https://aws.amazon.com/about-aws/global-infrastructure/</a></p>
<h3 id="IAM-Introduction"><a href="#IAM-Introduction" class="headerlink" title="IAM Introduction"></a>IAM Introduction</h3><ul>
<li><p>IAM (Identity and Access Management)</p>
</li>
<li><p>Your whole AWS security is there:</p>
<ul>
<li>Users</li>
<li>Groups</li>
<li>Roles</li>
</ul>
</li>
<li><p>Root account should never be used (and shared)</p>
</li>
<li><p>Users must be created with proper permissions.</p>
</li>
<li><p>IAM is at the center of AWS.</p>
</li>
<li><p>Policies are written in JSON (JavaScript Object Notation.</p>
</li>
<li><p>IAM has a global view.</p>
</li>
<li><p>Permissions are governed by Policies (JSON).</p>
</li>
<li><p>MFA (Multi Factor Authentication) can be setup. ***</p>
</li>
<li><p>IAM has predefined “managed policies”.</p>
</li>
<li><p>We’ll see IAM policies in details in the future.</p>
</li>
<li><p>It’s best to give users the minimal amount of permissions they need to perform their job (least privilege principles).</p>
<p>IAM is a global service (encompasses all regions)</p>
<p>Q: You are getting started with AWS and your manager wants things to remain simple yet secure. He wants the management of engineers to be easy, and not re-invent the wheel every time someone joins your company. What will you do?
A: I’ll create multiple IAM users and groups, and assign policies to groups. New users will be added to groups</p>
</li>
</ul>
<p><strong>IAM Federation</strong></p>
<ul>
<li>Big enterprises usually integrate their own repository of users with IAM</li>
<li>This way, one can login into AWS using their company credentials</li>
<li>Identity Federation uses the SAML standard (Active Directory)</li>
</ul>
<p><strong>IAM 101 Brain Dump</strong></p>
<ul>
<li>One IAM User per PHYSICAL PERSON</li>
<li>One IAM Role per Application</li>
<li>IAM credentials should NEVER BE SHARED</li>
<li>Never, ever, ever, ever, write IAM credentials in code. EVER.</li>
<li>And even less, NEVER EVER EVER COMMIT YOUR IAM credentials</li>
<li>Never use the ROOT account except for initial setup.</li>
<li>Never use ROOT IAM Credentials</li>
</ul>
<h3 id="What-is-EC2"><a href="#What-is-EC2" class="headerlink" title="What is EC2?"></a>What is EC2?</h3><ul>
<li>EC2 is one of most popular of AWS offering</li>
<li>It mainly consists in the capability of :</li>
<li>Renting virtual machines (EC2)</li>
<li>Storing data on virtual drives (EBS)</li>
<li>Distributing load across machines (ELB)</li>
<li>Scaling the services using an auto-scaling group (ASG)</li>
<li>Knowing EC2 is fundamental to understand how the Cloud works</li>
</ul>
<p><strong>Hands-On:</strong>
Launching an EC2 Instance running Linux</p>
<ul>
<li>We’ll be launching our first virtual server using the AWS Console</li>
<li>We’ll get a first high level approach to the various parameters</li>
<li>We’ll learn how to start &#x2F; stop &#x2F; terminate our instance.</li>
</ul>
<p><strong>Add Tags</strong>
Define pairs, to identify the instances.</p>
<p><strong>Security Group</strong>
Define the firewall around the instances</p>
<h2 id="How-to-SSH-into-your-EC2-Instance"><a href="#How-to-SSH-into-your-EC2-Instance" class="headerlink" title="How to SSH into your EC2 Instance"></a>How to SSH into your EC2 Instance</h2><p>Linux &#x2F; Mac OS X</p>
<ul>
<li>We’ll learn how to SSH into your EC2 instance using Linux &#x2F; Mac.</li>
<li>SSH is one of the most important function. It allows you to control a remote machine, all using the command line.</li>
<li>We will see how we can configure OpenSSH ~&#x2F;.ssh&#x2F;config to facilitate the SSH into our EC2 instances.</li>
</ul>
<p>Public DNS (IPv4)
ec2-54-153-150-4.ap-southeast-2.compute.amazonaws.com
IPv4 Public IP 54.153.150.4</p>
<p>ssh <a href="mailto:&#101;&#x63;&#50;&#x2d;&#117;&#x73;&#101;&#114;&#64;&#x65;&#x63;&#50;&#45;&#x35;&#x34;&#x2d;&#x31;&#x35;&#x33;&#45;&#x31;&#x35;&#x30;&#45;&#52;&#x2e;&#97;&#x70;&#45;&#x73;&#x6f;&#x75;&#116;&#x68;&#x65;&#x61;&#115;&#116;&#45;&#x32;&#46;&#x63;&#111;&#109;&#112;&#x75;&#x74;&#x65;&#46;&#97;&#x6d;&#97;&#x7a;&#x6f;&#x6e;&#97;&#119;&#115;&#46;&#x63;&#111;&#x6d;">&#101;&#x63;&#50;&#x2d;&#117;&#x73;&#101;&#114;&#64;&#x65;&#x63;&#50;&#45;&#x35;&#x34;&#x2d;&#x31;&#x35;&#x33;&#45;&#x31;&#x35;&#x30;&#45;&#52;&#x2e;&#97;&#x70;&#45;&#x73;&#x6f;&#x75;&#116;&#x68;&#x65;&#x61;&#115;&#116;&#45;&#x32;&#46;&#x63;&#111;&#109;&#112;&#x75;&#x74;&#x65;&#46;&#97;&#x6d;&#97;&#x7a;&#x6f;&#x6e;&#97;&#119;&#115;&#46;&#x63;&#111;&#x6d;</a>
ssh <a href="mailto:&#x65;&#x63;&#50;&#45;&#x75;&#x73;&#101;&#114;&#64;&#x35;&#52;&#x2e;&#49;&#53;&#x33;&#46;&#49;&#53;&#x30;&#x2e;&#52;">&#x65;&#x63;&#50;&#45;&#x75;&#x73;&#101;&#114;&#64;&#x35;&#52;&#x2e;&#49;&#53;&#x33;&#46;&#49;&#53;&#x30;&#x2e;&#52;</a>
ssh -i EC2Tutorial.pem <a href="mailto:&#101;&#99;&#50;&#45;&#117;&#115;&#x65;&#x72;&#64;&#x35;&#52;&#x2e;&#49;&#53;&#x33;&#x2e;&#x31;&#53;&#x30;&#x2e;&#x34;">&#101;&#99;&#50;&#45;&#117;&#115;&#x65;&#x72;&#64;&#x35;&#52;&#x2e;&#49;&#53;&#x33;&#x2e;&#x31;&#53;&#x30;&#x2e;&#x34;</a></p>
<p>  Q: You are getting a permission error exception when trying to SSH into your Linux Instance
  A: the key is missing permissions chmod 0400
  The exam expects you to know this, even if you used Windows &#x2F; Putty to SSH into your instances. If you’re a windows user, just have a quick look at the Linux SSH lecture!</p>
<h2 id="EC2-Instance-Connect"><a href="#EC2-Instance-Connect" class="headerlink" title="EC2 Instance Connect"></a>EC2 Instance Connect</h2><ul>
<li>Connect to your EC2 instance within your browser</li>
<li>No need to use your key file that was downloaded</li>
<li>The “magic” is that a temporary key is uploaded onto EC2 by AWS</li>
<li>Works only out-of-the-box with Amazon Linux 2</li>
<li>Need to make sure the port 22 is still opened!</li>
</ul>
<ul>
<li>EC2 Instance Connect (browser-based SSH connection): Connect to your instance using SSH in the console</li>
</ul>
<h2 id="Introduction-to-Security-Groups"><a href="#Introduction-to-Security-Groups" class="headerlink" title="Introduction to Security Groups"></a>Introduction to Security Groups</h2><ul>
<li>Security Groups are the fundamental of network security in AWS</li>
<li>They control how traffic is allowed into or out of our EC2 Machines.</li>
<li>It is the most fundamental skill to learn to troubleshoot networking issues.</li>
<li>In this lecture, we’ll learn how to use them to allow, inbound and outbound ports.</li>
</ul>
<p><strong>Security Groups Deeper Dive</strong></p>
<ul>
<li><code>Security groups are acting as a “firewall” on EC2 instances</code>.</li>
<li>They regulate:<ul>
<li>Access to Ports</li>
<li>Authorised IP ranges – IPv4 and IPv6</li>
<li>Control of inbound network (from other to the instance)</li>
<li>Control of outbound network (from the instance to other)</li>
</ul>
</li>
</ul>
<p><strong>Good to know</strong></p>
<ul>
<li><p>Can be attached to multiple instances</p>
</li>
<li><p>Locked down to a region &#x2F; VPC combination</p>
</li>
<li><p>Does live “outside” the EC2 – if traffic is blocked the EC2 instance won’t see it</p>
</li>
<li><p><code>It’s good to maintain one separate security group for SSH access</code></p>
</li>
<li><p>If your application is not accessible (time out), then it’s a security group issue</p>
</li>
<li><p>If your application gives a “connection refused“ error, then it’s an application error or it’s not launched</p>
</li>
<li><p>All inbound traffic is <code>blocked</code> by default</p>
</li>
<li><p>All outbound traffic is <code>authorised</code> by default</p>
<p>Q: Security groups can reference all of the following except:
- IP Address
- CIDR block
- Security Group
- DNS name        [ok]</p>
</li>
</ul>
<h2 id="Private-vs-Public-IP-IPv4"><a href="#Private-vs-Public-IP-IPv4" class="headerlink" title="Private vs Public IP (IPv4)"></a>Private vs Public IP (IPv4)</h2><ul>
<li>Networking has two sorts of IPs. IPv4 and IPv6:</li>
<li>IPv4: 1.160.10.240</li>
<li>IPv6: 3ffe:1900:4545:3:200:f8ff:fe21:67cf</li>
<li>In this course, we will only be using IPv4.</li>
<li>IPv4 is still the most common format used online.</li>
<li>IPv6 is newer and solves problems for the Internet of Things (IoT).</li>
<li>IPv4 allows for <code>3.7 billion</code> different addresses in the public space</li>
<li>IPv4: [0-255].[0-255].[0-255].[0-255].</li>
</ul>
<p><strong>Fundamental Differences</strong></p>
<ul>
<li>Public IP:<ul>
<li>Public IP means the machine can be identified on the internet (WWW)</li>
<li>Must be unique across the whole web (not two machines can have the same public IP).</li>
<li>Can be geo-located easily</li>
</ul>
</li>
<li>Private IP:<ul>
<li>Private IP means the machine can only be identified on a private network only</li>
<li>The IP must be unique across the private network</li>
<li>BUT two different private networks (two companies) can have the same IPs.</li>
<li>Machines connect to WWW using a NAT + internet gateway (a proxy)</li>
<li>Only a specified range of IPs can be used as private IP</li>
</ul>
</li>
</ul>
<h2 id="Elastic-IP"><a href="#Elastic-IP" class="headerlink" title="Elastic IP"></a>Elastic IP</h2><ul>
<li><p>With an Elastic IP address, you can mask the failure of an instance or software
by rapidly remapping the address to another instance in your account.</p>
</li>
<li><p>You can only have 5 Elastic IP in your account (you can ask AWS to increase
that).</p>
</li>
<li><p>Overall, <code>try to avoid using Elastic IP</code>:</p>
<ul>
<li>They often reflect poor architectural decisions</li>
<li>Instead, use a random public IP and register a DNS name to it</li>
<li>Or, as we’ll see later,<code> use a Load Balancer and don’t use a public IP</code> best pattern (*)</li>
</ul>
</li>
</ul>
<h2 id="Private-vs-Public-IP-IPv4-1"><a href="#Private-vs-Public-IP-IPv4-1" class="headerlink" title="Private vs Public IP (IPv4)"></a>Private vs Public IP (IPv4)</h2><p>AWS EC2 – Hands On</p>
<ul>
<li><p>By default, your EC2 machine comes with:</p>
<ul>
<li>A private IP for the internal AWS Network</li>
<li>A public IP, for the WWW.</li>
</ul>
</li>
<li><p>When we are doing SSH into our EC2 machines:</p>
<ul>
<li>We can’t use a private IP, because we are not in the same network</li>
<li>We can only use the public IP.</li>
</ul>
</li>
<li><p>If your machine is stopped and then started, <code>the public IP can change</code></p>
</li>
</ul>
<p><strong>Launching an Apache Server on EC2</strong></p>
<ul>
<li>Let’s leverage our EC2 instance</li>
<li>We’ll install an Apache Web Server to display a web page</li>
<li>We’ll create an index.html that shows the hostname of our machine</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">yum install -y httpd.x86_64</span><br><span class="line"></span><br><span class="line">systemctl start httpd.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># Enable across reboot .</span></span><br><span class="line">systemctl <span class="built_in">enable</span> httpd.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># First test page</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;Hello World from <span class="subst">$(hostname -f)</span>&quot;</span> &gt; /var/www/html/index.html</span><br></pre></td></tr></table></figure>


<p><strong>EC2 User Data</strong></p>
<ul>
<li>It is possible to bootstrap our instances using an <code>EC2 User data</code> script.</li>
<li><code>bootstrapping</code> means launching commands when a machine starts.</li>
<li>That script is <code>only run once</code> at the instance <code>first start</code></li>
<li>EC2 user data is used to automate boot tasks such as:<ul>
<li>Installing updates</li>
<li>Installing software</li>
<li>Downloading common files from the internet</li>
<li>Anything you can think of</li>
</ul>
</li>
<li>The EC2 User Data Script runs with the root user.</li>
</ul>
<p><strong>More EC2 User Data</strong>
Hands-On</p>
<ul>
<li>We want to make sure that this EC2 instance has an Apache HTTP
server installed on it – to display a simple web page</li>
<li>For it, we are going to write a user-data script.</li>
<li>This script will be executed at the first boot of the instance.</li>
<li>Let’s get hands on!</li>
</ul>
<ol>
<li>Terminate instance.</li>
<li>Create instances</li>
</ol>
<ul>
<li>Linux 2</li>
<li>Step 3:
User data, <code>As text</code><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment"># Use this for your user data (script without newlines)</span></span><br><span class="line"><span class="comment"># install httpd (Linux 2 version)</span></span><br><span class="line">yum update -y</span><br><span class="line">yum install -y httpd.x86_64</span><br><span class="line">systemctl start httpd.service</span><br><span class="line">systemctl <span class="built_in">enable</span> httpd.service</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;Hello World from <span class="subst">$(hostname -f)</span>&quot;</span> &gt; /var/www/html/index.html</span><br></pre></td></tr></table></figure></li>
</ul>
<p><strong>EC2 Instance Launch Types</strong></p>
<ul>
<li>On Demand Instances: short workload, predictable pricing.</li>
<li>Reserved: (MINIMUM 1 year) E.g: A database.<ul>
<li>Reserved Instances: long workloads</li>
<li>Convertible Reserved Instances: long workloads with flexible instances.</li>
<li>Scheduled Reserved Instances: example – every Thursday between 3 and 6 pm</li>
</ul>
</li>
<li>Spot Instances: short workloads, for cheap, can lose instances (<code>less reliable</code>)</li>
<li>Dedicated Instances: no other customers will share your hardware.</li>
<li>Dedicated Hosts: book an entire physical server, control instance placement.</li>
</ul>
<p><strong>EC2 On Demand</strong></p>
<ul>
<li><p>Pay for what you use (billing per second, after the first minute).</p>
</li>
<li><p>Has the highest cost but no upfront payment</p>
</li>
<li><p>No long term commitment</p>
</li>
<li><p>Recommended for short-term and un-interrupted workloads, where you can’t predict how the application will behave. Ideal for elastic workflows</p>
</li>
</ul>
<p><strong>EC2 Reserved Instances</strong></p>
<ul>
<li><p>Up to 75% discount compared to On-demand</p>
</li>
<li><p>Pay upfront for what you use with long term commitment</p>
</li>
<li><p>Reservation period can be 1 or 3 years</p>
</li>
<li><p>Reserve a specific instance type</p>
</li>
<li><p>Recommended for steady state usage applications (think <code>database</code>)</p>
</li>
<li><p>Convertible Reserved Instance</p>
<ul>
<li>Can change the EC2 instance type (you can make it evolve).</li>
<li>Up to 54% discount.</li>
</ul>
</li>
<li><p>Scheduled Reserved Instances</p>
<ul>
<li>launch within time window you reserve</li>
<li>When you require a fraction of day &#x2F; week &#x2F; month</li>
</ul>
</li>
</ul>
<p><strong>EC2 Spot Instances</strong></p>
<ul>
<li><p>Can get a discount of up to 90% compared to On-demand</p>
</li>
<li><p>Instances that you can “lose” at any point of time if your max price is less than the current spot price</p>
</li>
<li><p>The MOST cost-efficient instances in AWS</p>
</li>
<li><p><strong>Useful for workloads that are resilient to failure</strong></p>
<ul>
<li>Batch jobs</li>
<li>Data analysis</li>
<li>Image processing</li>
<li>…</li>
</ul>
</li>
<li><p><strong>Not great for critical jobs or databases</strong></p>
</li>
<li><p><strong>Great combo: Reserved Instances for baseline (Web app) + On-Demand &amp; Spot for peaks (Unpredictable, more agility, save money)</strong></p>
</li>
</ul>
<p>Q: You plan on running an open-source MongoDB database year-round on EC2. Which instance launch mode should you choose?
A: Reserved instances
    This will allow you to save cost as you know in advance that the instance will be a up for a full year</p>
<p><strong>EC2 Dedicated Hosts</strong></p>
<ul>
<li><p>Physical dedicated EC2 server for your use</p>
</li>
<li><p>Full control of EC2 Instance placement</p>
</li>
<li><p>Visibility into the underlying sockets &#x2F; physical cores of the hardware</p>
</li>
<li><p>Allocated for your account for a <code>3 year period</code> reservation</p>
</li>
<li><p><code>More expensive</code></p>
</li>
<li><p>Useful for software that have complicated licensing model (BYOL – Bring Your Own License)</p>
</li>
<li><p>Or for companies that have strong regulatory or compliance needs.</p>
</li>
</ul>
<p><strong>EC2 Dedicated Instances</strong></p>
<ul>
<li>Instances running on hardware that’s dedicated to you.</li>
<li>May share hardware with other instances in same account.</li>
<li>No control over instance placement (can move hardware after Stop &#x2F; Start).</li>
</ul>
<p><strong>Which host is right for me?</strong></p>
<ul>
<li>On demand: coming and staying in resort whenever we like, we pay the full price</li>
<li>Reserved: like planning ahead and if we plan to stay for a long time, we may get a good discount.</li>
<li>Spot instances: the hotel allows people to bid for the empty rooms and the highest bidder
keeps the rooms. You can get kicked out at any time</li>
<li>Dedicated Hosts: We book an entire building of the resort</li>
</ul>
<p>Q: You would like to deploy a database technology and the vendor license bills you based on the physical cores and underlying network socket visibility. Which EC2 launch modes allow you to get visibility into them?
A: Dedicated Hosts.</p>
<p><strong>Price Comparison</strong>
Example – m4.large – us-east-1
Price Type Price (per hour)
On-demand $0.10
Spot Instance (Spot Price) $0.032 - $0.045 (up to 90% off)
Spot Block (1 to 6 hours) ~ Spot Price
Reserved Instance (12 months) – no upfront $0.062
Reserved Instance (12 months) – all upfront $0.058
Reserved Instance (36 months) – no upfront $0.043
Reserved Convertible Instance (12 months) – no upfront $0.071
Reserved Dedicated Instance (12 months) – all upfront $0.064
Reserved Scheduled Instance (recurring schedule on 12 months term) $0.090 – $0.095 (5%-10% off)
Dedicated Host On-demand price
Dedicated Host Reservation Up to 70% off</p>
<p><strong>EC2 Spot Instance Requests</strong></p>
<ul>
<li><p>Can get a discount of up to 90% compared to On-demand</p>
</li>
<li><p>Define <em>max spot price</em> and get the instance while <strong>current spot price &lt; max</strong></p>
<ul>
<li>The hourly spot price varies based on offer and capacity</li>
<li>If the current spot price &gt; your max price you can choose to <strong>stop</strong> or <strong>terminate</strong> your instance with a 2 minutes grace period.</li>
</ul>
</li>
<li><p>Other strategy: <strong>Spot Block</strong></p>
<ul>
<li>“block” spot instance during a specified time frame (1 to 6 hours) without interruptions</li>
<li>In rare situations, <em>the instance may be reclaimed</em></li>
</ul>
</li>
<li><p><strong>Used for batch jobs, data analysis, or workloads that are resilient to failures</strong>.</p>
</li>
<li><p><strong>Not great for critical jobs or databases</strong></p>
</li>
</ul>
<p><em>EC2 Spot Instances Pricing</em>
<a target="_blank" rel="noopener" href="https://console.aws.amazon.com/">https://console.aws.amazon.com/</a></p>
<p><strong>Spot Fleets</strong></p>
<ul>
<li><p>Spot Fleets &#x3D; set of Spot Instances + (optional) On-Demand Instances</p>
</li>
<li><p>The Spot Fleet will try to meet the target capacity with price constraints</p>
<ul>
<li>Define possible <strong>launch pools</strong>: instance type (m5.large), OS, Availability Zone</li>
<li>Can have multiple launch pools, so that the fleet can choose</li>
<li>Spot Fleet stops launching instances when reaching capacity or max cost</li>
</ul>
</li>
<li><p>Strategies to allocate Spot Instances:</p>
<ul>
<li><strong>lowestPrice</strong>: from the pool with the lowest price (cost optimization, short workload)</li>
<li><strong>diversified</strong>: distributed across all pools (great for availability, long workloads)</li>
<li><strong>capacityOptimized</strong>: pool with the optimal capacity for the number of instances</li>
</ul>
</li>
<li><p><strong>Spot Fleets allow us to automatically request Spot Instances with the lowest price</strong></p>
</li>
</ul>
<p><strong>EC2 Instance Types – Main ones</strong></p>
<ul>
<li><p>R: applications that needs a lot of RAM – in-memory caches</p>
</li>
<li><p>C: applications that needs good CPU – compute &#x2F; databases</p>
</li>
<li><p>M: applications that are balanced (think “medium”) – general &#x2F; web app</p>
</li>
<li><p>I: applications that need good local I&#x2F;O (instance storage) – databases</p>
</li>
<li><p>G: applications that need a GPU – video rendering &#x2F; machine learning</p>
</li>
<li><p>T2 &#x2F; T3: burstable instances (up to a capacity)</p>
</li>
<li><p>T2 &#x2F; T3 - unlimited: unlimited burst</p>
</li>
<li><p>Real-world tip: use <a target="_blank" rel="noopener" href="https://www.ec2instances.info/">https://www.ec2instances.info</a></p>
</li>
</ul>
<p><strong>Burstable Instances (T2&#x2F;T3)</strong></p>
<ul>
<li><p>AWS has the concept of burstable instances (T2&#x2F;T3 machines).</p>
</li>
<li><p>Burst means that overall, the instance has OK CPU performance.</p>
</li>
<li><p>When the machine needs to process something unexpected (a spike in
load for example), it can burst, and CPU can be VERY good.</p>
</li>
<li><p>If the machine bursts, it utilizes “<strong>burst credits</strong>”</p>
</li>
<li><p>If all the credits are gone, the CPU becomes BAD</p>
</li>
<li><p>If the machine stops bursting, credits are accumulated over time</p>
</li>
<li><p>Burstable instances can be amazing to handle unexpected traffic and getting the insurance that it will be handled correctly</p>
</li>
<li><p>If your instance consistently runs low on credit, you need to move to a different kind of non-burstable instance Credit usage Credit balance</p>
</li>
</ul>
<p><strong>T2&#x2F;T3 Unlimited</strong></p>
<ul>
<li><p>Nov 2017: It is possible to have an “unlimited burst credit balance”</p>
</li>
<li><p>You pay extra money if you go over your credit balance, but you don’t
lose in performance</p>
</li>
<li><p>Overall, it is a new offering, so be careful, costs could go high if you’re
not monitoring the health of your instances</p>
</li>
<li><p>Read more here: <a target="_blank" rel="noopener" href="https://aws.amazon.com/blogs/aws/new-t2-unlimitedgoing-beyond-the-burst-with-high-performance">https://aws.amazon.com/blogs/aws/new-t2-unlimitedgoing-beyond-the-burst-with-high-performance</a></p>
</li>
</ul>
<h2 id="What’s-an-AMI"><a href="#What’s-an-AMI" class="headerlink" title="What’s an AMI?"></a>What’s an AMI?</h2><ul>
<li><p>As we saw, AWS comes with base images such as:</p>
<ul>
<li>Ubuntu</li>
<li>Fedora</li>
<li>RedHat</li>
<li>Windows</li>
<li>Etc…</li>
</ul>
</li>
<li><p>These images can be customised at runtime using EC2 User data</p>
</li>
<li><p>But what if we could create our own image, ready to go?</p>
</li>
<li><p>That’s an AMI – <strong>an image to use to create our instances</strong></p>
</li>
<li><p>AMIs can be built for Linux or Windows machines</p>
</li>
</ul>
<p><strong>Why would you use a custom AMI?</strong></p>
<ul>
<li>Using a custom built AMI can provide the following advantages:<ul>
<li>Pre-installed packages needed</li>
<li>Faster boot time (no need for ec2 user data at boot time)</li>
<li>Machine comes configured with monitoring &#x2F; enterprise software</li>
<li>Security concerns – control over the machines in the network</li>
<li>Control of maintenance and updates of AMIs over time</li>
<li>Active Directory Integration out of the box</li>
<li>Installing your app ahead of time (for faster deploys when auto-scaling)</li>
<li>Using someone else’s AMI that is optimised for running an app, DB, etc…</li>
</ul>
</li>
<li>AMI are built for a specific AWS region (!) <strong>NO GLOBAL</strong></li>
</ul>
<p>Q: You built and published an AMI in the ap-southeast-2 region, and your colleague in us-east-1 region cannot see it
A: An AMI created for a region can only be seen in that region.</p>
<p>Q: You are launching an EC2 instance in us-east-1 using this Python script snippet:</p>
<p>(we will see SDK in a later section, for now just look at the code reference ImageId)</p>
<p>ec2.create_instances(ImageId&#x3D;’ami-b23a5e7’, MinCount&#x3D;1, MaxCount&#x3D;1)
It works well, so you decide to deploy your script in us-west-1 as well. There, the script does not work and fails with “ami not found” error. What’s the problem?
A: AMI is region locked and the same ID cannot be used across regions</p>
<p><strong>Using Public AMIs</strong></p>
<ul>
<li><p>You can leverage AMIs from other people</p>
</li>
<li><p>You can also pay for other people’s AMI by the hour</p>
<ul>
<li>These people have optimised the software</li>
<li>The machine is easy to run and configure</li>
<li>You basically rent “expertise” from the AMI creator</li>
</ul>
</li>
<li><p>AMI can be found and published on the Amazon Marketplace</p>
</li>
<li><p><strong>Warning</strong>:</p>
<ul>
<li>Do not use an AMI you don’t trust!</li>
<li>Some AMIs might come with malware or may not be secure for your enterprise</li>
</ul>
</li>
</ul>
<p><strong>AMI Storage</strong></p>
<ul>
<li><p>Your AMI take space and they live in Amazon S3</p>
</li>
<li><p>Amazon S3 is a durable, cheap and resilient storage where most of your
backups will live (but you won’t see them in the S3 console)</p>
</li>
<li><p>By default, your AMIs are private, and locked for your account &#x2F; region</p>
</li>
<li><p>You can also make your AMIs public and share them with other AWS
accounts or sell them on the AMI Marketplace</p>
</li>
</ul>
<p><strong>AMI Pricing</strong></p>
<ul>
<li><p>AMIs live in Amazon S3, so you get charged for the actual space in takes in Amazon S3</p>
</li>
<li><p>Amazon S3 pricing in US-EAST-1:</p>
<ul>
<li>First 50 TB &#x2F; month: $0.023 per GB</li>
<li>Next 450 TB &#x2F; month: $0.022 per GB</li>
</ul>
</li>
<li><p>Overall it is quite inexpensive to store private AMIs.</p>
</li>
<li><p>Make sure to remove the AMIs you don’t use</p>
</li>
</ul>
<p><strong>Cross Account AMI Copy (FAQ + Exam Tip)</strong></p>
<ul>
<li>You can share an AMI with another AWS account.</li>
<li>Sharing an AMI does not affect the ownership of the AMI.</li>
<li><strong>If you copy an AMI that has been shared with your account, you are the owner of the target AMI in your account</strong>.</li>
<li>To copy an AMI that was shared with you from another account, the owner of the source AMI must grant you read permissions for the storage that backs the AMI, either the associated EBS snapshot (for an Amazon EBS-backed AMI) or an associated S3 bucket (for an instance store-backed AMI).</li>
<li><strong>Limits</strong>:</li>
<li>You can’t copy an encrypted AMI that was shared with you from another account. Instead, <strong>if the underlying snapshot and encryption key were shared with you, you can copy the snapshot while re-encrypting it with a key of your own. You own the copied snapshot, and can register it as a new AMI.</strong> [*]</li>
<li>You can’t copy an AMI with an associated <strong>billingProduct</strong> code that was shared with you from another account. This includes Windows AMIs and AMIs from the AWS Marketplace. To copy a shared AMI with a <strong>billingProduct</strong> code, <strong>launch an EC2 instance in your account using the shared AMI and then create an AMI from the instance</strong>. [*]</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.ingamis.html/">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Copy</a></p>
<h2 id="Placement-Groups"><a href="#Placement-Groups" class="headerlink" title="Placement Groups"></a>Placement Groups</h2><ul>
<li>Sometimes you want control over the EC2 Instance placement strategy</li>
<li>That strategy can be defined using placement groups</li>
<li>When you create a placement group, you specify one of the following
strategies for the group:<ul>
<li>Cluster—clusters instances into a low-latency group in a single Availability Zone</li>
<li>Spread—spreads instances across underlying hardware (max 7 instances per
group per AZ) - critical applications</li>
<li>Partition—spreads instances across many different partitions (which rely on
different sets of racks) within an AZ. Scales to 100s of EC2 instances per group
(Hadoop, Cassandra, Kafka)</li>
</ul>
</li>
</ul>
<h3 id="Cluster"><a href="#Cluster" class="headerlink" title="Cluster"></a>Cluster</h3><ul>
<li>Pros: Great network (10 Gbps bandwidth between instances)</li>
<li>Cons: If the rack fails, all instances fails at the same time</li>
<li>Use case:<ul>
<li>Big Data job that needs to complete fast</li>
<li>Application that needs extremely low latency and high network throughput</li>
</ul>
</li>
</ul>
<h3 id="Spread"><a href="#Spread" class="headerlink" title="Spread"></a>Spread</h3><ul>
<li>Pros:<ul>
<li>Can span across Availability Zones (AZ)</li>
<li>Reduced risk is simultaneous failure</li>
<li><strong>EC2</strong> Instances are on <strong>different physical hardware</strong></li>
</ul>
</li>
<li>Cons:<ul>
<li>Limited to <strong>7 instances per AZ per placement group</strong> [*]</li>
</ul>
</li>
<li>Use case:<ul>
<li>Application that needs to maximize high availability</li>
<li>Critical Applications where each instance must be isolated from failure from each other EC2</li>
</ul>
</li>
</ul>
<h3 id="Partition"><a href="#Partition" class="headerlink" title="Partition"></a>Partition</h3><ul>
<li>Up to 7 partitions per AZ</li>
<li>Up to 100s of EC2 instances</li>
<li>The instances in a partition do not share racks with the instances in the other partitions</li>
<li>A partition failure can affect many EC2 but won’t affect other partitions</li>
<li>EC2 instances get access to the partition information as metadata</li>
<li>Use cases: HDFS, HBase, Cassandra, Kafka</li>
</ul>
<p>Q: You would like to make sure your EC2 instances have the highest performance while talking to each other as you’re performing big data analysis. Which placement group should you choose?
A: Cluster
    Cluster placement groups places your instances next to each other giving you high performance computing and networking</p>
<p>Q: You are launching an application on EC2 and the whole process of installing the application takes about 30 minutes. You would like to minimize the total time for your instance to boot up and be operational to serve traffic. What do you recommend?
A: Create an AMI after installing and launch from the AMI
    Creating an AMI after installing the applications allows you to start more EC2 instances directly from that AMI, hence bypassing the need to install the application (as it’s already installed)</p>
<p>Q: You are running a critical workload of three hours per week, on Monday. As a solutions architect, which EC2 Instance Launch Type should you choose to maximize the cost savings while ensuring the application stability?
A: Scheduled Reserved Instances</p>
<h2 id="Elastic-Network-Interfaces-ENI"><a href="#Elastic-Network-Interfaces-ENI" class="headerlink" title="Elastic Network Interfaces (ENI)"></a>Elastic Network Interfaces (ENI)</h2><ul>
<li>Logical component in a VPC that represent <strong>a virtual network card</strong></li>
<li>The ENI can have the following attributes:<ul>
<li>Primary private IPv4, one or more secondary IPv4</li>
<li>One Elastic IP (IPv4) per private IPv4</li>
<li>One Public IPv4</li>
<li>One or more security groups</li>
<li>A MAC address</li>
</ul>
</li>
<li>You can create ENI independently and attach them on the fly (move them) on EC2 instances for failover</li>
<li>Bound to a specific availability zone (AZ)</li>
</ul>
<h2 id="EC2-Hibernate"><a href="#EC2-Hibernate" class="headerlink" title="EC2 Hibernate"></a>EC2 Hibernate</h2><ul>
<li><p>We know we can stop, terminate instances</p>
<ul>
<li>Stop: the data on disk (EBS) is kept intact in the next start</li>
<li>Terminate: any EBS volumes (root) also set-up to be destroyed is lost</li>
</ul>
</li>
<li><p>On start, the following happens:</p>
<ul>
<li>First start: the OS boots &amp; the EC2 User Data script is run</li>
<li>Following starts: the OS boots up</li>
<li>Then your application starts, caches get warmed up, and that can take time!</li>
</ul>
</li>
</ul>
<p><strong>EC2 Hibernate</strong></p>
<ul>
<li><p>Introducing <strong>EC2 Hibernate</strong>:</p>
<ul>
<li><strong>The in-memory (RAM) state is preserved</strong></li>
<li>The instance boot is much faster! (the OS is not stopped &#x2F; restarted)</li>
<li>Under the hood: the RAM state is written to a file in the root EBS volume</li>
<li>The root EBS <strong>volume must be encrypted</strong></li>
</ul>
</li>
<li><p><strong>Use cases:</strong></p>
<ul>
<li>long-running processing</li>
<li>saving the RAM state</li>
<li>services that take time to initialize
<a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Hibernate.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Hibernate.html</a></li>
</ul>
</li>
</ul>
<p><strong>EC2 Hibernate – Good to know</strong></p>
<ul>
<li><p><strong>Supported instance families</strong> - C3, C4, C5, M3, M4, M5, R3, R4, and R5.</p>
</li>
<li><p><strong>Instance RAM size</strong> - must be less than 150 GB.</p>
</li>
<li><p><strong>Instance size</strong> - not supported for bare metal instances.</p>
</li>
<li><p><strong>AMI</strong>: Amazon Linux 2, Linux AMI, Ubuntu &amp; Windows…</p>
</li>
<li><p><strong>Root Volume</strong>: must be EBS, encrypted, not instance store, and large</p>
</li>
<li><p>Available for On-Demand and Reserved Instances</p>
</li>
<li><p>An instance cannot be hibernated more than 60 days</p>
</li>
</ul>
<h2 id="EC2-for-Solutions-Architects"><a href="#EC2-for-Solutions-Architects" class="headerlink" title="EC2 for Solutions Architects"></a>EC2 for Solutions Architects</h2><ul>
<li>EC2 instances are billed by the second, t2.micro is free tier</li>
<li>On Linux &#x2F; Mac we use SSH, on Windows we use Putty</li>
<li>SSH is on port 22, lock down the security group to your IP</li>
<li>Timeout issues &#x3D;&gt; Security groups issues</li>
<li>Permission issues on the SSH key &#x3D;&gt; run “chmod 0400”</li>
<li>Security Groups can reference other Security Groups instead of IP
ranges (very popular exam question)</li>
<li>Know the difference between Private, Public and Elastic IP</li>
<li>You can customize an EC2 instance at boot time using EC2 User Data</li>
</ul>
<p><strong>EC2 for Solutions Architects</strong></p>
<ul>
<li>Know the 4 EC2 launch modes:<ul>
<li>On demand</li>
<li>Reserved</li>
<li>Spot instances</li>
<li>Dedicated Hosts</li>
</ul>
</li>
<li>Know the basic instance types: R,C,M,I,G, T2&#x2F;T3</li>
<li>You can create AMIs to pre-install software on your EC2 &#x3D;&gt; faster boot</li>
<li>AMI can be copied across regions and accounts</li>
<li>EC2 instances can be started in placement groups:<ul>
<li>Cluster</li>
<li>Spread</li>
</ul>
</li>
</ul>
<h1 id="Section-4-High-Availability-and-Scalability-ELB-amp-ASG"><a href="#Section-4-High-Availability-and-Scalability-ELB-amp-ASG" class="headerlink" title="Section 4: High Availability and Scalability: ELB &amp; ASG"></a>Section 4: High Availability and Scalability: ELB &amp; ASG</h1><h2 id="Scalability-amp-High-Availability"><a href="#Scalability-amp-High-Availability" class="headerlink" title="Scalability &amp; High Availability"></a>Scalability &amp; High Availability</h2><ul>
<li>Scalability means that an application &#x2F; system can handle greater loads by adapting.</li>
<li>There are two kinds of scalability:<ul>
<li>Vertical Scalability</li>
<li>Horizontal Scalability (&#x3D; elasticity)</li>
</ul>
</li>
<li><strong>Scalability is linked but different to High Availability</strong></li>
<li>Let’s deep dive into the distinction, using a call center as an example</li>
</ul>
<p><strong>Vertical Scalability</strong></p>
<ul>
<li>Vertically scalability means <strong>increasing the size of the instance</strong></li>
<li>For example, your application runs on a t2.micro</li>
<li>Scaling that application vertically means running it on a t2.large</li>
<li>Vertical scalability is very common for non distributed systems, such as a database.</li>
<li>RDS, ElastiCache are services that can scale vertically.</li>
<li>There’s usually a limit to how much you can vertically scale (hardware limit) junior operator senior operator</li>
</ul>
<p><strong>Horizontal Scalability</strong></p>
<ul>
<li><p>Horizontal Scalability means <strong>increasing the number of instances &#x2F; systems for your application</strong></p>
</li>
<li><p>Horizontal scaling implies distributed systems.</p>
</li>
<li><p>This is very common for web applications &#x2F; modern applications</p>
</li>
<li><p>It’s easy to horizontally scale thanks the cloud offerings such as Amazon EC2</p>
</li>
</ul>
<h2 id="High-Availability-first-building-in-New-York"><a href="#High-Availability-first-building-in-New-York" class="headerlink" title="High Availability first building in New York"></a>High Availability first building in New York</h2><ul>
<li><p>High Availability usually goes hand in hand with horizontal scaling</p>
</li>
<li><p>High availability means running your application &#x2F; system in at least 2 data
centers (&#x3D;&#x3D; Availability Zones)</p>
</li>
<li><p>The goal of high availability is to survive a data center loss</p>
</li>
<li><p>The high availability can be passive (for RDS Multi AZ for example)</p>
</li>
<li><p>The high availability can be active (for horizontal scaling)</p>
</li>
</ul>
<h2 id="High-Availability-amp-Scalability-For-EC2"><a href="#High-Availability-amp-Scalability-For-EC2" class="headerlink" title="High Availability &amp; Scalability For EC2"></a>High Availability &amp; Scalability For EC2</h2><ul>
<li><p>Vertical Scaling: <strong>Increase instance size</strong> (&#x3D; scale up &#x2F; down)</p>
<ul>
<li>From: t2.nano - 0.5G of RAM, 1 vCPU</li>
<li>To: u-12tb1.metal – 12.3 TB of RAM, 448 vCPUs</li>
</ul>
</li>
<li><p>Horizontal Scaling: <strong>Increase number of instances</strong> (&#x3D; scale out &#x2F; in)</p>
<ul>
<li>Auto Scaling Group</li>
<li>Load Balancer</li>
</ul>
</li>
<li><p>High Availability: Run instances for the same application across multi AZ</p>
<ul>
<li>Auto Scaling Group multi AZ</li>
<li>Load Balancer multi AZ</li>
</ul>
</li>
</ul>
<h2 id="What-is-load-balancing"><a href="#What-is-load-balancing" class="headerlink" title="What is load balancing?"></a>What is load balancing?</h2><ul>
<li>Load balancers are servers that <strong>forward internet traffic to multiple servers (EC2 Instances)</strong> downstream.</li>
</ul>
<h2 id="Why-use-a-load-balancer"><a href="#Why-use-a-load-balancer" class="headerlink" title="Why use a load balancer?"></a>Why use a load balancer?</h2><ul>
<li>Spread load across multiple downstream instances</li>
<li><strong>Expose a single point of access (DNS) to your application</strong></li>
<li>Seamlessly handle failures of downstream instances</li>
<li>Do regular health checks to your instances</li>
<li>Provide SSL termination (HTTPS) for your websites</li>
<li>Enforce stickiness with cookies</li>
<li>High availability across zones - Separate public traffic from private traffic</li>
</ul>
<h2 id="Why-use-an-EC2-Load-Balancer"><a href="#Why-use-an-EC2-Load-Balancer" class="headerlink" title="Why use an EC2 Load Balancer?"></a>Why use an EC2 Load Balancer?</h2><ul>
<li><p>An ELB (EC2 Load Balancer) is a <strong>managed load balancer</strong></p>
<ul>
<li>AWS guarantees that it will be working</li>
<li>AWS takes care of upgrades, maintenance, high availability</li>
<li>AWS provides only a few configuration knobs</li>
</ul>
</li>
<li><p>It costs less to setup your own load balancer but it will be a lot more effort on your end.</p>
</li>
<li><p>It is integrated with many AWS offerings &#x2F; services</p>
</li>
</ul>
<p><strong>Health Checks</strong></p>
<ul>
<li>Health Checks are crucial for Load Balancers</li>
<li>They enable the load balancer to know if instances it forwards traffic to are available to reply to requests</li>
<li>The health check is done on a port and a route (&#x2F;health is common)</li>
<li><strong>(If the response is not 200 (OK), then the instance is unhealthy)</strong></li>
</ul>
<p><strong>Types of load balancer on AWS</strong></p>
<ul>
<li><p>AWS has <strong>3 kinds of managed Load Balancers</strong></p>
</li>
<li><p>Classic Load Balancer (v1 - old generation) – 2009</p>
<ul>
<li>HTTP, HTTPS, TCP</li>
</ul>
</li>
<li><p>Application Load Balancer (v2 - new generation) – 2016</p>
<ul>
<li>HTTP, HTTPS, WebSocket</li>
</ul>
</li>
<li><p>Network Load Balancer (v2 - new generation) – 2017</p>
<ul>
<li>TCP, TLS (secure TCP) &amp; UDP</li>
</ul>
</li>
<li><p>Overall, it is recommended to use the newer &#x2F; v2 generation load balancers as they
provide more features</p>
</li>
<li><p>You can setup <strong>internal</strong> (private) or <strong>external</strong> (public) ELBs</p>
</li>
</ul>
<p><strong>Load Balancer</strong>
<strong>Good to Know</strong></p>
<ul>
<li>LBs can scale but not instantaneously – contact AWS for a “warm-up”</li>
<li>Troubleshooting<ul>
<li>4xx errors are client induced errors</li>
<li>5xx errors are application induced errors</li>
<li>Load Balancer Errors 503 means at capacity or no registered target</li>
<li>If the LB can’t connect to your application, check your security groups!</li>
</ul>
</li>
<li>Monitoring<ul>
<li>ELB access logs will log all access requests (so you can debug per request)</li>
<li>CloudWatch Metrics will give you aggregate statistics (ex: connections count)</li>
</ul>
</li>
</ul>
<h3 id="Classic-Load-Balancers-v1"><a href="#Classic-Load-Balancers-v1" class="headerlink" title="Classic Load Balancers (v1)"></a>Classic Load Balancers (v1)</h3><ul>
<li>Supports <strong>TCP</strong> (Layer 4), <strong>HTTP</strong> &amp; <strong>HTTPS</strong> (Layer 7)</li>
<li>Health checks are TCP or HTTP based</li>
<li>Fixed hostname XXX.region.elb.amazonaws.com</li>
</ul>
<h3 id="Application-Load-Balancer-v2"><a href="#Application-Load-Balancer-v2" class="headerlink" title="Application Load Balancer (v2)"></a>Application Load Balancer (v2)</h3><ul>
<li><p>Application load balancers is Layer 7 (HTTP)</p>
</li>
<li><p>Load balancing to multiple HTTP applications across machines (<strong>target groups</strong>)</p>
</li>
<li><p>Load balancing to multiple applications on the same machine (ex: containers)</p>
</li>
<li><p>Support for HTTP&#x2F;2 and WebSocket</p>
</li>
<li><p>Support redirects (from HTTP to HTTPS for example)</p>
</li>
<li><p>Routing tables to different target groups:</p>
<ul>
<li>Routing based on path in URL (example.com&#x2F;users &amp; example.com&#x2F;posts)</li>
<li>Routing based on hostname in URL (one.example.com &amp; other.example.com)</li>
<li>Routing based on Query String, Headers (example.com&#x2F;users?id&#x3D;123&amp;order&#x3D;false)</li>
</ul>
</li>
<li><p><strong>ALB are a great fit for micro services &amp; container-based application</strong> (example: Docker &amp; Amazon ECS)</p>
</li>
<li><p>Has a port mapping feature to redirect to a dynamic port in ECS</p>
</li>
<li><p>In comparison, we’d need multiple Classic Load Balancer per application</p>
</li>
</ul>
<p><strong>Application Load Balancer (v2)</strong>
<strong>Target Groups</strong></p>
<ul>
<li><p>EC2 instances (can be managed by an Auto Scaling Group) – HTTP</p>
</li>
<li><p>ECS tasks (managed by ECS itself) – HTTP</p>
</li>
<li><p>Lambda functions – HTTP request is translated into a JSON event</p>
</li>
<li><p>IP Addresses – must be private IPs</p>
</li>
<li><p>ALB can route to multiple target groups</p>
</li>
<li><p>Health checks are at the target group level</p>
</li>
</ul>
<h3 id="Good-to-Know"><a href="#Good-to-Know" class="headerlink" title="Good to Know"></a>Good to Know</h3><ul>
<li>Fixed hostname (XXX.region.elb.amazonaws.com)</li>
<li>The application servers don’t see the IP of the client directly<ul>
<li>The true IP of the client is inserted in the header <strong>X-Forwarded-For</strong></li>
<li>We can also get Port (<strong>X-Forwarded-Port</strong>) and proto (<strong>X-Forwarded-Proto</strong>)</li>
</ul>
</li>
</ul>
<h2 id="Network-Load-Balancer-v2"><a href="#Network-Load-Balancer-v2" class="headerlink" title="Network Load Balancer (v2)"></a>Network Load Balancer (v2)</h2><ul>
<li><p>Network load balancers (Layer 4) allow to:</p>
<ul>
<li><strong>Forward TCP &amp; UDP traffic to your instances</strong></li>
<li>Handle millions of request per seconds</li>
<li>Less latency ~100 ms (vs 400 ms for ALB)</li>
</ul>
</li>
<li><p><strong>NLB has one static IP per AZ, and supports assigning Elastic IP</strong> (helpful for whitelisting specific IP)</p>
</li>
<li><p>NLB are used for extreme performance, TCP or UDP traffic</p>
</li>
<li><p>Not included in the AWS free tier</p>
</li>
</ul>
<h2 id="Load-Balancer-Stickiness"><a href="#Load-Balancer-Stickiness" class="headerlink" title="Load Balancer Stickiness"></a>Load Balancer Stickiness</h2><ul>
<li>It is possible to implement stickiness so that the same client is always redirected
to the same instance behind a load balancer</li>
<li>This works for Classic Load Balancers &amp; Application Load Balancers</li>
<li>The “cookie” used for stickiness has an expiration date you control</li>
<li>Use case: make sure the user doesn’t lose his session data</li>
<li>Enabling stickiness may bring imbalance to the load over the backend EC2 instances</li>
</ul>
<h2 id="Cross-Zone-Load-Balancing"><a href="#Cross-Zone-Load-Balancing" class="headerlink" title="Cross-Zone Load Balancing"></a>Cross-Zone Load Balancing</h2><ul>
<li><p>It is possible to implement stickiness so that the same client is always redirected
to the same instance behind a load balancer</p>
</li>
<li><p>This works for Classic Load Balancers &amp; Application Load Balancers</p>
</li>
<li><p>The “cookie” used for stickiness has an expiration date you control</p>
</li>
<li><p>Use case: make sure the user doesn’t lose his session data</p>
</li>
<li><p>Enabling stickiness may bring imbalance to the load over the backend EC2 instances</p>
</li>
<li><p><strong>With Cross Zone Load Balancing: each load balancer instance distributes evenly across all registered instances in all AZ</strong></p>
</li>
<li><p>Otherwise, each load balancer node distributes requests evenly across the registered instances in its Availability Zone only</p>
</li>
<li><p>Classic Load Balancer</p>
<ul>
<li><strong>Disabled by default</strong></li>
<li>No charges for inter AZ data if enabled</li>
</ul>
</li>
<li><p>Application Load Balancer</p>
<ul>
<li>Always on (can’t be disabled)</li>
<li>No charges for inter AZ data</li>
</ul>
</li>
<li><p>Network Load Balancer</p>
<ul>
<li>Disabled by default</li>
<li><strong>You pay charges</strong> ($) for inter AZ data if enabled</li>
</ul>
</li>
</ul>
<h2 id="SSL-x2F-TLS-Basics"><a href="#SSL-x2F-TLS-Basics" class="headerlink" title="SSL&#x2F;TLS - Basics"></a>SSL&#x2F;TLS - Basics</h2><ul>
<li><p>An SSL Certificate allows traffic between your clients and your load balancer
to be encrypted in transit (in-flight encryption)</p>
</li>
<li><p>SSL refers to Secure Sockets Layer, used to encrypt connections</p>
</li>
<li><p>TLS refers to Transport Layer Security, which is a newer version</p>
</li>
<li><p>Nowadays, <strong>TLS certificates are mainly used</strong>, but people still refer as SSL</p>
</li>
<li><p>Public SSL certificates are issued by Certificate Authorities (CA)</p>
</li>
<li><p>Comodo, Symantec, GoDaddy, GlobalSign, Digicert, Letsencrypt, etc…</p>
</li>
<li><p>SSL certificates have an expiration date (you set) and must be renewed</p>
</li>
</ul>
<h2 id="Load-Balancer-SSL-Certificates"><a href="#Load-Balancer-SSL-Certificates" class="headerlink" title="Load Balancer - SSL Certificates"></a>Load Balancer - SSL Certificates</h2><ul>
<li>The load balancer uses an <strong>X.509 certificate (SSL&#x2F;TLS server certificate)</strong></li>
<li>You can manage certificates using ACM (AWS Certificate Manager)</li>
<li>You can create upload your own certificates alternatively</li>
<li>HTTPS listener:<ul>
<li>You must specify a default certificate</li>
<li>You can add an optional list of certs to support multiple domains</li>
<li><strong>Clients can use SNI (Server Name Indication) to specify the hostname they reach</strong></li>
<li>Ability to specify a security policy to support older versions of SSL &#x2F; TLS (legacy clients)</li>
</ul>
</li>
</ul>
<h2 id="SSL-–-Server-Name-Indication-SNI"><a href="#SSL-–-Server-Name-Indication-SNI" class="headerlink" title="SSL – Server Name Indication (SNI)"></a>SSL – Server Name Indication (SNI)</h2><ul>
<li>SNI solves the problem of loading <strong>multiple SSL certificates onto one web server</strong> (to serve multiple websites)</li>
<li>It’s a “newer” protocol, and requires the client to <strong>indicate</strong> the hostname of the target server in the initial SSL handshake</li>
<li>The server will then find the correct certificate, or return the default one</li>
</ul>
<p><strong>Note:</strong></p>
<ul>
<li>Only works for ALB &amp; NLB (newer generation), CloudFront</li>
<li>Does not work for CLB (older gen)</li>
</ul>
<h2 id="Elastic-Load-Balancers-–-SSL-Certificates"><a href="#Elastic-Load-Balancers-–-SSL-Certificates" class="headerlink" title="Elastic Load Balancers – SSL Certificates"></a>Elastic Load Balancers – SSL Certificates</h2><ul>
<li><p><strong>Classic Load Balancer (v1)</strong></p>
<ul>
<li>Support only one SSL certificate</li>
<li>Must use multiple CLB for multiple hostname with multiple SSL certificates</li>
</ul>
</li>
<li><p><strong>Application Load Balancer (v2)</strong></p>
<ul>
<li>Supports multiple listeners with multiple SSL certificates</li>
<li>Uses Server Name Indication (SNI) to make it work</li>
</ul>
</li>
<li><p><strong>Network Load Balancer (v2)</strong></p>
<ul>
<li>Supports multiple listeners with multiple SSL certificates</li>
<li>Uses Server Name Indication (SNI) to make it work</li>
</ul>
</li>
</ul>
<h2 id="ELB-–-Connection-Draining"><a href="#ELB-–-Connection-Draining" class="headerlink" title="ELB – Connection Draining [*]"></a>ELB – Connection Draining [*]</h2><ul>
<li><p><strong>Feature naming</strong>:</p>
<ul>
<li>CLB: Connection Draining</li>
<li>Target Group: Deregistration Delay (for ALB &amp; NLB)</li>
</ul>
</li>
<li><p><strong>Time to complete “in-flight requests” while the instance is de-registering or unhealthy</strong></p>
</li>
<li><p>Stops sending new requests to the instance which is de-registering</p>
</li>
<li><p>Between 1 to 3600 seconds, default is 300 seconds</p>
</li>
<li><p>Can be disabled (set value to 0)</p>
</li>
<li><p>Set to a low value if your requests are short</p>
</li>
</ul>
<h2 id="What’s-an-Auto-Scaling-Group"><a href="#What’s-an-Auto-Scaling-Group" class="headerlink" title="What’s an Auto Scaling Group?"></a>What’s an Auto Scaling Group?</h2><p>In real-life, the load on your websites and application can change</p>
<ul>
<li><p>In the cloud, you can create and get rid of servers very quickly</p>
</li>
<li><p>The goal of an Auto Scaling Group (ASG) is to:</p>
<ul>
<li>Scale out (add EC2 instances) to match an increased load</li>
<li>Scale in (remove EC2 instances) to match a decreased load</li>
<li>Ensure we have a minimum and a maximum number of machines running</li>
<li>Automatically Register new instances</li>
</ul>
</li>
</ul>
<h2 id="ASGs-have-the-following-attributes"><a href="#ASGs-have-the-following-attributes" class="headerlink" title="ASGs have the following attributes"></a>ASGs have the following attributes</h2><ul>
<li>A launch configuration<ul>
<li>AMI + Instance Type</li>
<li>EC2 User Data</li>
<li>EBS Volumes</li>
<li>Security Groups</li>
<li>SSH Key Pair</li>
</ul>
</li>
<li>Min Size &#x2F; Max Size &#x2F; Initial Capacity</li>
<li>Network + Subnets Information</li>
<li>Load Balancer Information</li>
<li>Scaling Policies</li>
</ul>
<h2 id="Auto-Scaling-Alarms"><a href="#Auto-Scaling-Alarms" class="headerlink" title="Auto Scaling Alarms"></a>Auto Scaling Alarms</h2><ul>
<li>It is possible to scale an ASG based on CloudWatch alarms</li>
<li>An Alarm monitors a metric (such as Average CPU)</li>
<li><strong>Metrics are computed for the overall ASG instances</strong></li>
<li>Based on the alarm:<ul>
<li>We can create scale-out policies (increase the number of instances)</li>
<li>We can create scale-in policies (decrease the number of instances)</li>
</ul>
</li>
</ul>
<h2 id="Auto-Scaling-New-Rules"><a href="#Auto-Scaling-New-Rules" class="headerlink" title="Auto Scaling New Rules"></a>Auto Scaling New Rules</h2><ul>
<li>It is now possible to define ”better” auto scaling rules that are directly
managed by EC2<ul>
<li>Target Average CPU Usage</li>
<li>Number of requests on the ELB per instance</li>
<li>Average Network In</li>
<li>Average Network Out</li>
</ul>
</li>
<li>These rules are easier to set up and can make more sense</li>
</ul>
<h2 id="Auto-Scaling-Custom-Metric"><a href="#Auto-Scaling-Custom-Metric" class="headerlink" title="Auto Scaling Custom Metric"></a>Auto Scaling Custom Metric</h2><ul>
<li><p>We can auto scale based on a custom metric (ex: number of connected
users)</p>
</li>
<li><ol>
<li>Send custom metric from application on EC2 to CloudWatch
(PutMetric API)</li>
</ol>
</li>
<li><ol start="2">
<li>Create CloudWatch alarm to react to low &#x2F; high values</li>
</ol>
</li>
<li><ol start="3">
<li>Use the CloudWatch alarm as the scaling policy for ASG</li>
</ol>
</li>
</ul>
<h2 id="ASG-Brain-Dump"><a href="#ASG-Brain-Dump" class="headerlink" title="ASG Brain Dump"></a>ASG Brain Dump</h2><ul>
<li>Scaling policies can be on CPU, Network… and can even be on custom metrics or
based on a schedule (if you know your visitors patterns)</li>
<li>ASGs use Launch configurations or Launch Templates (newer)</li>
<li>To update an ASG, you must provide a new launch configuration &#x2F; launch template</li>
<li>IAM roles attached to an ASG will get assigned to EC2 instances</li>
<li>ASG are free. You pay for the underlying resources being launched</li>
<li>Having instances under an ASG means that if they get terminated for whatever reason, the ASG will automatically <em><strong>create new ones as a replacement</strong></em>. Extra safety!</li>
<li>ASG can terminate instances marked as unhealthy by an LB (and hence replace them)</li>
</ul>
<h2 id="Auto-Scaling-Groups-–-Scaling-Policies"><a href="#Auto-Scaling-Groups-–-Scaling-Policies" class="headerlink" title="Auto Scaling Groups – Scaling Policies"></a>Auto Scaling Groups – Scaling Policies</h2><ul>
<li><strong>Target Tracking Scaling</strong><ul>
<li>Most simple and easy to set-up</li>
<li>Example: I want the average ASG CPU to stay at around 40%</li>
</ul>
</li>
<li><strong>Simple &#x2F; Step Scaling</strong><ul>
<li>When a CloudWatch alarm is triggered (example CPU &gt; 70%), then add 2 units</li>
<li>When a CloudWatch alarm is triggered (example CPU &lt; 30%), then remove 1</li>
</ul>
</li>
<li><strong>Scheduled Actions</strong><ul>
<li>Anticipate a scaling based on known usage patterns</li>
<li>Example: increase the min capacity to 10 at 5 pm on Fridays</li>
</ul>
</li>
</ul>
<h2 id="Auto-Scaling-Groups-Scaling-Cooldowns"><a href="#Auto-Scaling-Groups-Scaling-Cooldowns" class="headerlink" title="Auto Scaling Groups - Scaling Cooldowns"></a>Auto Scaling Groups - Scaling Cooldowns</h2><ul>
<li><strong>The cooldown period helps to ensure that your Auto Scaling group doesn’t launch or terminate additional instances before the previous scaling activity takes effect</strong>.</li>
<li>In addition to default cooldown for Auto Scaling group, we can create cooldowns
that apply to a specific <strong>simple scaling policy</strong></li>
<li>A scaling-specific cooldown period overrides the default cooldown period.</li>
<li>One common use for scaling-specific cooldowns is with a scale-in policy—a policy that terminates instances based on a specific criteria or metric. Because this policy
terminates instances, Amazon EC2 Auto Scaling needs less time to determine
whether to terminate additional instances.</li>
<li><strong>If the default cooldown period of 300 seconds is too long—you can reduce costs y applying a scaling-specific cooldown period of 180 seconds to the scale-in policy</strong>.</li>
<li>If your application is scaling up and down multiple times each hour, modify the
Auto Scaling Groups cool-down timers and the CloudWatch Alarm Period that
triggers the scale in</li>
</ul>
<h2 id="ASG-for-Solutions-Architects"><a href="#ASG-for-Solutions-Architects" class="headerlink" title="ASG for Solutions Architects"></a>ASG for Solutions Architects</h2><ul>
<li><strong>ASG Default Termination Policy (simplified version)</strong>:</li>
</ul>
<ol>
<li>Find the AZ which has the most number of instances</li>
<li>If there are multiple instances in the AZ to choose from, delete the one with the oldest launch configuration</li>
</ol>
<ul>
<li>ASG tries the balance the number of instances across AZ by default</li>
</ul>
<p><strong>Lifecycle Hooks</strong></p>
<ul>
<li>By default as soon as an instance is launched in an ASG it’s in service.</li>
<li>You have the ability to perform extra steps before the instance goes in service (Pending state)</li>
<li>You have the ability to perform some actions before the instance is terminated(Terminating state)
<a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html</a></li>
</ul>
<p><strong>Launch Template vs Launch Configuration</strong></p>
<ul>
<li><p><strong>Both</strong>:</p>
<ul>
<li>ID of the Amazon Machine Image (AMI), the instance type, a key pair, security  roups, and the other parameters that you use to launch EC2 instances (tags, EC2 user-data…)</li>
</ul>
</li>
<li><p><strong>Launch Configuration (legacy):</strong></p>
<ul>
<li>Must be re-created every time</li>
</ul>
</li>
<li><p><strong>Launch Template (newer):</strong></p>
<ul>
<li>Can have multiple versions</li>
<li>Create parameters subsets (partial configuration for re-use and inheritance)</li>
<li>Provision using both On-Demand and Spot instances (or a mix)</li>
<li>Can use T2 unlimited burst feature</li>
<li><strong>Recommended by AWS going forward</strong></li>
</ul>
<hr>
</li>
</ul>
<p>Q1: Load Balancers provide a
A: static DNS name we can use our application
  The reason being that AWS wants your load balancer to be accessible using a static endpoint, even if the underlying infrastructure that AWS manages changes</p>
<p>Q2: You are running a website with a load balancer and 10 EC2 instances. Your users are complaining about the fact that your website always asks them to re-authenticate when they switch pages. You are puzzled, because it’s working just fine on your machine and in the dev environment with 1 server. What could be the reason?
A: The Load Balancer does not have stickiness enabled</p>
<p>  Stickiness ensures traffic is sent to the same backend instance for a client. This helps maintaining session data</p>
<p>Question 3:
Your application is using an Application Load Balancer. It turns out your application only sees traffic coming from private IP which are in fact your load balancer’s. What should you do to find the true IP of the clients connected to your website?
A: Look into the X-Forwarded-For header in the backend</p>
<p>  This header is created by your load balancer and passed on to your backend application</p>
<p>Question 4:
A: Question 4:
You quickly created an ELB and it turns out your users are complaining about the fact that sometimes, the servers just don’t work. You realise that indeed, your servers do crash from time to time. How to protect your users from seeing these crashes?
A: Enable Health Checks</p>
<p>  Health checks ensure your ELB won’t send traffic to unhealthy (crashed) instances</p>
<p>Question 5:
You are designing a high performance application that will require millions of connections to be handled, as well as low latency. The best Load Balancer for this is
A: Network Load Balancer
  NLB provide the highest performance if your application needs it</p>
<p>Question 6:
Application Load Balancers handle all these protocols except
A: TCP
  Use a NLB (Network Load Balancer) support TCP instead</p>
<p>Question 7: X
The application load balancer can route to different target groups based on all these except…
A: Geography -  This was discussed in Lecture 40: Elastic Load Balancing (ELB) Overview
    X: Source IP</p>
<p>Question 8:
You are running at desired capacity of 3 and the maximum capacity of 3. You have alarms set at 60% CPU to scale out your application. Your application is now running at 80% capacity. What will happen?
A: Nothing
  The capacity of your ASG cannot go over the maximum capacity you have allocated during scale out events</p>
<p>Question 9:
I have an ASG and an ALB, and I setup my ASG to get health status of instances thanks to my ALB. One instance has just been reported unhealthy. What will happen?
A: The ASG will terminate the EC2 instance.
  Because the ASG has been configured to leverage the ALB health checks, unhealthy instances will be terminated</p>
<p>Question 10:
Your boss wants to scale your ASG based on the number of requests per minute your application makes to your database.
A: You create a CloudWatch custom metric and build an alarm on this to scale your ASG
  The metric “requests per minute” is not an AWS metric, hence it needs to be a custom metric</p>
<p>Question 11:
Scaling an instance from an r4.large to an r4.4xlarge is called
A: Vertical</p>
<p>Question 12:
Running an application on an auto scaling group that scales the number of instances in and out is called
A: Horizontal Scalability</p>
<p>Question 13:
You would like to expose a fixed static IP to your end-users for compliance purposes, so they can write firewall rules that will be stable and approved by regulators. Which Load Balancer should you use?
A: Network Load Balancer
  Network Load Balancers expose a public static IP, whereas an Application or Classic Load Balancer exposes a static DNS (URL)</p>
<p>Question 14:
A web application hosted in EC2 is managed by an ASG. You are exposing this application through an Application Load Balancer. The ALB is deployed on the VPC with the following CIDR: 192.168.0.0&#x2F;18. How do you configure the EC2 instance security group to ensure only the ALB can access the port 80?
A: Open up the EC2 security on port 80 to the ALB’s security group
  This is the most secure way of ensuring only the ALB can access the EC2 instances. Referencing by security groups in rules is an extremely powerful rule and many questions at the exam rely on it. Make sure you fully master the concepts behind it!</p>
<p>Question 15:
Your application load balancer is hosting 3 target groups with hostnames being users.example.com, api.external.example.com and checkout.example.com. You would like to expose HTTPS traffic for each of these hostnames. How do you configure your ALB SSL certificates to make this work?
A: Use SNI
  SNI (Server Name Indication) is a feature allowing you to expose multiple SSL certs if the client supports it. Read more here: <a target="_blank" rel="noopener" href="https://aws.amazon.com/blogs/aws/new-application-load-balancer-sni/">https://aws.amazon.com/blogs/aws/new-application-load-balancer-sni/</a></p>
<p>Question 16:
An ASG spawns across 2 availability zones. AZ-A has 3 EC2 instances and AZ-B has 4 EC2 instances. The ASG is about to go into a scale-in event. What will happen?
A: Make sure you remember the Default Termination Policy for ASG. It tries to balance across AZ first, and then delete based on the age of the launch configuration.</p>
<p>Question 17:
The Application Load Balancers target groups can be all of these EXCEPT…
A: Network Load Balancer</p>
<p>Question 18:
You are running an application in 3 AZ, with an Auto Scaling Group and a Classic Load Balancer. It seems that the traffic is not evenly distributed amongst all the backend EC2 instances, with some AZ being overloaded. Which feature should help distribute the traffic across all the available EC2 instances?
A: Cross Zone Load Balancing</p>
<p>Question 19:
Your Application Load Balancer (ALB) currently is routing to two target groups, each of them is routed to based on hostname rules. You have been tasked with enabling HTTPS traffic for each hostname and have loaded the certificates onto the ALB. Which ALB feature will help it choose the right certificate for your clients?
A: Server Name Indication (SNI)</p>
<p>Question 20:
An application is deployed with an Application Load Balancer and an Auto Scaling Group. Currently, the scaling of the Auto Scaling Group is done manually and you would like to define a scaling policy that will ensure the average number of connections to your EC2 instances is averaging at around 1000. Which scaling policy should you use?
A: Target Tracking</p>
<hr>
<hr>
<h2 id="EBS-amp-EFS"><a href="#EBS-amp-EFS" class="headerlink" title="EBS &amp; EFS"></a>EBS &amp; EFS</h2><p>What’s an EBS Volume?</p>
<ul>
<li><p>An EC2 machine loses its root volume (main drive) when it is manually terminated.</p>
</li>
<li><p>Unexpected terminations might happen from time to time (AWS would email you)</p>
</li>
<li><p>Sometimes, you need a way to store your instance data somewhere</p>
</li>
<li><p>An EBS <strong>(Elastic Block Store) Volume</strong> is a <strong>network</strong> drive you can attach to your instances while they run</p>
</li>
<li><p>It allows your instances to persist data Amazon EBS</p>
</li>
</ul>
<h3 id="EBS-Volume"><a href="#EBS-Volume" class="headerlink" title="EBS Volume"></a>EBS Volume</h3><ul>
<li><p><strong>It’s a network drive</strong> (i.e. not a physical drive)</p>
<ul>
<li>It uses the network to communicate the instance, which means there might be a bit of latency</li>
<li>It <strong>can be detached from an EC2 instance and attached to another one quickly</strong>.</li>
</ul>
</li>
<li><p><strong>It’s locked to an Availability Zone (AZ)</strong></p>
<ul>
<li>An EBS Volume in us-east-1a cannot be attached to us-east-1b</li>
<li>To move a volume across, you first need to snapshot it</li>
</ul>
</li>
<li><p>Have a provisioned capacity (size in GBs, and IOPS)</p>
<ul>
<li>You get billed for all the provisioned capacity</li>
<li>You can increase the capacity of the drive over time</li>
</ul>
</li>
</ul>
<h3 id="EBS-Volume-Types"><a href="#EBS-Volume-Types" class="headerlink" title="EBS Volume Types"></a>EBS Volume Types</h3><ul>
<li><p>EBS Volumes come in 4 types</p>
<ul>
<li><strong>GP2 (SSD)</strong>: General purpose SSD volume that balances price and performance for a wide variety of workloads</li>
<li><strong>IO1 (SSD)</strong>: Highest-performance SSD volume for mission-critical low-latency or high- throughput workloads</li>
<li><strong>ST1 (HDD)</strong>: Low cost HDD volume designed for frequently accessed, throughput- intensive workloads</li>
<li><strong>SC1 (HDD)</strong>: Lowest cost HDD volume designed for less frequently accessed workloads</li>
</ul>
</li>
<li><p>EBS Volumes are characterized in Size | Throughput | IOPS (I&#x2F;O Ops Per Sec)</p>
</li>
<li><p>When in doubt always consult the AWS documentation – it’s good!</p>
</li>
<li><p><strong>Only GP2 and IO1 can be used as boot volumes</strong></p>
</li>
</ul>
<p>$ lsblk
$ sudo file -s &#x2F;dev&#x2F;xvdb
&#x2F;dev&#x2F;xvdb: data
$ sudo mkfs -t ext4 &#x2F;dev&#x2F;xvdb
mke2fs 1.42.9 (28-Dec-2013)
Filesystem label&#x3D;
OS type: Linux
Block size&#x3D;4096 (log&#x3D;2)
Fragment size&#x3D;4096 (log&#x3D;2)
Stride&#x3D;0 blocks, Stripe width&#x3D;0 blocks
131072 inodes, 524288 blocks
26214 blocks (5.00%) reserved for the super user
First data block&#x3D;0
Maximum filesystem blocks&#x3D;536870912
16 block groups
32768 blocks per group, 32768 fragments per group
8192 inodes per group
Superblock backups stored on blocks:
        32768, 98304, 163840, 229376, 294912</p>
<p>Allocating group tables: done
Writing inode tables: done
Creating journal (16384 blocks): done
Writing superblocks and filesystem accounting information: done</p>
<p>[ec2-user@ip-172-31-15-70 ~]$ sudo mkdir &#x2F;mnt&#x2F;data
[ec2-user@ip-172-31-15-70 ~]$ sudo mount &#x2F;dev&#x2F;xvdb &#x2F;mnt&#x2F;data&#x2F;
[ec2-user@ip-172-31-15-70 ~]$ lsblk
NAME    MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
xvda    202:0    0   8G  0 disk
└─xvda1 202:1    0   8G  0 part &#x2F;
xvdb    202:16   0   2G  0 disk &#x2F;mnt&#x2F;data</p>
<h2 id="EBS-Volume-Types-Use-cases-GP2-from-AWS-doc"><a href="#EBS-Volume-Types-Use-cases-GP2-from-AWS-doc" class="headerlink" title="EBS Volume Types Use cases GP2 (from AWS doc)"></a>EBS Volume Types Use cases GP2 (from AWS doc)</h2><ul>
<li><p>Recommended for most workloads</p>
</li>
<li><p>System boot volumes</p>
</li>
<li><p>Virtual desktops</p>
</li>
<li><p>Low-latency interactive apps</p>
</li>
<li><p>Development and test environments</p>
</li>
<li><p>1 GiB - 16 TiB</p>
</li>
<li><p>Small gp2 volumes can burst IOPS to 3000</p>
</li>
<li><p>Max IOPS is 16,000…</p>
</li>
<li><p>3 IOPS per GB, means at 5,334GB we are at the max IOPS</p>
</li>
</ul>
<h2 id="EBS-Volume-Types-Use-cases"><a href="#EBS-Volume-Types-Use-cases" class="headerlink" title="EBS Volume Types Use cases"></a>EBS Volume Types Use cases</h2><p><strong>IO1 (from AWS doc)</strong></p>
<ul>
<li><p>Critical business applications that require sustained IOPS performance, or
more than 16,000 IOPS per volume (gp2 limit)</p>
</li>
<li><p>Large database workloads, such as:</p>
</li>
<li><p>MongoDB, Cassandra, Microsoft SQL Server, MySQL, PostgreSQL, Oracle</p>
</li>
<li><p>4 GiB - 16 TiB</p>
</li>
<li><p>IOPS is provisioned (PIOPS) – MIN 100 - MAX 64,000 (Nitro instances) else MAX 32,000 (other instances)</p>
</li>
<li><p>The maximum ratio of provisioned IOPS to requested volume size (in GiB) is 50:1</p>
</li>
</ul>
<h2 id="EBS-Volume-Types-Use-cases-1"><a href="#EBS-Volume-Types-Use-cases-1" class="headerlink" title="EBS Volume Types Use cases"></a>EBS Volume Types Use cases</h2><p><strong>ST1 (from AWS doc)</strong></p>
<ul>
<li><p>Streaming workloads requiring consistent, fast throughput at a low price.</p>
</li>
<li><p>Big data, Data warehouses, Log processing</p>
</li>
<li><p>Apache Kafka</p>
</li>
<li><p>Cannot be a boot volume</p>
</li>
<li><p>500 GiB - 16 TiB</p>
</li>
<li><p>Max IOPS is 500</p>
</li>
<li><p>Max throughput of 500 MiB&#x2F;s – can burst</p>
</li>
</ul>
<h2 id="EBS-Volume-Types-Use-cases-2"><a href="#EBS-Volume-Types-Use-cases-2" class="headerlink" title="EBS Volume Types Use cases"></a>EBS Volume Types Use cases</h2><p><strong>SC1 (from AWS doc)</strong></p>
<ul>
<li><p>Throughput-oriented storage for large volumes of data that is
infrequently accessed</p>
</li>
<li><p>Scenarios where the lowest storage cost is important</p>
</li>
<li><p>Cannot be a boot volume</p>
</li>
<li><p>500 GiB - 16 TiB</p>
</li>
<li><p>Max IOPS is 250</p>
</li>
<li><p>Max throughput of 250 MiB&#x2F;s – can burst</p>
</li>
</ul>
<h2 id="EBS-–Volume-Types-Summary"><a href="#EBS-–Volume-Types-Summary" class="headerlink" title="EBS –Volume Types Summary"></a>EBS –Volume Types Summary</h2><ul>
<li><strong>gp2: General Purpose Volumes (cheap)</strong><ul>
<li>3 IOPS &#x2F; GiB, minimum 100 IOPS, burst to 3000 IOPS, max 16000 IOPS</li>
<li>1 GiB – 16 TiB , +1 TB &#x3D; +3000 IOPS</li>
</ul>
</li>
<li><strong>io1: Provisioned IOPS (expensive)</strong><ul>
<li>Min 100 IOPS, Max 64000 IOPS (Nitro) or 32000 (other)</li>
<li>4 GiB - 16 TiB. Size of volume and IOPS are independent</li>
</ul>
</li>
<li><strong>st1: Throughput Optimized HDD</strong><ul>
<li>500 GiB – 16 TiB , 500 MiB &#x2F;s throughput</li>
</ul>
</li>
<li><strong>sc1: Cold HDD, Infrequently accessed data</strong><ul>
<li>500 GiB – 16 TiB , 250 MiB &#x2F;s throughput</li>
</ul>
</li>
</ul>
<h2 id="EBS-Snapshots"><a href="#EBS-Snapshots" class="headerlink" title="EBS Snapshots"></a>EBS Snapshots</h2><ul>
<li><strong>Incremental – only backup changed blocks</strong> [*]</li>
<li>EBS backups use IO and you shouldn’t run them while your application is handling a lot of traffic</li>
<li>Snapshots will be stored in S3 (but you won’t directly see them)</li>
<li>Not necessary to detach volume to do snapshot, but recommended</li>
<li>Max 100,000 snapshots</li>
<li>Can copy snapshots across AZ or Region</li>
<li>Can make Image (AMI) from Snapshot</li>
<li><strong>EBS volumes restored by snapshots need to be pre-warmed (using fio or dd command to read the entire volume)</strong></li>
<li><strong>Snapshots can be automated using Amazon Data Lifecycle Manager</strong></li>
</ul>
<p>Tip: Use “Create Snapshot Lifecycle Policy” for backups</p>
<h2 id="EBS-Migration"><a href="#EBS-Migration" class="headerlink" title="EBS Migration"></a>EBS Migration</h2><ul>
<li><p>EBS Volumes are only locked to a specific AZ</p>
</li>
<li><p>To migrate it to a different AZ (or region):</p>
<ul>
<li>Snapshot the volume</li>
<li>(optional) Copy the volume to a different region</li>
<li>Create a volume from the snapshot in the AZ of your choice</li>
</ul>
</li>
<li><p>Let’s practice!</p>
</li>
</ul>
<h2 id="EBS-Encryption"><a href="#EBS-Encryption" class="headerlink" title="EBS Encryption"></a>EBS Encryption</h2><ul>
<li>When you create an encrypted EBS volume, you get the following:<ul>
<li>Data at rest is encrypted inside the volume</li>
<li>All the data in flight moving between the instance and the volume is encrypted</li>
<li>All snapshots are encrypted</li>
<li>All volumes created from the snapshot</li>
</ul>
</li>
<li>Encryption and decryption are handled transparently (you have nothing to do)</li>
<li>Encryption has a minimal impact on latency</li>
<li>EBS Encryption leverages keys from KMS (AES-256)</li>
<li>Copying an unencrypted snapshot allows encryption</li>
<li>Snapshots of encrypted volumes are encrypted</li>
</ul>
<h3 id="Encryption-encrypt-an-unencrypted-EBS-volume"><a href="#Encryption-encrypt-an-unencrypted-EBS-volume" class="headerlink" title="Encryption: encrypt an unencrypted EBS volume"></a>Encryption: encrypt an unencrypted EBS volume</h3><ul>
<li>Create an EBS snapshot of the volume</li>
<li>Encrypt the EBS snapshot ( using copy )</li>
<li>Create new ebs volume from the snapshot ( the volume will also be encrypted )</li>
<li>Now you can attach the encrypted volume to the original instance</li>
</ul>
<h2 id="EBS-vs-Instance-Store"><a href="#EBS-vs-Instance-Store" class="headerlink" title="EBS vs Instance Store"></a>EBS vs Instance Store</h2><ul>
<li>Some instance do not come with Root EBS volumes</li>
<li>Instead, they come with “Instance Store” (&#x3D; ephemeral storage)</li>
<li>Instance store is physically attached to the machine (EBS is a network drive)</li>
<li>Pros:<ul>
<li>Better I&#x2F;O performance (EBS gp2 has an max IOPS of 16000, io1 of 64000)</li>
<li>Good for buffer &#x2F; cache &#x2F; scratch data &#x2F; temporary content</li>
<li>Data survives reboots</li>
</ul>
</li>
<li>Cons:<ul>
<li>On stop or termination, the instance store is lost</li>
<li>You can’t resize the instance store</li>
<li>Backups must be operated by the user</li>
</ul>
</li>
</ul>
<h2 id="Local-EC2-Instance-Store"><a href="#Local-EC2-Instance-Store" class="headerlink" title="Local EC2 Instance Store"></a>Local EC2 Instance Store</h2><ul>
<li><strong>Physical disk attached to the physical server where your EC2 is</strong></li>
<li>Very High IOPS (because physical)</li>
<li>Disks up to 7.5 TiB (can change over time), stripped to reach 30 TiB (can change over time…)</li>
<li>Block Storage (just like EBS)</li>
<li>Cannot be increased in size</li>
<li>Risk of data loss if hardware fails</li>
</ul>
<h2 id="EBS-RAID-Options"><a href="#EBS-RAID-Options" class="headerlink" title="EBS RAID Options"></a>EBS RAID Options</h2><ul>
<li><strong>EBS is already redundant storage</strong> (replicated within an AZ)</li>
<li>But what if you want to increase IOPS to say 100 000 IOPS?</li>
<li>What if you want to mirror your EBS volumes?</li>
<li>You would mount volumes in parallel in RAID settings!</li>
<li>RAID is possible as long as your OS supports it</li>
<li>Some RAID options are:<ul>
<li>RAID 0</li>
<li>RAID 1</li>
<li>RAID 5 (not recommended for EBS – see documentation)</li>
<li>RAID 6 (not recommended for EBS – see documentation)</li>
</ul>
</li>
<li>We’ll explore RAID 0 and RAID 1</li>
</ul>
<h3 id="RAID-0-increase-performance"><a href="#RAID-0-increase-performance" class="headerlink" title="RAID 0 (increase performance) [*]"></a>RAID 0 (increase performance) [*]</h3><ul>
<li>Combining 2 or more volumes and getting the total disk space and I&#x2F;O</li>
<li>But one disk fails, all the data is failed</li>
<li>Use cases would be:<ul>
<li>An application that needs a lot of IOPS and doesn’t need fault-tolerance</li>
<li>A database that has replication already built-in</li>
</ul>
</li>
<li>Using this, we can have a very big disk with a lot of IOPS</li>
<li>For example<ul>
<li>two 500 GiB Amazon EBS io1 volumes with 4,000 provisioned IOPS each will create a…</li>
<li>1000 GiB RAID 0 array with an available bandwidth of 8,000 IOPS and 1,000 MB&#x2F;s of throughput</li>
</ul>
</li>
</ul>
<h3 id="RAID-1-increase-fault-tolerance"><a href="#RAID-1-increase-fault-tolerance" class="headerlink" title="RAID 1 (increase fault tolerance)"></a>RAID 1 (increase fault tolerance)</h3><ul>
<li>RAID 1 &#x3D; Mirroring a volume to another</li>
<li>If one disk fails, our logical volume is still working</li>
<li>We have to send the data to two EBS volume at the same time (2x network)</li>
<li>Use case:<ul>
<li>Application that need increase volume fault tolerance</li>
<li>Application where you need to service disks</li>
</ul>
</li>
<li>For example:<ul>
<li>two 500 GiB Amazon EBS io1 volumes with 4,000 provisioned IOPS each will create a…</li>
<li>500 GiB RAID 1 array with an available bandwidth of 4,000 IOPS and 500 MB&#x2F;s of throughput</li>
</ul>
</li>
</ul>
<h2 id="EFS-–-Elastic-File-System"><a href="#EFS-–-Elastic-File-System" class="headerlink" title="EFS – Elastic File System"></a>EFS – Elastic File System</h2><ul>
<li>Managed NFS (network file system) that can be mounted on many EC2</li>
<li>EFS works with EC2 instances in multi-AZ</li>
<li>Highly available, scalable, expensive (3x gp2), pay per use</li>
</ul>
<h3 id="EFS-–-Elastic-File-System-1"><a href="#EFS-–-Elastic-File-System-1" class="headerlink" title="EFS – Elastic File System"></a>EFS – Elastic File System</h3><ul>
<li><p>Use cases: <strong>content management, web serving, data sharing, Wordpress</strong></p>
</li>
<li><p>Uses NFSv4.1 protocol</p>
</li>
<li><p>Uses <strong>security group</strong> to control access to EFS</p>
</li>
<li><p><strong>Compatible with Linux</strong> based AMI (not Windows)</p>
</li>
<li><p>Encryption at rest using KMS</p>
</li>
<li><p>POSIX file system (~Linux) that has a standard file API</p>
</li>
<li><p>File system scales automatically, pay-per-use, no capacity planning!</p>
</li>
</ul>
<h3 id="EFS-–-Performance-amp-Storage-Classes"><a href="#EFS-–-Performance-amp-Storage-Classes" class="headerlink" title="EFS – Performance &amp; Storage Classes [*]"></a>EFS – Performance &amp; Storage Classes [*]</h3><ul>
<li><strong>EFS Scale</strong><ul>
<li>1000s of concurrent NFS clients, 10 GB+ &#x2F;s throughput</li>
<li>Grow to Petabyte-scale network file system, automatically</li>
</ul>
</li>
<li><strong>Performance mode (set at EFS creation time)</strong><ul>
<li>General purpose (default): latency-sensitive use cases (web server, CMS, etc…)</li>
<li>Max I&#x2F;O – higher latency, throughput, highly parallel (big data, media processing)</li>
</ul>
</li>
<li><strong>Storage Tiers (lifecycle management feature – move file after N days)</strong><ul>
<li>Standard: for frequently accessed files</li>
<li>Infrequent access (EFS-IA): cost to retrieve files, lower price to store</li>
</ul>
</li>
</ul>
<p>To set up your EC2 instance:</p>
<p>Using the Amazon EC2 console, associate your EC2 instance with a VPC security group that enables access to your mount target. For example, if you assigned the “default” security group to your mount target, you should assign the “default” security group to your EC2 instance. Learn more
Open an SSH client and connect to your EC2 instance. (Find out how to connect.)
If you’re using an Amazon Linux EC2 instance, install the EFS mount helper with the following command:
sudo yum install -y amazon-efs-utils
You can still use the EFS mount helper if you’re not using an Amazon Linux instance. Learn more</p>
<p>If you’re not using the EFS mount helper, install the NFS client on your EC2 instance:
On a Red Hat Enterprise Linux or SUSE Linux instance, use this command:
sudo yum install -y nfs-utils
On an Ubuntu instance, use this command:
sudo apt-get install nfs-common
Mounting your file system</p>
<p>Open an SSH client and connect to your EC2 instance. (Find out how to connect).
Create a new directory on your EC2 instance, such as “efs”.
sudo mkdir efs
Mount your file system with a method listed following. If you need encryption of data in transit, use the EFS mount helper and the TLS mount option. Mounting considerations
Using the EFS mount helper:
sudo mount -t efs fs-776c8b4f:&#x2F; efs
Using the EFS mount helper and the TLS mount option:
sudo mount -t efs -o tls fs-776c8b4f:&#x2F; efs
Using the NFS client:
sudo mount -t nfs4 -o nfsvers&#x3D;4.1,rsize&#x3D;1048576,wsize&#x3D;1048576,hard,timeo&#x3D;600,retrans&#x3D;2,noresvport fs-776c8b4f.efs.ap-southeast-2.amazonaws.com:&#x2F; efs</p>
<h2 id="EBS-vs-EFS-–-Elastic-Block-Storage"><a href="#EBS-vs-EFS-–-Elastic-Block-Storage" class="headerlink" title="EBS vs EFS – Elastic Block Storage"></a>EBS vs EFS – Elastic Block Storage</h2><ul>
<li>EBS volumes…<ul>
<li>can be attached to <strong>only one instance</strong> at a time</li>
<li>are <strong>locked at the Availability Zone</strong> (AZ) level</li>
<li>gp2: IO increases if the disk size increases</li>
<li>io1: can increase IO independently</li>
</ul>
</li>
<li>To migrate an EBS volume across AZ<ul>
<li>Take a snapshot</li>
<li>Restore the snapshot to another AZ</li>
<li>EBS backups use IO and you shouldn’t run them while your application is handling a lot of traffic</li>
</ul>
</li>
<li>Root EBS Volumes of instances get terminated by default if the EC2 instance gets terminated.</li>
</ul>
<p><strong>EBS vs EFS – Elastic File System</strong></p>
<ul>
<li><p>Mounting 100s of instances across AZ</p>
</li>
<li><p>EFS share website files (WordPress)</p>
</li>
<li><p>Only for Linux Instances (POSIX)</p>
</li>
<li><p>EFS has a higher price point than EBS</p>
</li>
<li><p>Can leverage EFS-IA for cost savings</p>
</li>
<li><p>Remember: EFS vs EBS vs Instance Store</p>
</li>
</ul>
<hr>
<p>EC2 Data Management - EBS &amp; EFS Quiz</p>
<hr>
<p>Question 1:
Your instance in us-east-1a just got terminated, and the attached EBS volume is now available. Your colleague tells you he can’t seem to attach it to your instance in us-east-1b.
A: EBS volumes are AZ locked
  EBS Volumes are created for a specific AZ. It is possible to migrate them between different AZ through backup and restore</p>
<p>Question 2:
You have provisioned an 8TB gp2 EBS volume and you are running out of IOPS. What is NOT a way to increase performance?
A: Increase the EBS volume size.
  EBS IOPS peaks at 16,000 IOPS. or equivalent 5334 GB.</p>
<p>Question 3:
You would like to leverage EBS volumes in parallel to linearly increase performance, while accepting greater failure risks. Which RAID mode helps you in achieving that?
A: RAID 0
  See <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html</a></p>
<p>Question 4:
Although EBS is already a replicated solution, your company SysOps advised you to use a RAID mode that will mirror data and will allow your instance to not be affected if an EBS volume entirely fails. Which RAID mode did he recommend to you?
A: RAID 1
  See <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html</a></p>
<p>Question 5:
You would like to have the same data being accessible as an NFS drive cross AZ on all your EC2 instances. What do you recommend?
  EFS is a network file system (NFS) and allows to mount the same file system on EC2 instances that are in different AZ</p>
<p>Question 6:
You would like to have a high-performance cache for your application that mustn’t be shared. You don’t mind losing the cache upon termination of your instance. Which storage mechanism do you recommend as a Solution Architect?
A: Instance Store
  Instance Store provide the best disk performance</p>
<p>Question 7:
You are running a high-performance database that requires an IOPS of 210,000 for its underlying filesystem. What do you recommend?
A: Use EC2 Instance Store
  Is running a DB on EC2 instance store possible? It is possible to run a database on EC2. It is also possible to use instance store, but there are some considerations to have. The data will be lost if the instance is stopped, but it can be restarted without problems. One can also set up a replication mechanism on another EC2 instance with instance store to have a standby copy. One can also have back-up mechanisms. It’s all up to how you want to set up your architecture to validate your requirements. In this case, it’s around IOPS, and we build an architecture of replication and back up around i</p>
<hr>
<h2 id="RDS-Aurora-amp-ElastiCache"><a href="#RDS-Aurora-amp-ElastiCache" class="headerlink" title="RDS, Aurora &amp; ElastiCache"></a>RDS, Aurora &amp; ElastiCache</h2><h3 id="AWS-RDS-Overview"><a href="#AWS-RDS-Overview" class="headerlink" title="AWS RDS Overview"></a>AWS RDS Overview</h3><ul>
<li>RDS stands for Relational Database Service</li>
<li>It’s a managed DB service for DB use SQL as a query language.</li>
<li>It allows you to create databases in the cloud that are managed by AWS<ul>
<li>Postgres</li>
<li>MySQL</li>
<li>MariaDB</li>
<li>Oracle</li>
<li>Microsoft SQL Server</li>
<li>Aurora (AWS Proprietary database)</li>
</ul>
</li>
</ul>
<h3 id="Advantage-over-using-RDS-versus-deploying-DB-on-EC2"><a href="#Advantage-over-using-RDS-versus-deploying-DB-on-EC2" class="headerlink" title="Advantage over using RDS versus deploying DB on EC2"></a>Advantage over using RDS versus deploying DB on EC2</h3><ul>
<li>RDS is a managed service:<ul>
<li>Automated provisioning, OS patching</li>
<li>Continuous backups and restore to specific timestamp (Point in Time Restore)!</li>
<li>Monitoring dashboards</li>
<li>Read replicas for improved read performance</li>
<li>Multi AZ setup for DR (Disaster Recovery)</li>
<li>Maintenance windows for upgrades</li>
<li>Scaling capability (vertical and horizontal)</li>
<li>Storage backed by EBS (gp2 or io1)</li>
</ul>
</li>
<li>BUT you can’t SSH into your instances</li>
</ul>
<h3 id="RDS-Backups"><a href="#RDS-Backups" class="headerlink" title="RDS Backups"></a>RDS Backups</h3><ul>
<li>Backups are automatically enabled in RDS</li>
<li>Automated backups:<ul>
<li>Daily full backup of the database (during the maintenance window)</li>
<li>Transaction logs are backed-up by RDS every 5 minutes</li>
<li>&#x3D;&gt; ability to restore to any point in time (from oldest backup to 5 minutes ago)</li>
<li>7 days retention (can be increased to 35 days)</li>
</ul>
</li>
<li>DB Snapshots:<ul>
<li>Manually triggered by the user</li>
<li>Retention of backup for as long as you want</li>
</ul>
</li>
</ul>
<h3 id="RDS-Read-Replicas-for-read-scalability"><a href="#RDS-Read-Replicas-for-read-scalability" class="headerlink" title="RDS Read Replicas for read scalability [*]"></a>RDS Read Replicas for read scalability [*]</h3><ul>
<li>Up to 5 Read Replicas</li>
<li>Within AZ, Cross AZ or Cross Region</li>
<li>Replication is <strong>ASYNC</strong>, so reads are eventually consistent</li>
<li>Replicas can be promoted to their own DB</li>
<li>Applications must update the connection string to leverage read replicas</li>
</ul>
<h3 id="RDS-Read-Replicas-–-Use-Cases"><a href="#RDS-Read-Replicas-–-Use-Cases" class="headerlink" title="RDS Read Replicas – Use Cases [*]"></a>RDS Read Replicas – Use Cases [*]</h3><ul>
<li>You have a production database that is taking on normal load</li>
<li>You want to run a reporting application to run some analytics</li>
<li>You create a Read Replica to run the new workload there</li>
<li>The production application is unaffected</li>
<li>Read replicas are used for SELECT (&#x3D;read) only kind of statements (not INSERT, UPDATE, DELETE)</li>
</ul>
<h3 id="RDS-Read-Replicas-–-Network-Cost"><a href="#RDS-Read-Replicas-–-Network-Cost" class="headerlink" title="RDS Read Replicas – Network Cost"></a>RDS Read Replicas – Network Cost</h3><ul>
<li>In AWS there’s a network cost when data goes from one AZ to another.</li>
<li>To reduce the cost, you can have your Read Replicas in the same AZ</li>
</ul>
<h3 id="RDS-Multi-AZ-Disaster-Recovery"><a href="#RDS-Multi-AZ-Disaster-Recovery" class="headerlink" title="RDS Multi AZ (Disaster Recovery)"></a>RDS Multi AZ (Disaster Recovery)</h3><ul>
<li><strong>SYNC</strong> replication</li>
<li>One DNS name – automatic app failover to standby</li>
<li>Increase <strong>availability</strong></li>
<li>Failover in case of loss of AZ, loss of network, instance or storage failure</li>
<li>No manual intervention in apps</li>
<li>Not used for scaling</li>
<li><strong>Note</strong>: <strong>The Read Replicas be setup as Multi AZ for Disaster Recovery (DR)</strong> [*]</li>
</ul>
<h3 id="RDS-Security-Encryption"><a href="#RDS-Security-Encryption" class="headerlink" title="RDS Security - Encryption"></a>RDS Security - Encryption</h3><ul>
<li><p>At rest encryption</p>
<ul>
<li>Possibility to encrypt the master &amp; read replicas with AWS KMS - AES-256 encryption</li>
<li>Encryption has to be defined at launch time</li>
<li><strong>If the master is not encrypted, the read replicas <strong>cannot</strong> be encrypted</strong> [*]</li>
<li>Transparent Data Encryption (TDE) available for Oracle and SQL Server</li>
</ul>
</li>
<li><p>In-flight encryption</p>
<ul>
<li>SSL certificates to encrypt data to RDS in flight</li>
<li>Provide SSL options with trust certificate when connecting to database</li>
<li>To enforce SSL:<ul>
<li>PostgreSQL: rds.force_ssl&#x3D;1 in the AWS RDS Console (Parameter Groups)</li>
<li>MySQL: Within the DB: GRANT USAGE ON <em>.</em> TO ‘mysqluser‘@’%’ REQUIRE SSL;</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="RDS-Encryption-Operations"><a href="#RDS-Encryption-Operations" class="headerlink" title="RDS Encryption Operations"></a>RDS Encryption Operations</h3><ul>
<li><p><strong>Encrypting RDS backups</strong></p>
<ul>
<li>Snapshots of un-encrypted RDS databases are un-encrypted</li>
<li>Snapshots of encrypted RDS databases are encrypted</li>
<li><strong>Can copy a snapshot into an encrypted one</strong></li>
</ul>
</li>
<li><p><strong>To encrypt an un-encrypted RDS database:</strong></p>
<ul>
<li>Create a snapshot of the un-encrypted database</li>
<li>Copy the snapshot and enable encryption for the snapshot</li>
<li>Restore the database from the encrypted snapshot</li>
<li>Migrate applications to the new database, and delete the old database</li>
</ul>
</li>
</ul>
<h3 id="RDS-Security-–-Network-amp-IAM"><a href="#RDS-Security-–-Network-amp-IAM" class="headerlink" title="RDS Security – Network &amp; IAM"></a>RDS Security – Network &amp; IAM</h3><ul>
<li><p>Network Security</p>
<ul>
<li>RDS databases are usually deployed <strong>within a private subnet</strong>, not in a public one</li>
<li>RDS security works by leveraging security groups (the same concept as for EC2 instances) – it controls which IP &#x2F; security group can <strong>communicate</strong> with RDS</li>
</ul>
</li>
<li><p>Access Management</p>
<ul>
<li>IAM policies help control who can <strong>manage</strong> AWS RDS (through the RDS API)</li>
<li>Traditional Username and Password can be used to <strong>login</strong> into the database</li>
<li><strong>IAM-based authentication can be used to login into RDS MySQL &amp; PostgreSQL</strong></li>
</ul>
</li>
</ul>
<h3 id="RDS-IAM-Authentication"><a href="#RDS-IAM-Authentication" class="headerlink" title="RDS - IAM Authentication"></a>RDS - IAM Authentication</h3><ul>
<li><p>IAM database authentication works with <strong>MySQL and PostgreSQL</strong></p>
</li>
<li><p>You don’t need a password, just an authentication token obtained through IAM &amp; RDS API calls</p>
</li>
<li><p>Auth token has a lifetime of 15 minutes</p>
</li>
<li><p>Benefits:</p>
<ul>
<li>Network in&#x2F;out must be encrypted using SSL</li>
<li>IAM to centrally manage users instead of DB</li>
<li>Can leverage IAM Roles and EC2 Instance</li>
</ul>
</li>
</ul>
<h3 id="RDS-Security-–-Summary"><a href="#RDS-Security-–-Summary" class="headerlink" title="RDS Security – Summary"></a>RDS Security – Summary</h3><ul>
<li>Encryption at rest:<ul>
<li>Is done only when you first create the DB instance</li>
<li>or: unencrypted DB &#x3D;&gt; snapshot &#x3D;&gt; copy snapshot as encrypted &#x3D;&gt; create DB from snapshot</li>
</ul>
</li>
<li>Your responsibility:<ul>
<li>Check the ports &#x2F; IP &#x2F; security group inbound rules in DB’s SG</li>
<li>In-database user creation and permissions or manage through IAM</li>
<li>Creating a database with or without public access</li>
<li>Ensure parameter groups or DB is configured to only allow SSL connections</li>
</ul>
</li>
<li>AWS responsibility:<ul>
<li>No SSH access</li>
<li>No manual DB patching</li>
<li>No manual OS patching</li>
<li>No way to audit the underlying instance</li>
</ul>
</li>
</ul>
<h2 id="Amazon-Aurora-Exam-many-question"><a href="#Amazon-Aurora-Exam-many-question" class="headerlink" title="Amazon Aurora [*] Exam many question"></a>Amazon Aurora [*] Exam many question</h2><ul>
<li>Aurora is a proprietary technology from AWS (not open sourced)</li>
<li>Postgres and MySQL are both supported as Aurora DB (that means your drivers will work as if Aurora was a Postgres or MySQL database)</li>
<li>Aurora is “AWS cloud optimized” and claims 5x performance improvement over MySQL on RDS, over 3x the performance of Postgres on RDS</li>
<li>Aurora storage automatically grows in increments of 10GB, up to 64 TB.</li>
<li>Aurora can have 15 replicas while MySQL has 5, and the replication process is faster (sub 10 ms replica lag)</li>
<li>Failover in Aurora is instantaneous. It’s HA (High Availability) native.</li>
<li>Aurora costs more than RDS (20% more) – but is more efficient</li>
</ul>
<h3 id="Aurora-High-Availability-and-Read-Scaling"><a href="#Aurora-High-Availability-and-Read-Scaling" class="headerlink" title="Aurora High Availability and Read Scaling"></a>Aurora High Availability and Read Scaling</h3><ul>
<li>6 copies of your data across 3 AZ:<ul>
<li>4 copies out of 6 needed for writes</li>
<li>3 copies out of 6 need for reads</li>
<li>Self healing with peer-to-peer replication</li>
<li>Storage is striped across 100s of volumes</li>
</ul>
</li>
<li>One Aurora Instance takes writes (master)</li>
<li>Automated failover for master in less than 30 seconds</li>
<li>Master + up to 15 Aurora Read Replicas serve reads</li>
<li>Support for Cross Region Replication Shared storage Volume</li>
</ul>
<p><strong>Replication + Self Healing + Auto Expanding</strong></p>
<h3 id="Aurora-DB-Cluster"><a href="#Aurora-DB-Cluster" class="headerlink" title="Aurora DB Cluster"></a>Aurora DB Cluster</h3><p>Shared storage Volume
Auto Expanding from 10G to 64 TB</p>
<p>Reader Endpoint Connection LB</p>
<h3 id="Features-of-Aurora"><a href="#Features-of-Aurora" class="headerlink" title="Features of Aurora"></a>Features of Aurora</h3><ul>
<li>Automatic fail-over</li>
<li>Backup and Recovery</li>
<li>Isolation and security</li>
<li>Industry compliance</li>
<li>Push-button scaling</li>
<li>Automated Patching with Zero Downtime</li>
<li>Advanced Monitoring</li>
<li>Routine Maintenance</li>
<li>Backtrack: restore data at any point of time without using backups</li>
</ul>
<h3 id="Aurora-Security"><a href="#Aurora-Security" class="headerlink" title="Aurora Security"></a>Aurora Security</h3><ul>
<li>Similar to RDS because uses the same engines</li>
<li>Encryption at rest using KMS</li>
<li>Automated backups, snapshots and replicas are also encrypted</li>
<li>Encryption in flight using SSL (same process as MySQL or Postgres)</li>
<li><hr>
</li>
<li>You are responsible for protecting the instance with security groups</li>
<li>You can’t SSH</li>
</ul>
<h3 id="Aurora-Serverless"><a href="#Aurora-Serverless" class="headerlink" title="Aurora Serverless"></a>Aurora Serverless</h3><ul>
<li>Automated database instantiation and auto</li>
<li><strong>scaling based on actual usage</strong></li>
<li><strong>Good for infrequent, intermittent or unpredictable workloads</strong></li>
<li>No capacity planning needed</li>
<li>Pay per second, can be more cost-effective</li>
</ul>
<h3 id="Global-Aurora"><a href="#Global-Aurora" class="headerlink" title="Global Aurora"></a>Global Aurora</h3><ul>
<li>Aurora Cross Region Read Replicas:<ul>
<li>Useful for disaster recovery</li>
<li>Simple to put in place</li>
</ul>
</li>
<li><strong>Aurora Global Database (recommended)</strong>:<ul>
<li>1 Primary Region (read &#x2F; write)</li>
<li>Up to 5 secondary (read-only) regions, replication lag is less than 1 second</li>
<li>Up to 16 Read Replicas per secondary region</li>
<li>Helps for decreasing latency</li>
<li>Promoting another region (for disaster recovery) has an RTO of &lt; 1 minute</li>
</ul>
</li>
</ul>
<p>dev&#x2F;test db.t2.small</p>
<h3 id="Amazon-ElastiCache-Overview"><a href="#Amazon-ElastiCache-Overview" class="headerlink" title="Amazon ElastiCache Overview"></a>Amazon ElastiCache Overview</h3><ul>
<li>The same way RDS is to get managed Relational Databases…</li>
<li>ElastiCache is to get managed Redis or Memcached</li>
<li>Caches are in-memory databases with really high performance, low latency</li>
<li>Helps reduce load off of databases for read intensive workloads</li>
<li>Helps make your application stateless</li>
<li>Write Scaling using sharding</li>
<li>Read Scaling using REad Replicas</li>
<li>Multi AZ with Failover Replicas.</li>
<li>AWS takes care of OS maintenance &#x2F; patching, optimizations, setup, configuration, monitoring, failure recovery and backups</li>
<li><strong><strong>Using ElastiCache involves heavy application code changes</strong></strong></li>
</ul>
<h2 id="ElastiCache"><a href="#ElastiCache" class="headerlink" title="ElastiCache"></a>ElastiCache</h2><p>Solution Architecture - DB Cache</p>
<ul>
<li>Applications queries ElastiCache, if not available, get from RDS and store in ElastiCache.</li>
<li>Helps relieve load in RDS</li>
<li>Cache must have an invalidation strategy to make sure only the most current data</li>
</ul>
<h3 id="Solution-Architecture-–-User-Session-Store"><a href="#Solution-Architecture-–-User-Session-Store" class="headerlink" title="Solution Architecture – User Session Store"></a>Solution Architecture – User Session Store</h3><ul>
<li>User logs into any of the application</li>
<li>The application writes the session data into ElastiCache</li>
<li>The user hits another instance of our application</li>
<li>The instance retrieves the data and the user is already logged in</li>
</ul>
<h3 id="ElastiCache-–-Redis-vs-Memcached"><a href="#ElastiCache-–-Redis-vs-Memcached" class="headerlink" title="ElastiCache – Redis vs Memcached"></a>ElastiCache – Redis vs Memcached</h3><p><strong>REDIS</strong></p>
<ul>
<li><strong>Multi AZ</strong> with Auto-Failover</li>
<li><strong>Read Replicas</strong> to scale reads and have <strong>high availability</strong></li>
<li>Data Durability using AOF persistence</li>
<li><strong>Backup and restore features</strong>
<strong>MEMCACHED</strong></li>
<li>Multi-node for partitioning of data (sharding)</li>
<li><strong>Non persistent</strong></li>
<li><strong>No backup and restore</strong></li>
<li>Multi-threaded architecture</li>
</ul>
<ul>
<li>Replication</li>
</ul>
<h3 id="ElastiCache-–-Cache-Security"><a href="#ElastiCache-–-Cache-Security" class="headerlink" title="ElastiCache – Cache Security"></a>ElastiCache – Cache Security</h3><ul>
<li>All caches in ElastiCache:<ul>
<li>Support SSL in flight encryption</li>
<li><strong><strong>Do not support IAM authentication</strong></strong></li>
<li>IAM policies on ElastiCache are only used for AWS API-level security</li>
</ul>
</li>
<li><strong>Redis AUTH</strong><ul>
<li>You can set a “password&#x2F;token” when you create a Redis cluster</li>
<li>This is an extra level of security for your cache (on top of security groups)</li>
</ul>
</li>
<li>Memcached<ul>
<li>Supports SASL-based authentication (advanced)</li>
</ul>
</li>
</ul>
<h3 id="ElastiCache-for-Solutions-Architects"><a href="#ElastiCache-for-Solutions-Architects" class="headerlink" title="ElastiCache for Solutions Architects"></a>ElastiCache for Solutions Architects</h3><ul>
<li><p>Patterns for ElastiCache</p>
<ul>
<li><strong>Lazy Loading</strong>: all the read data is cached, data can become stale in cache</li>
<li><strong>Write Through</strong>: Adds or update data in the cache when written to a DB (no stale data)</li>
<li><strong>Session Store</strong>: store temporary session data in a cache (using TTL features)</li>
</ul>
</li>
<li><p>Quote: There are only two hard things in Computer Science: cache invalidation and naming things</p>
</li>
</ul>
<hr>
<p>RDS &#x2F; Aurora &#x2F; ElastiCache Quiz</p>
<p>Quiz 5</p>
<p>Question 1:
My company would like to have a MySQL database internally that is going to be available even in case of a disaster in the AWS Cloud. I should setup
A: Multi AZ</p>
<p>  In this question, we consider a disaster to be an entire Availability Zone going down. In which case Multi-AZ will help. If we want to plan against an entire region going down, backups and replication across regions would help.</p>
<p>Question 2:
Our RDS database struggles to keep up with the demand of the users from our website. Our million users mostly read news, and we don’t post news very often. Which solution is NOT adapted to this problem?
A:   An ElasticCache Cluster
     RDS Read Replicas</p>
<ul>
<li>Multi AZ</li>
</ul>
<p>  Be very careful with the way you read questions at the exam. Here, the question is asking which solution is NOT adapted to this problem. ElastiCache and RDS Read Replicas do indeed help with scaling reads.</p>
<p>Question 3:
We have setup read replicas on our RDS database, but our users are complaining that upon updating their social media posts, they do not see the update right away
A: Read Replicas have asynchronous replication and therefore it’s likely our users will only observe eventual consistency</p>
<p>Question 4:
Which RDS Classic (not Aurora) feature does not require us to change our SQL connection string?
A: Multi AZ</p>
<p>  Multi AZ keeps the same connection string regardless of which database is up. Read Replicas imply we need to reference them individually in our application as each read replica will have its own DNS name</p>
<p>Question 5:
You want to ensure your Redis cluster will always be available
A: X Enable Read Replicas</p>
<ul>
<li>Enable Multi AZ</li>
</ul>
<p>   Multi AZ ensures high availability</p>
<p>Question 6:
Your application functions on an ASG behind an ALB. Users have to constantly log back in and you’d rather not enable stickiness on your ALB as you fear it will overload some servers. What should you do?
A: Store session data in ElasticCache.</p>
<p>  Storing Session Data in ElastiCache is a common pattern to ensuring different instances can retrieve your user’s state if needed.</p>
<p>Question 7:
One analytics application is currently performing its queries against your main production database. These queries slow down the database which impacts the main user experience. What should you do to improve the situation?
A: Setup a Read Replicas</p>
<p>  Read Replicas will help as our analytics application can now perform queries against it, and these queries won’t impact the main production database.</p>
<p>Question 8:
You have a requirement to use TDE (Transparent Data Encryption) on top of KMS. Which database technology does NOT support TDE on RDS?
A: PostgreSQL</p>
<p>Question 9:
Which RDS database technology does NOT support IAM authentication?
A: Oracle</p>
<p>Question 10:
You would like to ensure you have a database available in another region if a disaster happens to your main region. Which database do you recommend?
A: Aurora Global Database</p>
<p>  Global Databases allow you to have cross region replication</p>
<p>Question 11:
How can you enhance the security of your Redis cache to force users to enter a password?
A: Use Redis AUTH</p>
<p>Question 12:
[SAA-C02] Your company has a production Node.js application that is using RDS MySQL 5.6 as its data backend. A new application programmed in Java will perform some heavy analytics workload to create a dashboard, on a regular hourly basis. You want to the final solution to minimize costs and have minimal disruption on the production application, what should you do?
A: Create a Read Replica in the same AZ and run the analytics workload on the replica database</p>
<p>  this will minimize cost because the data won’t have to move across AZ</p>
<p>Question 13:
[SAA-C02] You would like to create a disaster recovery strategy for your RDS PostgreSQL database so that in case of a regional outage, a database can be quickly made available for Read and Write workload in another region. The DR database must be highly available. What do you recommend?
A: Create a RR in a different region and enable multi-AZ on the main database.</p>
<p>Question 14:
You are managing a PostgreSQL database and for security reasons, you would like to ensure users are authenticated using short-lived credentials. What do you suggest doing?
A: Use PostgreSQL for RDS and authenticate using a token obtained through the RDS service.</p>
<p>  In this case, IAM is leveraged to obtain the RDS service token, so this is the IAM authentication use case.</p>
<p>[SAA-C02] An application is running in production, using an Aurora database as its backend. Your development team would like to run a version of the application in a scaled-down application, but still, be able to perform some heavy workload on a need-basis. Most of the time, the application will be unused. Your CIO has tasked you with helping the team while minimizing costs. What do you suggest?
A:</p>
<hr>
<p>[SAA-C02] List of Ports to be familiar with
Here’s a list of standard ports you should see at least once. You shouldn’t remember them (the exam will not test you on that), but you should be able to differentiate between an Important (HTTPS - port 443) and a database port (PostgreSQL - port 5432)</p>
<p>Important ports:</p>
<p>FTP: 21
SSH: 22
SFTP: 22 (same as SSH)
HTTP: 80
HTTPS: 443</p>
<p>vs RDS Databases ports:</p>
<p>PostgreSQL: 5432
MySQL: 3306
Oracle RDS: 1521
MSSQL Server: 1433
MariaDB: 3306 (same as MySQL)</p>
<p>Aurora: 5432 (if PostgreSQL compatible) or 3306 (if MySQL compatible)</p>
<p>Don’t stress out on remember those, just read that list once today and once before going into the exam and you should be all set :)</p>
<p>Remember, you should just be able to differentiate an “Important Port” vs an “RDS database Port”.</p>
<hr>
<h2 id="Route-53-Section"><a href="#Route-53-Section" class="headerlink" title="Route 53 Section"></a>Route 53 Section</h2><ul>
<li>TTL</li>
<li>CNAME vs Alias</li>
<li>Health Checks</li>
<li>Routing Policies<ul>
<li>Simple</li>
<li>Weighted</li>
<li>Latency</li>
<li>Failover</li>
<li>Geolocation</li>
<li>Multi Value</li>
</ul>
</li>
<li>3rd party domains integration</li>
</ul>
<h3 id="AWS-Route-53-Overview"><a href="#AWS-Route-53-Overview" class="headerlink" title="AWS Route 53 Overview"></a>AWS Route 53 Overview</h3><ul>
<li>Route53 is a Managed DNS (Domain Name System)</li>
<li>DNS is a collection of rules and records which helps clients understand how to reach a server through URLs.</li>
<li>In AWS, the most common records are:<ul>
<li>A: hostname to IPv4</li>
<li>AAAA: hostname to IPv6</li>
<li>CNAME: hostname to hostname</li>
<li>Alias: hostname to AWS resource.</li>
</ul>
</li>
</ul>
<h3 id="AWS-Route-53-Overview-1"><a href="#AWS-Route-53-Overview-1" class="headerlink" title="AWS Route 53 Overview"></a>AWS Route 53 Overview</h3><ul>
<li>Route53 can use:</li>
<li>public domain names you own (or buy)
application1.mypublicdomain.com</li>
<li>private domain names that can be resolved by your instances in your VPCs.
application1.company.internal</li>
<li>Route53 has advanced features such as:</li>
<li>Load balancing (through DNS – also called client load balancing)</li>
<li>Health checks (although limited…)</li>
<li>Routing policy: simple, failover, geolocation, latency, weighted, multi value</li>
<li>You pay $0.50 per month per hosted zone</li>
</ul>
<h3 id="DNS-Records-TTL-Time-to-Live"><a href="#DNS-Records-TTL-Time-to-Live" class="headerlink" title="DNS Records TTL (Time to Live)"></a>DNS Records TTL (Time to Live)</h3><ul>
<li><p><strong>High TTL: (e.g. 24hr)</strong></p>
<ul>
<li>Less traffic on DNS</li>
<li>Possibly outdated records</li>
</ul>
</li>
<li><p><strong>Low TTL: (e.g 60 s)</strong></p>
<ul>
<li>More traffic on DNS</li>
<li>Records are outdated for less time</li>
<li>Easy to change records</li>
</ul>
</li>
<li><p><strong>TTL is mandatory for each DNS record</strong></p>
</li>
</ul>
<h3 id="CNAME-vs-Alias"><a href="#CNAME-vs-Alias" class="headerlink" title="CNAME vs Alias [*]"></a>CNAME vs Alias [*]</h3><ul>
<li>AWS Resources (Load Balancer, CloudFront…) expose an AWS hostname:
<em><strong>lb1-1234.us-east-2.elb.amazonaws.com</strong></em> and you want <em><strong>myapp.mydomain.com</strong></em></li>
<li>CNAME:<ul>
<li>Points a hostname to any other hostname. (app.mydomain.com &#x3D;&gt; blabla.anything.com)</li>
<li><em><strong>ONLY FOR NON ROOT DOMAIN (aka. something.mydomain.com)</strong></em></li>
</ul>
</li>
<li>Alias:<ul>
<li>Points a hostname to an AWS Resource (app.mydomain.com &#x3D;&gt; blabla.amazonaws.com)</li>
<li><em><strong>Works for ROOT DOMAIN and NON ROOT DOMAIN (aka mydomain.com)</strong></em></li>
<li>Free of charge</li>
<li>Native health check</li>
</ul>
</li>
</ul>
<h3 id="Simple-Routing-Policy"><a href="#Simple-Routing-Policy" class="headerlink" title="Simple Routing Policy"></a>Simple Routing Policy</h3><ul>
<li>Maps a hostname to another hostname</li>
<li>Use when you need to redirect to a single resource</li>
<li>You can’t attach health checks to simple routing policy</li>
<li>If multiple values are returned, a random one is chosen by the client</li>
</ul>
<h3 id="Weighted-Routing-Policy"><a href="#Weighted-Routing-Policy" class="headerlink" title="Weighted Routing Policy"></a>Weighted Routing Policy</h3><ul>
<li>Control the % of the requests that go to specific endpoint</li>
<li>Helpful to test 1% of traffic on new app version for example</li>
<li>Helpful to split traffic between two regions</li>
<li>Can be associated with Health Checks</li>
</ul>
<h3 id="Latency-Routing-Policy"><a href="#Latency-Routing-Policy" class="headerlink" title="Latency Routing Policy"></a>Latency Routing Policy</h3><ul>
<li>Redirect to the server that has the least latency close to us</li>
<li>Super helpful when latency of users is a priority</li>
<li><strong>Latency is evaluated in terms of user to designated AWS Region</strong></li>
<li>Germany may be directed to the US (if that’s the lowest latency)</li>
</ul>
<h3 id="Health-Checks"><a href="#Health-Checks" class="headerlink" title="Health Checks"></a>Health Checks</h3><ul>
<li><p>Have X health checks failed &#x3D;&gt; unhealthy (default 3)</p>
</li>
<li><p>After X health checks passed &#x3D;&gt; health (default 3)</p>
</li>
<li><p>Default Health Check Interval: 30s (can set to 10s – higher cost)</p>
</li>
<li><p>About 15 health checkers will check the endpoint health</p>
</li>
<li><p>&#x3D;&gt; one request every 2 seconds on average</p>
</li>
<li><p>Can have HTTP, TCP and HTTPS health checks (no SSL verification)</p>
</li>
<li><p>Possibility of integrating the health check with CloudWatch</p>
</li>
<li><p><strong>Health checks can be linked to Route53 DNS queries!</strong></p>
</li>
</ul>
<h3 id="Geo-Location-Routing-Policy"><a href="#Geo-Location-Routing-Policy" class="headerlink" title="Geo Location Routing Policy"></a>Geo Location Routing Policy</h3><ul>
<li>Different from Latency based!</li>
<li><strong>This is routing based on user location</strong></li>
<li>Here we specify: traffic from the UK should go to this specific IP</li>
<li>Should create a “default” policy (in case there’s no match on location)</li>
</ul>
<h3 id="Multi-Value-Routing-Policy"><a href="#Multi-Value-Routing-Policy" class="headerlink" title="Multi Value Routing Policy"></a>Multi Value Routing Policy</h3><ul>
<li>Use when routing traffic to multiple resources</li>
<li>Want to associate a Route 53 health checks with records</li>
<li>Up to 8 healthy records are returned for each Multi Value query</li>
<li><em><strong>Multi Value is not a substitute for having an ELB</strong></em></li>
</ul>
<h3 id="Route53-as-a-Registrar"><a href="#Route53-as-a-Registrar" class="headerlink" title="Route53 as a Registrar"></a>Route53 as a Registrar</h3><ul>
<li>A domain name registrar is an organization that manages the
reservation of Internet domain names</li>
<li>Famous names:</li>
<li>GoDaddy</li>
<li>Google Domains</li>
<li>Etc…</li>
<li>And also… Route53 (e.g. AWS)!</li>
<li><strong>Domain Registrar !&#x3D; DNS</strong></li>
</ul>
<h3 id="3rd-Party-Registrar-with-AWS-Route-53"><a href="#3rd-Party-Registrar-with-AWS-Route-53" class="headerlink" title="3rd Party Registrar with AWS Route 53"></a>3rd Party Registrar with AWS Route 53</h3><ul>
<li><p><strong>If you buy your domain on 3rd party website, you can still use Route53</strong>.</p>
</li>
<li><ol>
<li>Create a Hosted Zone in Route 53</li>
</ol>
</li>
<li><ol start="2">
<li>Update NS Records on 3rd party website to use Route 53 <strong>name servers</strong></li>
</ol>
</li>
<li><p><strong>Domain Registrar !&#x3D; DNS</strong></p>
</li>
<li><p>(But each domain registrar usually comes with some DNS features)</p>
</li>
</ul>
<hr>
<p>Question 1:
You have purchased “mycoolcompany.com” on the AWS registrar and would like for it to point to lb1-1234.us-east-2.elb.amazonaws.com . What sort of Route 53 record is NOT POSSIBLE to set up for this?
A: CNAME</p>
<p>  The DNS protocol does not allow you to create a CNAME record for the top node of a DNS namespace (mycoolcompany.com), also known as the zone apex</p>
<p>Question 2:
You have deployed a new Elastic Beanstalk environment and would like to direct 5% of your production traffic to this new environment, in order to monitor for CloudWatch metrics and ensuring no bugs exist. What type of Route 53 records allows you to do so?
A: Weighted</p>
<p>  Weighted allows you to redirect a part of the traffic based on a weight (hence a percentage). It’s common to use to send a part of a traffic to a new application you’re deploying</p>
<p>Question 3:
After updating a Route 53 record to point “myapp.mydomain.com” from an old Load Balancer to a new load balancer, it looks like the users are still not redirected to your new load balancer. You are wondering why…
A: It’s because of the TTL</p>
<p>  DNS records have a TTL (Time to Live) in order for clients to know for how long to caches these values and not overload the DNS with DNS requests. TTL should be set to strike a balance between how long the value should be cached vs how much pressure should go on the DNS.</p>
<p>Question 4:
You want your users to get the best possible user experience and that means minimizing the response time from your servers to your users. Which routing policy will help?
A: Latency</p>
<p>  Latency will evaluate the latency results and help your users get a DNS response that will minimize their latency (e.g. response time)</p>
<p>Question 5:
You have a legal requirement that people in any country but France should not be able to access your website. Which Route 53 record helps you in achieving this?
A: Geolocation</p>
<p>Question 6:
You have purchased a domain on Godaddy and would like to use it with Route 53. What do you need to change to make this work?
A: Create a public hosted zone and update the 3rd party registrar NS records</p>
<p>  Private hosted zones are meant to be used for internal network queries and are not publicly accessible. Public Hosted Zones are meant to be used for people requesting your website through the public internet. Finally, NS records must be updated on the 3rd party registrar.</p>
<hr>
<h2 id="Classic-Solutions-Architecture"><a href="#Classic-Solutions-Architecture" class="headerlink" title="Classic Solutions Architecture"></a>Classic Solutions Architecture</h2><h3 id="Section-Introduction"><a href="#Section-Introduction" class="headerlink" title="Section Introduction"></a>Section Introduction</h3><ul>
<li><p>These solutions architectures are the best part of this course</p>
</li>
<li><p>Let’s understand how all the technologies we’ve seen work together</p>
</li>
<li><p>This is a section you need to be 100% comfortable with</p>
</li>
<li><p>We’ll see the progression of a Solution’s architect mindset through many sample case studies:</p>
<ul>
<li>WhatIsTheTime.Com</li>
<li>MyClothes.Com</li>
<li>MyWordPress.Com</li>
<li>Instantiating applications quickly</li>
<li>Beanstalk</li>
</ul>
</li>
</ul>
<h3 id="Stateless-Web-App-WhatIsTheTime-com"><a href="#Stateless-Web-App-WhatIsTheTime-com" class="headerlink" title="Stateless Web App: WhatIsTheTime.com"></a>Stateless Web App: WhatIsTheTime.com</h3><ul>
<li><p>WhatIsTheTime.com allows people to know what time it is</p>
</li>
<li><p>We don’t need a database</p>
</li>
<li><p>We want to start small and can accept downtime</p>
</li>
<li><p>We want to fully scale vertically and horizontally, no downtime</p>
</li>
<li><p>Let’s go through the Solutions Architect journey for this app</p>
</li>
<li><p>Let’s see how we can proceed!</p>
</li>
</ul>
<h4 id="In-this-lecture-we’ve-discussed…"><a href="#In-this-lecture-we’ve-discussed…" class="headerlink" title="In this lecture we’ve discussed…"></a>In this lecture we’ve discussed…</h4><ul>
<li>Public vs Private IP and EC2 instances</li>
<li>Elastic IP vs Route 53 vs Load Balancers</li>
<li>Route 53 TTL, A records and Alias Records</li>
<li>Maintaining EC2 instances manually vs Auto Scaling Groups</li>
<li>Multi AZ to survive disasters</li>
<li>ELB Health Checks</li>
<li>Security Group Rules</li>
<li>Reservation of capacity for costing savings when possible</li>
<li>We’re considering 5 pillars for a well architected application:<ul>
<li>costs,</li>
<li>performance,</li>
<li>reliability,</li>
<li>security,</li>
<li>operational excellence</li>
</ul>
</li>
</ul>
<h3 id="Stateful-Web-App-MyClothes-com"><a href="#Stateful-Web-App-MyClothes-com" class="headerlink" title="Stateful Web App: MyClothes.com"></a>Stateful Web App: MyClothes.com</h3><ul>
<li>MyClothes.com allows people to buy clothes online.</li>
<li>There’s a shopping cart</li>
<li>Our website is having hundreds of users at the same time</li>
<li>We need to scale, maintain horizontal scalability and keep our web application as stateless as possible</li>
<li>Users should not lose their shopping cart</li>
<li>Users should have their details (address, etc) in a database</li>
</ul>
<h3 id="In-this-lecture-we’ve-discussed…-1"><a href="#In-this-lecture-we’ve-discussed…-1" class="headerlink" title="In this lecture we’ve discussed…"></a>In this lecture we’ve discussed…</h3><p><strong>3-tier architectures for web applications</strong></p>
<ul>
<li>ELB sticky sessions</li>
<li>Web clients for storing cookies and making our web app stateless</li>
<li>ElastiCache<ul>
<li>For storing sessions (alternative: DynamoDB)</li>
<li>For caching data from RDS</li>
<li>Multi AZ</li>
</ul>
</li>
<li>RDS<ul>
<li>For storing user data</li>
<li>Read replicas for scaling reads</li>
<li>Multi AZ for disaster recovery</li>
</ul>
</li>
<li>Tight Security with security groups referencing each other</li>
</ul>
<h3 id="Stateful-Web-App-MyWordPress-com"><a href="#Stateful-Web-App-MyWordPress-com" class="headerlink" title="Stateful Web App: MyWordPress.com"></a>Stateful Web App: MyWordPress.com</h3><ul>
<li>We are trying to create a fully scalable WordPress website</li>
<li>We want that website to access and correctly display picture uploads</li>
<li>Our user data, and the blog content should be stored in a MySQL database.</li>
</ul>
<p>In this lecture we’ve discussed… - Aurora Database to have easy Multi-AZ and Read
-Replicas</p>
<ul>
<li>Storing data in EBS (single instance application) - Vs Storing data in EFS (distributed application)</li>
</ul>
<h2 id="Instantiating-Applications-quickly"><a href="#Instantiating-Applications-quickly" class="headerlink" title="Instantiating Applications quickly"></a>Instantiating Applications quickly</h2><ul>
<li><p>When launching a full stack (EC2, EBS, RDS), it can take time to:</p>
<ul>
<li>Install applications</li>
<li>Insert initial (or recovery) data</li>
<li>Configure everything</li>
<li>Launch the application</li>
</ul>
</li>
<li><p>We can take advantage of the cloud to speed that up!</p>
</li>
</ul>
<h2 id="Instantiating-Applications-quickly-1"><a href="#Instantiating-Applications-quickly-1" class="headerlink" title="Instantiating Applications quickly"></a>Instantiating Applications quickly</h2><ul>
<li>EC2 Instances:<ul>
<li><strong>Use a Golden AMI</strong>: Install your applications, OS dependencies etc.. beforehand and launch your EC2 instance from the Golden AMI</li>
<li><strong>Bootstrap using User Data</strong>: For dynamic configuration, use User Data scripts</li>
<li><strong>Hybrid</strong>: mix Golden AMI and User Data (Elastic Beanstalk)</li>
</ul>
</li>
<li>RDS Databases:<ul>
<li>Restore from a snapshot: the database will have schemas and data ready!</li>
</ul>
</li>
<li>EBS Volumes:<ul>
<li>Restore from a snapshot: the disk will already be formatted and have data!</li>
</ul>
</li>
</ul>
<h3 id="Developer-problems-on-AWS"><a href="#Developer-problems-on-AWS" class="headerlink" title="Developer problems on AWS"></a>Developer problems on AWS</h3><ul>
<li><p>Managing infrastructure</p>
</li>
<li><p>Deploying Code</p>
</li>
<li><p>Configuring all the databases, load balancers, etc</p>
</li>
<li><p>Scaling concerns</p>
</li>
<li><p>Most web apps have the same architecture (ALB + ASG)</p>
</li>
<li><p>All the developers want is for their code to run!</p>
</li>
<li><p>Possibly, consistently across different applications and environments</p>
</li>
</ul>
<h2 id="AWS-Elastic-Beanstalk-Overview"><a href="#AWS-Elastic-Beanstalk-Overview" class="headerlink" title="AWS Elastic Beanstalk Overview"></a>AWS Elastic Beanstalk Overview</h2><ul>
<li><p>Elastic Beanstalk is a developer centric view of deploying an application
on AWS</p>
</li>
<li><p>It uses all the component’s we’ve seen before:
EC2, ASG, ELB, RDS, etc…</p>
</li>
<li><p>But it’s all in one view that’s easy to make sense of!</p>
</li>
<li><p>We still have full control over the configuration</p>
</li>
<li><p>Beanstalk is free but you pay for the underlying instances</p>
</li>
</ul>
<h3 id="Elastic-Beanstalk"><a href="#Elastic-Beanstalk" class="headerlink" title="Elastic Beanstalk"></a>Elastic Beanstalk</h3><ul>
<li><p>Managed service</p>
<ul>
<li>Instance configuration &#x2F; OS is handled by Beanstalk</li>
<li>Deployment strategy is configurable but performed by Elastic Beanstalk</li>
</ul>
</li>
<li><p>Just the application code is the responsibility of the developer</p>
</li>
<li><p>Three architecture models:</p>
<ul>
<li>Single Instance deployment: good for dev</li>
<li>LB + ASG: great for production or pre-production web applications</li>
<li>ASG only: great for non-web apps in production (workers, etc..)</li>
</ul>
</li>
</ul>
<h3 id="Elastic-Beanstalk-1"><a href="#Elastic-Beanstalk-1" class="headerlink" title="Elastic Beanstalk"></a>Elastic Beanstalk</h3><ul>
<li><p>Elastic Beanstalk has three components</p>
</li>
<li><p>Application</p>
</li>
<li><p>Application version: each deployment gets assigned a version</p>
</li>
<li><p>Environment name (dev, test, prod…): free naming</p>
</li>
<li><p>You deploy application versions to environments and can promote application
versions to the next environment</p>
</li>
<li><p>Rollback feature to previous application version</p>
</li>
<li><p>Full control over lifecycle of environments</p>
</li>
</ul>
<h3 id="Elastic-Beanstalk-2"><a href="#Elastic-Beanstalk-2" class="headerlink" title="Elastic Beanstalk"></a>Elastic Beanstalk</h3><ul>
<li><p>Support for many platforms:</p>
<ul>
<li>Go</li>
<li>Java SE</li>
<li>Java with Tomcat</li>
<li>.NET on Windows Server with IIS</li>
<li>Node.js</li>
<li>PHP</li>
<li>Python</li>
<li>Ruby</li>
<li>Packer Builder</li>
<li>Single Container Docker</li>
<li>Multicontainer Docker</li>
<li>Preconfigured Docker</li>
</ul>
</li>
<li><p>If not supported, you can write your custom platform (advanced)</p>
</li>
</ul>
<hr>
<p>Question 1:
You have an ASG that scales on demand based on the traffic going to your new website: TriangleSunglasses.Com. You would like to optimise for cost, so you have selected an ASG that scales based on demand going through your ELB. Still, you want your solution to be highly available so you have selected the minimum instances to 2. How can you further optimize the cost while respecting the requirements?
A: Reserve two EC2 instances</p>
<p>  This is the way to save further costs as we know we will run 2 EC2 instances no matter what.</p>
<p>Question 2:
Which of the following will NOT help make our application tier stateless?
A: Storing shared data on EBS volumes</p>
<p>  EBS volumes are created for a specific AZ and can only be attached to one EC2 instance at a time. This will not help make our application stateles</p>
<p>Question 3:
You are looking to store shared software updates data across 100s of EC2 instances. The software updates should be dynamically loaded on the EC2 instances and shouldn’t require heavy operations. What do you sugges
A: Store the software updates on EFS and mount EFS as a network drive</p>
<p>  EFS is a network file system (NFS) and allows to mount the same file system to 100s of EC2 instances. Publishing software updates their allow each EC2 instance to access them.</p>
<p>Question 4:
As a solution architect managing a complex ERP software suite, you are orchestrating a migration to the AWS cloud. The software traditionally takes well over an hour to setup on a Linux machine, and you would like to make sure your application does leverage the ASG feature of auto scaling based on the demand. How do you recommend you speed up the installation process?
A: Use a Golden AMI</p>
<p>  Golden AMI are a standard in making sure you snapshot a state after an application installation so that future instances can boot up from that AMI quickly.</p>
<p>Question 5:
I am creating an application and would like for it to be running with minimal cost in a development environment with Elastic Beanstalk. I should run it in
A: Single Instance Mode</p>
<p>  This will create one EC2 instance and one Elastic IP</p>
<p>Question 6:
My deployments on Elastic Beanstalk have been painfully slow, and after looking at the logs, I realize this is due to the fact that my dependencies are resolved on each EC2 machine at deployment time. How can I speed up my deployment with the minimal impact?
A: Create a Golden AMI that contains the dependencies and launch the EC2 instances from that.</p>
<p>  Golden AMI are a standard in making sure save the state after the installation or pulling dependencies so that future instances can boot up from that AMI quickly.</p>
<hr>
<h2 id="S3-Storage-and-Data-Management"><a href="#S3-Storage-and-Data-Management" class="headerlink" title="S3 Storage and Data Management"></a>S3 Storage and Data Management</h2><h3 id="Section-introduction"><a href="#Section-introduction" class="headerlink" title="Section introduction"></a>Section introduction</h3><ul>
<li><p>Amazon S3 is one of the main building blocks of AWS</p>
</li>
<li><p>It’s advertised as <strong>”infinitely scaling”</strong> storage</p>
</li>
<li><p>It’s widely popular and deserves its own section</p>
</li>
<li><p>Many websites use Amazon S3 as a backbone</p>
</li>
<li><p>Many AWS services uses Amazon S3 as an integration as well</p>
</li>
<li><p>We’ll have a step-by-step approach to S3</p>
</li>
</ul>
<h3 id="Amazon-S3-Overview-Buckets"><a href="#Amazon-S3-Overview-Buckets" class="headerlink" title="Amazon S3 Overview - Buckets"></a>Amazon S3 Overview - Buckets</h3><ul>
<li>Amazon S3 allows people to store objects (files) in “buckets” (directories)</li>
<li>Buckets must have a <strong>globally unique name</strong></li>
<li>Buckets are defined at the <strong>region level</strong><ul>
<li>Naming convention</li>
<li>No uppercase</li>
<li>No underscore</li>
<li>3-63 characters long</li>
<li>Not an IP</li>
<li>Must start with lowercase letter or number</li>
</ul>
</li>
</ul>
<h2 id="Amazon-S3-Overview-Objects"><a href="#Amazon-S3-Overview-Objects" class="headerlink" title="Amazon S3 Overview - Objects"></a>Amazon S3 Overview - Objects</h2><ul>
<li>Objects (files) have a Key</li>
<li>The <strong>key</strong> is the <strong>FULL</strong> path:<ul>
<li>s3:&#x2F;&#x2F;my-bucket&#x2F;my_file.txt</li>
<li>s3:&#x2F;&#x2F;my-bucket&#x2F;my_folder1&#x2F;another_folder&#x2F;my_file.txt</li>
</ul>
</li>
<li>The key is composed of prefix + object name<ul>
<li>s3:&#x2F;&#x2F;my-bucket&#x2F;my_folder1&#x2F;another_folder&#x2F;my_file.txt</li>
</ul>
</li>
<li>There’s no concept of “directories” within buckets (although the UI will trick you to think otherwise)</li>
<li>Just keys with very long names that contain slashes (“&#x2F;”)</li>
</ul>
<h2 id="Amazon-S3-Overview-–-Objects-continued"><a href="#Amazon-S3-Overview-–-Objects-continued" class="headerlink" title="Amazon S3 Overview – Objects (continued)"></a>Amazon S3 Overview – Objects (continued)</h2><ul>
<li><p>Object values are the content of the body:</p>
<ul>
<li>Max Object Size is 5TB (5000GB)</li>
<li>If uploading more than 5GB, must use “multi-part upload”</li>
</ul>
</li>
<li><p>Metadata (list of text key &#x2F; value pairs – system or user metadata)</p>
</li>
<li><p>Tags (Unicode key &#x2F; value pair – up to 10) – useful for security &#x2F; lifecycle</p>
</li>
<li><p>Version ID (if versioning is enabled)</p>
</li>
</ul>
<h3 id="Amazon-S3-Versioning"><a href="#Amazon-S3-Versioning" class="headerlink" title="Amazon S3 - Versioning"></a>Amazon S3 - Versioning</h3><ul>
<li>You can version your files in Amazon S3</li>
<li>It is enabled at the <strong>bucket level</strong></li>
<li>Same key overwrite will increment the “version”: 1, 2, 3….</li>
<li>It is best practice to version your buckets<ul>
<li>Protect against unintended deletes (ability to restore a version)</li>
<li>Easy roll back to previous version</li>
</ul>
</li>
<li>Notes:<ul>
<li>Any file that is not versioned prior to enabling versioning will have version “null”</li>
<li>Suspending versioning does not delete the previous versions</li>
</ul>
</li>
</ul>
<h3 id="S3-Encryption-for-Objects"><a href="#S3-Encryption-for-Objects" class="headerlink" title="S3 Encryption for Objects [*]"></a>S3 Encryption for Objects [*]</h3><ul>
<li><p>There are 4 methods of encrypting objects in S3</p>
<ul>
<li>SSE-S3: encrypts S3 objects using keys handled &amp; managed by AWS</li>
<li>SSE-KMS: leverage AWS Key Management Service to manage encryption keys</li>
<li>SSE-C: when you want to manage your own encryption keys</li>
<li>Client Side Encryption</li>
</ul>
</li>
<li><p>It’s important to understand which ones are adapted to which situation
for the exam</p>
</li>
</ul>
<h3 id="SSE-S3"><a href="#SSE-S3" class="headerlink" title="SSE-S3"></a>SSE-S3</h3><ul>
<li>SSE-S3: encryption using keys handled &amp; managed by Amazon S3</li>
<li>Object is encrypted server side</li>
<li>AES-256 encryption type</li>
<li>Must set header: <em><strong>“x-amz-server-side-encryption”: “AES256”</strong></em></li>
</ul>
<h3 id="SSE-KMS"><a href="#SSE-KMS" class="headerlink" title="SSE-KMS"></a>SSE-KMS</h3><ul>
<li>SSE-KMS: encryption using keys handled &amp; managed by KMS</li>
<li>KMS Advantages: user control + audit trail</li>
<li>Object is encrypted server side</li>
<li>Must set header: <strong>“x-amz-server-side-encryption”: ”aws:kms”</strong></li>
</ul>
<h3 id="SSE-C"><a href="#SSE-C" class="headerlink" title="SSE-C"></a>SSE-C</h3><ul>
<li>SSE-C: server-side encryption using data keys fully managed by the customer outside of AWS</li>
<li>Amazon S3 does not store the encryption key you provide</li>
<li><strong>HTTPS must be used</strong></li>
<li>Encryption key must provided in HTTP headers, for every HTTP request made</li>
</ul>
<h3 id="Client-Side-Encryption"><a href="#Client-Side-Encryption" class="headerlink" title="Client Side Encryption"></a>Client Side Encryption</h3><ul>
<li>Client library such as the Amazon S3 Encryption Client</li>
<li>Clients must encrypt data themselves before sending to S3</li>
<li>Clients must decrypt data themselves when retrieving from S3</li>
<li>Customer fully manages the keys and encryption cycle’</li>
</ul>
<h3 id="Encryption-in-transit-SSL-x2F-TLS"><a href="#Encryption-in-transit-SSL-x2F-TLS" class="headerlink" title="Encryption in transit (SSL&#x2F;TLS)"></a>Encryption in transit (SSL&#x2F;TLS)</h3><ul>
<li><p>Amazon S3 exposes:</p>
<ul>
<li>HTTP endpoint: non encrypted</li>
<li>HTTPS endpoint: encryption in flight</li>
</ul>
</li>
<li><p>You’re free to use the endpoint you want, but HTTPS is recommended</p>
</li>
<li><p>Most clients would use the HTTPS endpoint by default</p>
</li>
<li><p>HTTPS is mandatory for SSE-C</p>
</li>
<li><p>Encryption in flight is also called SSL &#x2F; TLS</p>
</li>
</ul>
<h2 id="S3-Security"><a href="#S3-Security" class="headerlink" title="S3 Security"></a>S3 Security</h2><ul>
<li><strong>User based</strong><ul>
<li>IAM policies - which API calls should be allowed for a specific user from IAM
console</li>
</ul>
</li>
<li><strong>Resource Based</strong><ul>
<li>Bucket Policies - bucket wide rules from the S3 console - allows cross account</li>
<li>Object Access Control List (ACL) – finer grain</li>
<li>Bucket Access Control List (ACL) – less common</li>
</ul>
</li>
<li><strong>Note</strong>: an IAM principal can access an S3 object if</li>
<li>the user IAM permissions allow it <strong>OR</strong> the resource policy ALLOWS it</li>
<li><strong>AND</strong> there’s no explicit DENY</li>
</ul>
<h3 id="S3-Bucket-Policies"><a href="#S3-Bucket-Policies" class="headerlink" title="S3 Bucket Policies"></a>S3 Bucket Policies</h3><ul>
<li><p>JSON based policies</p>
<ul>
<li>Resources: buckets and objects</li>
<li>Actions: Set of API to Allow or Deny</li>
<li>Effect: Allow &#x2F; Deny</li>
<li>Principal: The account or user to apply the policy to</li>
</ul>
</li>
<li><p>Use S3 bucket for policy to:</p>
<ul>
<li>Grant public access to the bucket</li>
<li>Force objects to be encrypted at upload - Grant access to another account (Cross Account)</li>
</ul>
</li>
</ul>
<h2 id="Bucket-settings-for-Block-Public-Access"><a href="#Bucket-settings-for-Block-Public-Access" class="headerlink" title="Bucket settings for Block Public Access"></a>Bucket settings for Block Public Access</h2><ul>
<li><p>Block public access to buckets and objects granted through</p>
<ul>
<li>new access control lists (ACLs)</li>
<li>any access control lists (ACLs)</li>
<li>new public bucket or access point policies</li>
</ul>
</li>
<li><p>Block public and cross-account access to buckets and objects
through any public bucket or access point policies</p>
</li>
<li><p><strong>These settings were created to prevent company data leaks</strong></p>
</li>
<li><p>If you know your bucket should never be public, leave these on</p>
</li>
<li><p>Can be set at the account level</p>
</li>
</ul>
<h3 id="S3-Security-Other"><a href="#S3-Security-Other" class="headerlink" title="S3 Security - Other"></a>S3 Security - Other</h3><ul>
<li>Networking:<ul>
<li>Supports VPC Endpoints (for instances in VPC without www internet)</li>
</ul>
</li>
<li>Logging and Audit:<ul>
<li>S3 Access Logs can be stored in other S3 bucket</li>
<li>API calls can be logged in AWS CloudTrail</li>
</ul>
</li>
<li>User Security:<ul>
<li>MFA Delete: MFA (multi factor authentication) can be required in versioned
buckets to delete objects</li>
<li>Pre-Signed URLs: URLs that are valid only for a limited time (ex: premium video service for logged in users)</li>
</ul>
</li>
</ul>
<h3 id="S3-Websites"><a href="#S3-Websites" class="headerlink" title="S3 Websites"></a>S3 Websites</h3><ul>
<li><p>S3 can host static websites and have them accessible on the www</p>
</li>
<li><p>The website URL will be:</p>
<ul>
<li><bucket-name>.s3-website-<AWS-region>.amazonaws.com
OR</li>
<li><bucket-name>.s3-website.<AWS-region>.amazonaws.com</li>
</ul>
</li>
<li><p>If you get a 403 (Forbidden) error, make sure the bucket policy allows
public reads!</p>
</li>
</ul>
<h3 id="CORS-Explained"><a href="#CORS-Explained" class="headerlink" title="CORS - Explained"></a>CORS - Explained</h3><ul>
<li>An <strong>origin</strong> is a scheme (protocol), host (domain) and port<ul>
<li>E.g.: <a target="_blank" rel="noopener" href="https://www.example.com/">https://www.example.com</a> (implied port is 443 for HTTPS, 80 for HTTP)</li>
</ul>
</li>
<li>CORS means Cross-Origin Resource Sharing</li>
<li><em><strong>Web Browser</strong></em> based mechanism to allow requests to other origins while
visiting the main origin</li>
<li>Same origin: <a target="_blank" rel="noopener" href="http://example.com/app1">http://example.com/app1</a> &amp; <a target="_blank" rel="noopener" href="http://example.com/app2">http://example.com/app2</a></li>
<li>Different origins: <a target="_blank" rel="noopener" href="http://www.example.com/">http://www.example.com</a> &amp; <a target="_blank" rel="noopener" href="http://other.example.com/">http://other.example.com</a></li>
<li>The requests won’t be fulfilled unless the other origin allows for the
requests, using <strong>CORS Headers (ex: Access-Control-Allow-Origin)</strong></li>
</ul>
<p>Access-Control-Allow-Origin: <a target="_blank" rel="noopener" href="https://www.example.com/">https://www.example.com</a>
Access-Control-Allow-Methods: GET, PUT, DELETE
CORS – Diagram</p>
<h3 id="S3-CORS"><a href="#S3-CORS" class="headerlink" title="S3 CORS"></a>S3 CORS</h3><ul>
<li>If a client does a cross-origin request on our S3 bucket, we need to
enable the correct CORS headers</li>
<li><strong>It’s a popular exam question</strong> [*]</li>
<li>You can allow for a specific origin or for * (all origins)</li>
</ul>
<h3 id="Amazon-S3-Consistency-Model"><a href="#Amazon-S3-Consistency-Model" class="headerlink" title="Amazon S3 - Consistency Model"></a>Amazon S3 - Consistency Model</h3><ul>
<li><p><strong>Read after write consistency for PUTS of new objects</strong></p>
<ul>
<li>As soon as a new object is written, we can retrieve it
ex: (PUT 200 &#x3D;&gt; GET 200)</li>
<li>This is true, except if we did a GET before to see if the object existed
ex: (GET 404 &#x3D;&gt; PUT 200 &#x3D;&gt; GET 404) – eventually consistent</li>
</ul>
</li>
<li><p><strong>Eventual Consistency for DELETES and PUTS of existing objects</strong></p>
<ul>
<li>If we read an object after updating, we might get the older version
ex: (PUT 200 &#x3D;&gt; PUT 200 &#x3D;&gt; GET 200 (might be older version))</li>
<li>If we delete an object, we might still be able to retrieve it for a short time
ex: (DELETE 200 &#x3D;&gt; GET 200)</li>
</ul>
</li>
<li><p>Note: there’s no way to request “strong consistency”</p>
</li>
</ul>
<hr>
<p>Question 1:
You’re trying to upload a 25 GB file on S3 and it’s not working
A: X</p>
<p>Question 2:
I tried creating an S3 bucket named “dev” but it didn’t work. This is a new AWS Account and I have no buckets at all. What is the cause?
A: Bucket names must be globally unique and “dev” is already taken</p>
<p>Question 3:
You’ve added files in your bucket and then enabled versioning. The files you’ve already added will have which version?
A: null</p>
<p>Question 4:
Your client wants to make sure the encryption is happening in S3, but wants to fully manage the encryption keys and never store them in AWS. You recommend
A: X SSE-C</p>
<p>Question 5:
Your company wants data to be encrypted in S3, and maintain control of the rotation policy for the encryption keys, but not know the encryption keys values. You recommend
A: SSE-KMS</p>
<p>  With SSE-KMS you let AWS manage the encryption keys but you have full control of the key rotation policy</p>
<p>Question 6:
Your company does not trust S3 for encryption and wants it to happen on the application. You recommend
A: With Client Side Encryption you perform the encryption yourself and send the encrypted data to AWS directly. AWS does not know your encryption keys and cannot decrypt your data.</p>
<p>Question 7:
The bucket policy allows our users to read&#x2F;write files in the bucket, yet we were not able to perform a PutObject API call.
A: The IAM user must have an explicit DENY in the attached IAM policy</p>
<p>  Explicit DENY in an IAM policy will take precedence over a bucket policy permission</p>
<p>Question 8:
You have a website that loads files from another S3 bucket. When you try the URL of the files directly in your Chrome browser it works, but when the website you’re visiting tries to load these files it doesn’t. What’s the problem?
A: CORS is wrong</p>
<p>  Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. To learn more about CORS, go here: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html</a></p>
<hr>
<p>Developing on AWS</p>
<h2 id="Section-Introduction-1"><a href="#Section-Introduction-1" class="headerlink" title="Section Introduction"></a>Section Introduction</h2><ul>
<li>So far, we’ve interacts with services manually and they exposed standard
information for clients:</li>
<li>EC2 exposes a standard Linux machine we can use any way we want</li>
<li>RDS exposes a standard database we can connect to using a URL</li>
<li>ElastiCache exposes a cache URL we can connect to using a URL</li>
<li>ASG &#x2F; ELB are automated and we don’t have to program against them</li>
<li>Route53 was setup manual</li>
<li>Developing against AWS has two components:</li>
<li>How to perform interactions with AWS without using the Online Console?</li>
<li>How to interact with AWS Proprietary services? (S3, DynamoDB, etc…)</li>
</ul>
<h2 id="Section-Introduction-2"><a href="#Section-Introduction-2" class="headerlink" title="Section Introduction"></a>Section Introduction</h2><ul>
<li>Developing and performing AWS tasks against AWS can be done in
several ways</li>
<li>Using the AWS CLI on our local computer</li>
<li>Using the AWS CLI on our EC2 machines</li>
<li>Using the AWS SDK on our local computer</li>
<li>Using the AWS SDK on our EC2 machines</li>
<li>Using the AWS Instance Metadata Service for EC2</li>
<li>In this section, we’ll learn:</li>
<li>How to do all of those</li>
<li>In the right &amp; most secure way, adhering to best practices</li>
</ul>
<p>AWS NETWORK</p>
<h3 id="AWS-CLI-Configuration"><a href="#AWS-CLI-Configuration" class="headerlink" title="AWS CLI Configuration"></a>AWS CLI Configuration</h3><ul>
<li>Let’s learn how to properly configure the CLI</li>
<li>We’ll learn how to get our access credentials and protect them</li>
<li>Do not share your AWS Access Key and Secret key with anyone!</li>
</ul>
<h3 id="AWS-CLI-ON-EC2…-THE-BAD-WAY"><a href="#AWS-CLI-ON-EC2…-THE-BAD-WAY" class="headerlink" title="AWS CLI ON EC2… THE BAD WAY"></a>AWS CLI ON EC2… THE BAD WAY</h3><ul>
<li><p>We could run <code>aws configure</code> on EC2 just like we did (and it’ll work)</p>
</li>
<li><p>But… it’s SUPER INSECURE</p>
</li>
<li><p><strong>NEVER EVER EVER PUT YOUR PERSONAL CREDENTIALS ON AN EC2</strong></p>
</li>
<li><p>Your <strong>PERSONAL</strong> credentials are <strong>PERSONAL</strong> and only belong on your <strong>PERSONAL</strong> computer</p>
</li>
<li><p>If the EC2 is compromised, so is your personal account</p>
</li>
<li><p>If the EC2 is shared, other people may perform AWS actions while impersonating you</p>
</li>
<li><p>For EC2, there’s a better way… it’s called AWS IAM Roles</p>
</li>
</ul>
<h3 id="AWS-CLI-ON-EC2…-THE-RIGHT-WAY"><a href="#AWS-CLI-ON-EC2…-THE-RIGHT-WAY" class="headerlink" title="AWS CLI ON EC2… THE RIGHT WAY"></a>AWS CLI ON EC2… THE RIGHT WAY</h3><ul>
<li><strong>IAM Roles can be attached to EC2 instances</strong></li>
<li>IAM Roles can come with a policy authorizing exactly what the EC2 instance should be able to do</li>
<li>EC2 Instances can then use these profiles automatically without any additional configurations</li>
<li><strong>This is the best practice on AWS and you should 100% do this.</strong></li>
</ul>
<h3 id="AWS-EC2-Instance-Metadata"><a href="#AWS-EC2-Instance-Metadata" class="headerlink" title="AWS EC2 Instance Metadata"></a>AWS EC2 Instance Metadata</h3><ul>
<li>AWS EC2 Instance Metadata is powerful but one of the least known features to developers</li>
<li>It allows AWS EC2 instances to ”learn about themselves” <strong>without using an IAM Role for that purpose</strong>.</li>
<li>The URL is <a target="_blank" rel="noopener" href="http://169.254.169.254/latest/meta-data">http://169.254.169.254/latest/meta-data</a></li>
<li>You can retrieve the IAM Role name from the metadata, but you CANNOT
retrieve the IAM Policy.</li>
<li>Metadata &#x3D; Info about the EC2 instance</li>
<li>Userdata &#x3D; launch script of the EC2 instance</li>
</ul>
<h3 id="AWS-SDK-Overview"><a href="#AWS-SDK-Overview" class="headerlink" title="AWS SDK Overview"></a>AWS SDK Overview</h3><ul>
<li>What if you want to perform actions on AWS directly from your applications
code ? (without using the CLI).</li>
<li>You can use an SDK (software development kit) !</li>
<li>Official SDKs are…<ul>
<li>Java</li>
<li>.NET</li>
<li>Node.js</li>
<li>PHP</li>
<li>Python (named boto3 &#x2F; botocore)</li>
<li>Go</li>
<li>Ruby</li>
<li>C++</li>
</ul>
</li>
</ul>
<h3 id="AWS-SDK-Overview-1"><a href="#AWS-SDK-Overview-1" class="headerlink" title="AWS SDK Overview"></a>AWS SDK Overview</h3><ul>
<li><p>We have to use the AWS SDK when coding against AWS Services such as DynamoDB</p>
</li>
<li><p>Fun fact… the AWS CLI uses the Python SDK (boto3)</p>
</li>
<li><p>The exam expects you to know when you should use an SDK</p>
</li>
<li><p>We’ll practice the AWS SDK when we get to the Lambda functions</p>
</li>
<li><p>Good to know: if you don’t specify or configure a default region, then us-east-1 will be chosen by default</p>
</li>
</ul>
<h3 id="AWS-SDK-Credentials-Security"><a href="#AWS-SDK-Credentials-Security" class="headerlink" title="AWS SDK Credentials Security"></a>AWS SDK Credentials Security</h3><ul>
<li><p>It’s recommend to use the <strong>default credential provider chain</strong></p>
</li>
<li><p>The <strong>default credential provider chain</strong> works seamlessly with:</p>
<ul>
<li>AWS credentials at ~&#x2F;.aws&#x2F;credentials (only on our computers or on premise)</li>
<li>Instance Profile Credentials using IAM Roles (for EC2 machines, etc…)</li>
<li>Environment variables (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)</li>
</ul>
</li>
<li><p><strong>Overall, NEVER EVER STORE AWS CREDENTIALS IN YOUR CODE.</strong></p>
</li>
<li><p><strong>Best practice is for credentials to be inherited from mechanisms above, and 100% IAM Roles if working from within AWS Services</strong></p>
</li>
</ul>
<h3 id="Exponential-Backoff"><a href="#Exponential-Backoff" class="headerlink" title="Exponential Backoff"></a>Exponential Backoff</h3><ul>
<li>Any API that fails because of too many calls needs to be retried with Exponential Backoff</li>
<li>These apply to rate limited API</li>
<li>Retry mechanism included in SDK API calls</li>
</ul>
<hr>
<p>Question 1:
My EC2 Instance does not have the permissions to perform an API call PutObject on S3. What should I do?
A:</p>
<p>  IAM roles are the right way to provide credentials and permissions to an EC2 instance</p>
<p>Question 2:
I have an on-premise personal server that I’d like to use to perform AWS API calls
  A: O I should run <code>aws configure</code> and put my credentials there. Invalidate them when I’m done</p>
<pre><code>Even better would be to create a user specifically for that one on-premise server
</code></pre>
<p>  A: X I should attach an EC2 IAM Role to my personal server</p>
<pre><code>you can&#39;t attach EC2 IAM roles to on premise servers
</code></pre>
<p>Question 3:
I need my colleagues help to debug my code. When he runs the application on his machine, it’s working fine, whereas I get API authorisation exceptions. What should I do?
A: Compare his IAM policy and my IAM policy in the policy simulator to understand the differences</p>
<p>Question 4:
To get the instance id of my EC2 machine from the EC2 machine, the best thing is to…
A: Query the meta data at <a target="_blank" rel="noopener" href="http://169.254.169.254/latest/meta-data">http://169.254.169.254/latest/meta-data</a></p>
<hr>
<p>Advanced S3
<a target="_blank" rel="noopener" href="http://www.datacumulus.com/">www.datacumulus.com</a>
S3, Glacier, Athena</p>
<p>S3 MFA-Delete</p>
<ul>
<li>MFA (multi factor authentication) forces user to generate a code on a device (usually a
mobile phone or hardware) before doing important operations on S3</li>
<li>To use MFA-Delete, enable Versioning on the S3 bucket</li>
<li>You will need MFA to</li>
<li>permanently delete an object version</li>
<li>suspend versioning on the bucket</li>
<li>You won’t need MFA for</li>
<li>enabling versioning</li>
<li>listing deleted versions</li>
<li>Only the bucket owner (root account) can enable&#x2F;disable MFA-Delete</li>
<li>MFA-Delete currently can only be enabled using the CLI</li>
</ul>
<p>S3 Default Encryption vs Bucket Policies</p>
<ul>
<li>The old way to enable default encryption was to use a bucket policy
and refuse any HTTP command without the proper headers:</li>
<li>The new way is to use the “default encryption” option in S3</li>
<li>Note: Bucket Policies are evaluated before “default encryption”</li>
</ul>
<p>S3 Access Logs</p>
<ul>
<li>For audit purpose, you may want to log all access to S3
buckets</li>
<li>Any request made to S3, from any account, authorized or
denied, will be logged into another S3 bucket</li>
<li>That data can be analyzed using data analysis tools…</li>
<li>Or Amazon Athena as we’ll see later in this section!</li>
<li>The log format is at:
<a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonS3/latest/dev/LogFo">https://docs.aws.amazon.com/AmazonS3/latest/dev/LogFo</a>
rmat.html
My-bucket
Logging Bucket
requests
Log all
requests</li>
</ul>
<p>S3 Access Logs: Warning</p>
<ul>
<li>Do not set your logging bucket to be the monitored bucket</li>
<li>It will create a logging loop, and your bucket will grow in size exponentially
App Bucket &amp;
Logging Bucket
Logging loop
PutObject
Do not try this at home J</li>
</ul>
<h3 id="S3-Replication-CRR-amp-SRR"><a href="#S3-Replication-CRR-amp-SRR" class="headerlink" title="S3 Replication (CRR &amp; SRR)"></a>S3 Replication (CRR &amp; SRR)</h3><ul>
<li><p>Must enable versioning in source and destination</p>
</li>
<li><p>Cross Region Replication (CRR)</p>
</li>
<li><p>Same Region Replication (SRR)</p>
</li>
<li><p>Buckets can be in different accounts</p>
</li>
<li><p>Copying is asynchronous</p>
</li>
<li><p>Must give proper IAM permissions to S3</p>
</li>
<li><p><strong>CRR - Use cases</strong>: compliance, lower latency access, replication across accounts</p>
</li>
<li><p><strong>SRR - Use cases</strong>: log aggregation, live replication between production and test accounts</p>
</li>
</ul>
<h3 id="S3-Replication-–-Notes"><a href="#S3-Replication-–-Notes" class="headerlink" title="S3 Replication – Notes"></a>S3 Replication – Notes</h3><ul>
<li>After activating, only new objects are replicated (not retroactive)</li>
<li>For DELETE operations:</li>
<li>If you delete without a version ID, it adds a delete marker, not replicated</li>
<li>If you delete with a version ID, it deletes in the source, not replicated</li>
<li>There is no “chaining” of replication</li>
<li>If bucket 1 has replication into bucket 2, which has replication into bucket 3</li>
<li>Then objects created in bucket 1 are not replicated to bucket 3</li>
</ul>
<h3 id="S3-Pre-Signed-URLs"><a href="#S3-Pre-Signed-URLs" class="headerlink" title="S3 Pre-Signed URLs"></a>S3 Pre-Signed URLs</h3><ul>
<li>Can generate pre-signed URLs using SDK or CLI</li>
<li>For downloads (easy, can use the CLI)</li>
<li>For uploads (harder, must use the SDK)</li>
<li>Valid for a default of 3600 seconds, can change timeout with –expires-in
[TIME_BY_SECONDS] argument</li>
<li>Users given a pre-signed URL inherit the permissions of the person who generated the URL for GET &#x2F; PUT</li>
<li>Examples :</li>
<li>Allow only logged-in users to download a premium video on your S3 bucket</li>
<li>Allow an ever changing list of users to download files by generating URLs dynamically</li>
<li>Allow temporarily a user to upload a file to a precise location in our bucket</li>
</ul>
<p>aws –profile pablo-developer s3 presign s3:&#x2F;&#x2F;my-sample-bucket-monitored-pablo&#x2F;beach.jpg –expires-in 300 –region ap-southeast-2
<a target="_blank" rel="noopener" href="https://my-sample-bucket-monitored-pablo.s3.ap-southeast-2.amazonaws.com/beach.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAYZ33DWH4ML6QNG3/20200527/ap-southeast-2/s3/aws4_request&X-Amz-Date=20200527T035233Z&X-Amz-Expires=300&X-Amz-SignedHeaders=host&X-Amz-Signature=d3574302ba10d7197de6b58cf605a1b73c49e3ccf064410acb1c251c42fbe38e">https://my-sample-bucket-monitored-pablo.s3.ap-southeast-2.amazonaws.com/beach.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAYZ33DWH4ML6QNG3%2F20200527%2Fap-southeast-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20200527T035233Z&amp;X-Amz-Expires=300&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=d3574302ba10d7197de6b58cf605a1b73c49e3ccf064410acb1c251c42fbe38e</a></p>
<h3 id="S3-Storage-Classes"><a href="#S3-Storage-Classes" class="headerlink" title="S3 Storage Classes [*]"></a>S3 Storage Classes [*]</h3><ul>
<li><p>Amazon S3 Standard - General Purpose</p>
</li>
<li><p>Amazon S3 Standard-Infrequent Access (IA)</p>
</li>
<li><p>Amazon S3 One Zone-Infrequent Access</p>
</li>
<li><p>Amazon S3 Intelligent Tiering</p>
</li>
<li><p>Amazon Glacier</p>
</li>
<li><p>Amazon Glacier Deep Archive</p>
</li>
<li><p>Amazon S3 Reduced Redundancy Storage (deprecated - omitted)</p>
</li>
</ul>
<h3 id="S3-Standard-–-General-Purpose"><a href="#S3-Standard-–-General-Purpose" class="headerlink" title="S3 Standard – General Purpose"></a>S3 Standard – General Purpose</h3><ul>
<li><p>High durability (99.999999999%) of objects across multiple AZ</p>
</li>
<li><p>If you store 10,000,000 objects with Amazon S3, you can on average
expect to incur a loss of a single object once every 10,000 years</p>
</li>
<li><p>99.99% Availability over a given year</p>
</li>
<li><p>Sustain 2 concurrent facility failures</p>
</li>
<li><p><strong>Use Cases</strong>: <strong>Big Data analytics, mobile &amp; gaming applications, content distribution…</strong></p>
</li>
</ul>
<h3 id="S3-Standard-–-Infrequent-Access-IA"><a href="#S3-Standard-–-Infrequent-Access-IA" class="headerlink" title="S3 Standard – Infrequent Access (IA)"></a>S3 Standard – Infrequent Access (IA)</h3><ul>
<li><p>Suitable for data that is less frequently accessed, but requires rapid
access when needed</p>
</li>
<li><p>High durability (99.999999999%) of objects across multiple AZs</p>
</li>
<li><p>99.9% Availability</p>
</li>
<li><p>Low cost compared to Amazon S3 Standard</p>
</li>
<li><p>Sustain 2 concurrent facility failures</p>
</li>
<li><p><strong>Use Cases</strong>: <strong>As a data store for disaster recovery, backups…</strong></p>
</li>
</ul>
<p>S3 One Zone - Infrequent Access (IA)</p>
<ul>
<li><p>Same as IA but data is stored in a single AZ</p>
</li>
<li><p>High durability (99.999999999%) of objects in a single AZ; data lost when AZ
is destroyed</p>
</li>
<li><p>99.5% Availability</p>
</li>
<li><p>Low latency and high throughput performance</p>
</li>
<li><p>Supports SSL for data at transit and encryption at rest</p>
</li>
<li><p>Low cost compared to IA (by 20%)</p>
</li>
<li><p><strong>Use Cases: Storing secondary backup copies of on-premise data, or storing
data you can recreate</strong></p>
</li>
</ul>
<h3 id="S3-Intelligent-Tiering"><a href="#S3-Intelligent-Tiering" class="headerlink" title="S3 Intelligent Tiering"></a>S3 Intelligent Tiering</h3><ul>
<li>Same low latency and high throughput performance of S3 Standard</li>
<li>Small monthly monitoring and auto-tiering fee</li>
<li>Automatically moves objects between two access tiers based on
changing access patterns</li>
<li>Designed for durability of 99.999999999% of objects across multiple
Availability Zones</li>
<li>Resilient against events that impact an entire Availability Zone</li>
<li>Designed for 99.9% availability over a given year</li>
</ul>
<h3 id="Amazon-Glacier"><a href="#Amazon-Glacier" class="headerlink" title="Amazon Glacier"></a>Amazon Glacier</h3><ul>
<li>Low cost object storage meant for archiving &#x2F; backup</li>
<li>Data is retained for the longer term (10s of years)</li>
<li>Alternative to on-premise magnetic tape storage</li>
<li>Average annual durability is 99.999999999%</li>
<li>Cost per storage per month ($0.004 &#x2F; GB) + retrieval cost</li>
<li>Each item in Glacier is called “<strong>Archive”</strong> (up to 40TB)</li>
<li>Archives are stored in ”<strong>Vaults</strong>”</li>
</ul>
<h3 id="Amazon-Glacier-amp-Glacier-Deep-Archive"><a href="#Amazon-Glacier-amp-Glacier-Deep-Archive" class="headerlink" title="Amazon Glacier &amp; Glacier Deep Archive [*]"></a>Amazon Glacier &amp; Glacier Deep Archive [*]</h3><ul>
<li><p>Amazon Glacier – 3 retrieval options:</p>
<ul>
<li>Expedited (1 to 5 minutes)</li>
<li>Standard (3 to 5 hours)</li>
<li>Bulk (5 to 12 hours)</li>
<li>Minimum storage duration of 90 days</li>
</ul>
</li>
<li><p>Amazon Glacier Deep Archive – for long term storage – cheaper:</p>
<ul>
<li>Standard (12 hours)</li>
<li>Bulk (48 hours)</li>
<li>Minimum storage duration of 180 days</li>
</ul>
</li>
</ul>
<h3 id="S3-Storage-Classes-Comparison"><a href="#S3-Storage-Classes-Comparison" class="headerlink" title="S3 Storage Classes Comparison"></a>S3 Storage Classes Comparison</h3><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/s3/storage-classes/">https://aws.amazon.com/s3/storage-classes/</a></p>
<p>[SAAC-02]</p>
<h3 id="S3-–-Moving-between-storage-classes"><a href="#S3-–-Moving-between-storage-classes" class="headerlink" title="S3 – Moving between storage classes"></a>S3 – Moving between storage classes</h3><ul>
<li>You can transition objects between storage classes</li>
<li>For infrequently accessed object, move them to STANDARD_IA</li>
<li>For archive objects you don’t need in real
-time, GLACIER or DEEP_ARCHIVE</li>
<li>Moving objects can be automated using a lifecycle configuration</li>
</ul>
<h3 id="S3-Lifecycle-Rules"><a href="#S3-Lifecycle-Rules" class="headerlink" title="S3 Lifecycle Rules"></a>S3 Lifecycle Rules</h3><ul>
<li><p><strong>Transition actions</strong>: It defines when objects are transitioned to another storage class.</p>
<ul>
<li>Move objects to Standard IA class 60 days after creation</li>
<li>Move to Glacier for archiving after 6 months</li>
</ul>
</li>
<li><p>Expiration actions: configure objects to expire (delete) after some time</p>
<ul>
<li>Access log files can be set to delete after a 365 days</li>
<li><strong>Can be used to delete old versions of files (if versioning is enabled)</strong></li>
<li>Can be used to delete incomplete multi-part uploads</li>
</ul>
</li>
<li><p>Rules can be created for a certain prefix (ex - s3:&#x2F;&#x2F;mybucket&#x2F;mp3&#x2F;*)</p>
</li>
<li><p>Rules can be created for certain objects tags (ex - Department: Finance)</p>
</li>
</ul>
<h3 id="S3-Lifecycle-Rules-–-Scenario-1"><a href="#S3-Lifecycle-Rules-–-Scenario-1" class="headerlink" title="S3 Lifecycle Rules – Scenario 1"></a>S3 Lifecycle Rules – Scenario 1</h3><ul>
<li><p><strong>Your application on EC2 creates images thumbnails after profile photos are uploaded to Amazon S3. These thumbnails can be easily recreated, and only need to be kept for 45 days. The source images should be able to be immediately retrieved for these 45 days, and afterwards, the user can wait up to 6 hours. How would you design this?</strong></p>
</li>
<li><p>S3 source images can be on STANDARD, with a lifecycle configuration
to transition them to GLACIER after 45 days.</p>
</li>
<li><p>S3 thumbnails can be on ONEZONE_IA, with a lifecycle configuration
to expire them (delete them) after 45 days.</p>
</li>
</ul>
<h3 id="S3-Lifecycle-Rules-–-Scenario-2"><a href="#S3-Lifecycle-Rules-–-Scenario-2" class="headerlink" title="S3 Lifecycle Rules – Scenario 2"></a>S3 Lifecycle Rules – Scenario 2</h3><ul>
<li><p><strong>A rule in your company states that you should be able to recover your deleted S3 objects immediately for 15 days, although this may happen rarely. After this time, and for up to 365 days, deleted objects should be recoverable within 48 hours.</strong></p>
</li>
<li><p>You need to enable S3 versioning in order to have object versions, so that
“deleted objects” are in fact hidden by a “delete marker” and can be
recovered</p>
</li>
<li><p>You can transition these “noncurrent versions” of the object to S3_IA</p>
</li>
<li><p>You can transition afterwards these “noncurrent versions” to
DEEP_ARCHIVE</p>
</li>
</ul>
<p>[SAAC-02]</p>
<h3 id="S3-–-Baseline-Performance"><a href="#S3-–-Baseline-Performance" class="headerlink" title="S3 – Baseline Performance"></a>S3 – Baseline Performance</h3><ul>
<li>Amazon S3 automatically scales to high request rates, latency 100-200 ms</li>
<li>Your application can achieve at least <strong>3,500 PUT&#x2F;COPY&#x2F;POST&#x2F;DELETE and 5,500 GET&#x2F;HEAD requests per second per prefix in a bucket.</strong></li>
<li>There are no limits to the number of prefixes in a bucket.</li>
<li><strong>Example (object path &#x3D;&gt; prefix)</strong>:<ul>
<li>bucket&#x2F;folder1&#x2F;sub1&#x2F;file &#x3D;&gt; &#x2F;folder1&#x2F;sub1&#x2F;</li>
<li>bucket&#x2F;folder1&#x2F;sub2&#x2F;file &#x3D;&gt; &#x2F;folder1&#x2F;sub2&#x2F;</li>
<li>bucket&#x2F;1&#x2F;file &#x3D;&gt; &#x2F;1&#x2F;</li>
<li>bucket&#x2F;2&#x2F;file &#x3D;&gt; &#x2F;2&#x2F;</li>
</ul>
</li>
<li>If you spread reads across all four prefixes evenly, you can achieve 22,000
requests per second for GET and HEAD</li>
</ul>
<h3 id="S3-–-KMS-Limitation"><a href="#S3-–-KMS-Limitation" class="headerlink" title="S3 – KMS Limitation"></a>S3 – KMS Limitation</h3><ul>
<li>If you use SSE-KMS, you may be impacted by the KMS limits</li>
<li>When you upload, it calls the <strong>GenerateDataKey</strong> KMS API</li>
<li>When you download, it calls the <strong>Decrypt</strong> KMS API</li>
<li>Count towards the KMS quota per second (5500, 10000, 30000 req&#x2F;s based on region)</li>
<li>As of today, you cannot request a quota increase for KMS</li>
</ul>
<h3 id="S3-Performance"><a href="#S3-Performance" class="headerlink" title="S3 Performance"></a>S3 Performance</h3><ul>
<li><p><strong>Multi-Part upload</strong>:</p>
<ul>
<li>recommended for files &gt; 100MB, must use for files &gt; 5GB</li>
<li>Can help parallelize uploads (speed up transfers)</li>
</ul>
</li>
<li><p><strong>S3 Transfer Acceleration (upload only)</strong></p>
<ul>
<li>Increase transfer speed by transferring file to an AWS edge location which will
forward the data to the S3 bucket in the target region</li>
<li>Compatible with multi-part upload</li>
</ul>
</li>
</ul>
<h3 id="S3-Performance-–-S3-Byte-Range-Fetches"><a href="#S3-Performance-–-S3-Byte-Range-Fetches" class="headerlink" title="S3 Performance – S3 Byte-Range Fetches"></a>S3 Performance – S3 Byte-Range Fetches</h3><ul>
<li><p><strong>Parallelize GETs by requesting</strong> specific byte ranges</p>
</li>
<li><p>Better resilience in case of failures</p>
</li>
<li><p><strong>Can be used to speed up downloads</strong></p>
</li>
<li><p><strong>Can be used to retrieve only partial data (for example the head of a file)</strong></p>
</li>
</ul>
<h3 id="S3-Select-amp-Glacier-Select"><a href="#S3-Select-amp-Glacier-Select" class="headerlink" title="S3 Select &amp; Glacier Select"></a>S3 Select &amp; Glacier Select</h3><ul>
<li>Retrieve less data using SQL by performing <strong>server side filtering</strong></li>
<li>Can filter by rows &amp; columns (simple SQL statements)</li>
<li>Less network transfer, less CPU cost client-side</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/blogs/aws/s3-glacier-select/">https://aws.amazon.com/blogs/aws/s3-glacier-select/</a></p>
<hr>
<h3 id="S3-Event-Notifications"><a href="#S3-Event-Notifications" class="headerlink" title="S3 Event Notifications"></a>S3 Event Notifications</h3><p>Amazon S3</p>
<ul>
<li><p>S3:ObjectCreated, S3:ObjectRemoved,
S3:ObjectRestore, S3:Replication…</p>
</li>
<li><p>Object name filtering possible (*.jpg)</p>
</li>
<li><p><strong>Use case</strong>: generate thumbnails of images uploaded to S3</p>
</li>
<li><p><strong>Can create as many “S3 events” as desired</strong></p>
</li>
<li><p>S3 event notifications typically deliver events in seconds but can sometimes take a minute or longer</p>
</li>
<li><p>If two writes are made to a single non-versioned object at the same time, it is possible that only a single event notification will be sent</p>
</li>
<li><p>If you want to ensure that an event notification is sent for every successful write, you can enable versioning on your bucket.</p>
</li>
</ul>
<h3 id="AWS-Athena"><a href="#AWS-Athena" class="headerlink" title="AWS Athena"></a>AWS Athena</h3><ul>
<li><strong>Serverless</strong> service to perform analytics <strong>directly against S3 files</strong></li>
<li>Uses SQL language to query the files</li>
<li>Has a JDBC &#x2F; ODBC driver</li>
<li>Charged per query and amount of data scanned</li>
<li>Supports CSV, JSON, ORC, Avro, and Parquet (built on Presto)</li>
<li>Use cases: Business intelligence &#x2F; analytics &#x2F; reporting, analyze &amp; query
VPC Flow Logs, ELB Logs, CloudTrail trails, etc…</li>
<li>Exam Tip: Analyze data directly on S3 &#x3D;&gt; use Athena</li>
</ul>
<h3 id="S3-Object-Lock-amp-Glacier-Vault-Lock"><a href="#S3-Object-Lock-amp-Glacier-Vault-Lock" class="headerlink" title="S3 Object Lock &amp; Glacier Vault Lock"></a>S3 Object Lock &amp; Glacier Vault Lock</h3><ul>
<li><p><strong>S3 Object Lock</strong></p>
<ul>
<li>Adopt a WORM (Write Once Read Many) model</li>
<li>Block an object version deletion for a specified amount of time</li>
</ul>
</li>
<li><p><strong>Glacier Vault Lock</strong></p>
<ul>
<li>Adopt a WORM (Write Once Read Many) model</li>
<li>Lock the policy for future edits (can no longer be changed)</li>
<li>Helpful for compliance and data retention</li>
</ul>
</li>
</ul>
<hr>
<p>S3 Advanced &amp; Athena - Quiz</p>
<p>Question 1:
You have enabled versioning and want to be extra careful when it comes to deleting files on S3. What should you enable to prevent accidental permanent deletions?
A: Enable MFA Delete</p>
<p>  MFA Delete forces users to use MFA tokens before deleting objects. It’s an extra level of security to prevent accidental deletes</p>
<p>Question 2:
You would like all your files in S3 to be encrypted by default. What is the optimal way of achieving this?
A: Enable “Default Encryption” on S3</p>
<p>Question 3:
You suspect some of your employees to try to access files in S3 that they don’t have access to. How can you verify this is indeed the case without them noticing?
A: Enable S3 Access logs analyze the using Athena</p>
<p>  S3 Access Logs log all the requests made to buckets, and Athena can then be used to run serverless analytics on top of the logs files</p>
<p>Question 4:
You are looking for your entire S3 bucket to be available fully in a different region so you can perform data analysis optimally at the lowest possible cost. Which feature should you use?
A: S3  Cross Region Replication CRR</p>
<p>  S3 CRR is used to replicate data from an S3 bucket to another one in a different region</p>
<p>Question 5:
You are looking to provide temporary URLs to a growing list of federated users in order to allow them to perform a file upload on S3 to a specific location. What should you use?
A: S3 Pre-Signer URK</p>
<p>  Pre-Signed URL are temporary and grant time-limited access to some actions in your S3 bucket.</p>
<p>Question 6:
How can you automate the transition of S3 objects between their different tiers?
A: Use S3 Lifecycle Rules</p>
<p>Question 7:
Which of the following is NOT a Glacier retrieval mode?
A: Instan (10 seconds)</p>
<p>Question 8:
Which of the following is a Serverless data analysis service allowing you to query data in S3?
A: Athena</p>
<p>Question 9:
[SAA-C02] You are looking to build an index of your files in S3, using Amazon RDS PostgreSQL. To build this index, it is necessary to read the first 250 bytes of each object in S3, which contains some metadata about the content of the file itself. There is over 100,000 files in your S3 bucket, amounting to 50TB of data. how can you build this index efficiently?
A: Create an application that will traverse the S3 bucket, issue a Byte Range Fetch for the first 250 bytes, and store that information in RDS.</p>
<hr>
<h2 id="AWS-CloudFront"><a href="#AWS-CloudFront" class="headerlink" title="AWS CloudFront"></a>AWS CloudFront</h2><ul>
<li>Content Delivery Network (CDN)</li>
<li>Improves read performance, content is cached at the edge</li>
<li>216 Point of Presence globally (edge locations)</li>
<li>DDoS protection, integration with Shield, AWS Web Application Firewall</li>
<li>Can expose external HTTPS and can talk to internal HTTPS backends</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/cloudfront/features/?nc=sn&loc=2">https://aws.amazon.com/cloudfront/features/?nc=sn&amp;loc=2</a></p>
<h3 id="CloudFront-–-Origins"><a href="#CloudFront-–-Origins" class="headerlink" title="CloudFront – Origins"></a>CloudFront – Origins</h3><ul>
<li><p><em><strong>S3 bucket</strong></em></p>
<ul>
<li>For distributing files and caching them at the edge</li>
<li>Enhanced security with CloudFront <em><strong>Origin Access Identity (OAI)</strong></em></li>
<li>CloudFront can be used as an ingress (to upload files to S3)</li>
</ul>
</li>
<li><p><strong>Custom Origin (HTTP)</strong></p>
<ul>
<li>Application Load Balancer</li>
<li>EC2 instance</li>
<li>S3 website (must first enable the bucket as a static S3 website)</li>
<li>Any HTTP backend you want</li>
</ul>
</li>
</ul>
<h3 id="CloudFront-Geo-Restriction"><a href="#CloudFront-Geo-Restriction" class="headerlink" title="CloudFront Geo Restriction"></a>CloudFront Geo Restriction</h3><ul>
<li><p>You can restrict who can access your distribution</p>
<ul>
<li><strong>Whitelist</strong>: Allow your users to access your content only if they’re in one of the countries on a list of approved countries.</li>
<li><strong>Blacklist</strong>: Prevent your users from accessing your content if they’re in one of the countries on a blacklist of banned countries.</li>
</ul>
</li>
<li><p>The “country” is determined using a 3rd party Geo-IP database</p>
</li>
<li><p>Use case: Copyright Laws to control access to content</p>
</li>
</ul>
<h3 id="CloudFront-vs-S3-Cross-Region-Replication"><a href="#CloudFront-vs-S3-Cross-Region-Replication" class="headerlink" title="CloudFront vs S3 Cross Region Replication"></a>CloudFront vs S3 Cross Region Replication</h3><ul>
<li><p>CloudFront:</p>
<ul>
<li>Global Edge network</li>
<li>Files are cached for a TTL (maybe a day)</li>
<li><strong>Great for static content that must be available everywhere</strong></li>
</ul>
</li>
<li><p>S3 Cross Region Replication:</p>
<ul>
<li>Must be setup for each region you want replication to happen</li>
<li>Files are updated in near real-time</li>
<li>Read only</li>
<li><strong>Great for dynamic content that needs to be available at low-latency in few regions</strong></li>
</ul>
</li>
</ul>
<h3 id="AWS-CloudFront-Hands-On"><a href="#AWS-CloudFront-Hands-On" class="headerlink" title="AWS CloudFront Hands On"></a>AWS CloudFront Hands On</h3><ul>
<li>We’ll create an S3 bucket</li>
<li>We’ll create a CloudFront distribution</li>
<li>We’ll create an Origin Access Identity</li>
<li>We’ll limit the S3 bucket to be accessed only using this identity</li>
</ul>
<p>[SAA-C02]</p>
<h3 id="CloudFront-Signed-URL-x2F-Signed-Cookies"><a href="#CloudFront-Signed-URL-x2F-Signed-Cookies" class="headerlink" title="CloudFront Signed URL &#x2F; Signed Cookies"></a>CloudFront Signed URL &#x2F; Signed Cookies</h3><ul>
<li><p>You want to distribute paid shared content to premium users over the world</p>
</li>
<li><p>We can use CloudFront Signed URL &#x2F; Cookie. We attach a policy with:</p>
<ul>
<li>Includes URL expiration</li>
<li>Includes IP ranges to access the data from</li>
<li>Trusted signers (which AWS accounts can create signed URLs)</li>
</ul>
</li>
<li><p>How long should the URL be valid for?</p>
<ul>
<li>Shared content (movie, music): make it short (a few minutes)</li>
<li>Private content (private to the user): you can make it last for years</li>
</ul>
</li>
<li><p>Signed URL &#x3D; access to individual files (one signed URL per file)</p>
</li>
<li><p>Signed Cookies &#x3D; access to multiple files (one signed cookie for many files)</p>
</li>
</ul>
<p>[SAA-C02]</p>
<h3 id="CloudFront-Signed-URL-vs-S3-Pre-Signed-URL"><a href="#CloudFront-Signed-URL-vs-S3-Pre-Signed-URL" class="headerlink" title="CloudFront Signed URL vs S3 Pre-Signed URL"></a>CloudFront Signed URL vs S3 Pre-Signed URL</h3><ul>
<li><p>CloudFront Signed URL:</p>
<ul>
<li>Allow access to a path, no matter the origin</li>
<li>Account wide key-pair, only the root can manage it</li>
<li>Can filter by IP, path, date, expiration</li>
<li>Can leverage caching features</li>
</ul>
</li>
<li><p>S3 Pre-Signed URL:</p>
<ul>
<li>Issue a request as the person who pre-signed the URL</li>
<li>Uses the IAM key of the signing IAM principal</li>
<li>Limited lifetime</li>
</ul>
</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/global-accelerator/pricing/">https://aws.amazon.com/global-accelerator/pricing/</a></p>
<hr>
<p>CloudFront &amp; AWS Global Accelerator Quiz</p>
<p>Question 1:
Which features allows us to distribute paid content from S3 securely, globally, if the S3 bucket is secured to only exchange data with CloudFront?
A: CloudFront Signed URL</p>
<p>  CloudFront Signed URL are commonly used to distribute paid content through dynamic CloudFront Signed URL generation.</p>
<p>Question 2:
You are hosting highly dynamic content in Amazon S3 in us-east-1. Recently, there has been a need to make that data available with low latency in Singapore. What do you recommend using?
A: S3 Cross Region Replication</p>
<p>  S3 CRR allows you to replicate the data from one bucket in a region to another bucket in another region</p>
<p>Question 3:
How can you ensure that only users who access our website through Canada are authorized in CloudFront?
A: Use CloudFront Geo Restriction</p>
<p>Question 4:
You would like to provide your users access to hundreds of private files in your CloudFront distribution, which is fronting an HTTP web server behind an application load balancer. What should you use?
A: CloudFront Signed Cookies</p>
<p>  Allows you to access many files</p>
<p>Question 5:
[SAA-C02] You are creating an application that is going to expose an HTTP REST API. There is a need to provide request routing rules at the HTTP level. Due to security requirements, your application can only be exposed through the use of two static IPs. How can you create a solution that validates these requirements?
A: Use Global Accelerator and an Application Load Balancer.</p>
<p>  Global Accelerator will provide us with the two static IP, and the ALB will provide use with the HTTP routing rules</p>
<p>What does this S3 bucket policy do?</p>
<p> {
   “Version”:”2012-10-17”,
   “Id”:”Mystery policy”,
   “Statement”:[
     {
       “Sid”:”What could it be?”,
       “Effect”:”Allow”,
       “Principal”:{“CanonicalUser”:”CloudFront Origin Identity Canonical User ID”},
       “Action”:”s3:GetObject”,
       “Resource”:”arn:aws:s3:::examplebucket&#x2F;*”
     }
   ]
}</p>
<p>A: Only allows the S3 bucket content to be accessed from your CloudFront distribution origin identity</p>
<hr>
<p>Section 13: AWS Storage Extras</p>
<h3 id="AWS-Storage-Gateway-Summary"><a href="#AWS-Storage-Gateway-Summary" class="headerlink" title="AWS Storage Gateway Summary"></a>AWS Storage Gateway Summary</h3><ul>
<li><p>Exam tip: Read the question well, it will hint at which gateway to use</p>
</li>
<li><p>On premise data to the cloud &#x3D;&gt; Storage Gateway</p>
</li>
<li><p>File access &#x2F; NFS &#x3D;&gt; File Gateway
(backed by S3)</p>
</li>
<li><p>Volumes &#x2F; Block Storage &#x2F; iSCSI &#x3D;&gt; Volume gateway
(backed by S3 with EBS snapshots)</p>
</li>
<li><p>VTL Tape solution &#x2F; Backup with iSCSI &#x3D; &gt; Tape Gateway
(backed by S3 and Glacier)</p>
</li>
</ul>
<h3 id="Amazon-FSx-for-Windows-File-Server"><a href="#Amazon-FSx-for-Windows-File-Server" class="headerlink" title="Amazon FSx for Windows (File Server)"></a>Amazon FSx for Windows (File Server)</h3><ul>
<li><p>EFS is a shared POSIX system for Linux systems.</p>
</li>
<li><p><strong>FSx for Windows</strong> is a fully managed <strong>Windows</strong> file system share drive</p>
</li>
<li><p>Supports SMB protocol &amp; Windows NTFS</p>
</li>
<li><p>Microsoft Active Directory integration, ACLs, user quotas</p>
</li>
<li><p>Built on SSD, scale up to 10s of GB&#x2F;s, millions of IOPS, 100s PB of data</p>
</li>
<li><p>Can be accessed from your on-premise infrastructure</p>
</li>
<li><p>Can be configured to be Multi-AZ (high availability)</p>
</li>
<li><p>Data is backed-up daily to S3</p>
</li>
</ul>
<h3 id="Amazon-FSx-for-Lustre"><a href="#Amazon-FSx-for-Lustre" class="headerlink" title="Amazon FSx for Lustre"></a>Amazon FSx for Lustre</h3><ul>
<li><p>Lustre is a type of parallel distributed file system, for large-scale computing</p>
</li>
<li><p>The name Lustre is derived from “Linux” and “cluster”</p>
</li>
<li><p>Machine Learning, <strong>High Performance Computing (HPC)</strong></p>
</li>
<li><p>Video Processing, Financial Modeling, Electronic Design Automation</p>
</li>
<li><p>Scales up to 100s GB&#x2F;s, millions of IOPS, sub-ms latencies</p>
</li>
<li><p><strong>Seamless integration with S3</strong></p>
<ul>
<li>Can “read S3” as a file system (through FSx)</li>
<li>Can write the output of the computations back to S3 (through FSx)</li>
</ul>
</li>
<li><p>Can be used from on-premise servers</p>
</li>
</ul>
<h3 id="Storage-Comparison"><a href="#Storage-Comparison" class="headerlink" title="Storage Comparison"></a>Storage Comparison</h3><ul>
<li>S3: Object Storage</li>
<li>Glacier: Object Archival</li>
<li>EFS: Network File System for Linux instances, POSIX filesystem</li>
<li>FSx for Windows: Network File System for Windows servers</li>
<li>FSx for Lustre: High Performance Computing Linux file system</li>
<li>EBS volumes: Network storage for one EC2 instance at a time</li>
<li>Instance Storage: Physical storage for your EC2 instance (high IOPS)</li>
<li>Storage Gateway: File Gateway, Volume Gateway (cache &amp; stored), Tape Gateway</li>
<li>Snowball &#x2F; Snowmobile: to move large amount of data to the cloud, physically</li>
<li>Database: for specific workloads, usually with indexing and querying</li>
</ul>
<hr>
<h3 id="AWS-Storage-Extras-Quiz"><a href="#AWS-Storage-Extras-Quiz" class="headerlink" title="AWS Storage Extras - Quiz"></a>AWS Storage Extras - Quiz</h3><p>Question 1:
You need to move hundreds of Terabytes into the cloud in S3, and after that pre-process it using many EC2 instances in order to clean the data. You have a 1 Gbit&#x2F;s broadband and would like to optimize the process of moving the data and pre-processing it, in order to save time. What do you recommend?
A: Use Snowball Edge</p>
<p>  Snowball Edge is the right answer as it comes with computing capabilities and allows use to pre-process the data while it’s being moved in Snowball, so we save time on the pre-processing side as well.</p>
<p>Question 2:
You want to expose a virtually infinite storage for your tape backups. You want to keep the same software as today and want a iSCSI compatible interface. What do you use?
A: Tape Gateway</p>
<p>Question 3:
Your EC2 Windows Servers need to share some data by having a Network File System mounted, that respect the Windows security mechanisms and has integration with Active Directory. What do you recommend putting in place as an NFS?
A: FSx for Windows</p>
<p>Question 4:
You would like to have a distributed POSIX compliant file system that will allow you to maximize the IOPS in order to perform some HPC and genomics computational research. That file system will have to scale easily to millions of IOPS. What do you recommend?
A: FSx for Lustre</p>
<hr>
<h1 id="Section-14-Decoupling-applications-SQS-SNS-Kinesis-Active-MQ"><a href="#Section-14-Decoupling-applications-SQS-SNS-Kinesis-Active-MQ" class="headerlink" title="Section 14: Decoupling applications: SQS, SNS, Kinesis, Active MQ"></a>Section 14: Decoupling applications: SQS, SNS, Kinesis, Active MQ</h1><h2 id="Section-Introduction-3"><a href="#Section-Introduction-3" class="headerlink" title="Section Introduction"></a>Section Introduction</h2><ul>
<li><p>Synchronous between applications can be problematic if there are sudden spikes of traffic</p>
</li>
<li><p>What if you need to suddenly encode 1000 videos but usually it’s 10?</p>
</li>
<li><p>In that case, it’s better to <strong>decouple</strong> your applications,</p>
<ul>
<li>using SQS: queue model</li>
<li>using SNS: pub&#x2F;sub model</li>
<li>using Kinesis: real-time streaming model</li>
</ul>
</li>
<li><p>These services can scale independently from our application!</p>
</li>
</ul>
<h3 id="AWS-SQS-–-Standard-Queue"><a href="#AWS-SQS-–-Standard-Queue" class="headerlink" title="AWS SQS – Standard Queue"></a>AWS SQS – Standard Queue</h3><ul>
<li>Oldest offering (over 10 years old)</li>
<li>Fully managed</li>
<li>Scales from 1 message per second to 10,000s per second</li>
<li>Default retention of messages: 4 days, maximum of 14 days</li>
<li>No limit to how many messages can be in the queue</li>
<li>Low latency (&lt;10 ms on publish and receive)</li>
<li>Horizontal scaling in terms of number of consumers</li>
<li>Can have duplicate messages (at least once delivery, occasionally)</li>
<li>Can have out of order messages (best effort ordering)</li>
<li>Limitation of 256KB per message sent</li>
</ul>
<h3 id="AWS-SQS-–-Delay-Queue"><a href="#AWS-SQS-–-Delay-Queue" class="headerlink" title="AWS SQS – Delay Queue"></a>AWS SQS – Delay Queue</h3><ul>
<li>Delay a message (consumers don’t see it immediately) up to <strong>15 minutes</strong></li>
<li>Default is 0 seconds (message is available right away)</li>
<li>Can set a default at queue level</li>
<li>Can override the default using the <strong>DelaySeconds</strong> parameter</li>
</ul>
<h3 id="SQS-–-Consuming-Messages"><a href="#SQS-–-Consuming-Messages" class="headerlink" title="SQS – Consuming Messages"></a>SQS – Consuming Messages</h3><ul>
<li>Consumers…</li>
<li>Poll SQS for messages (receive up to 10 messages at a time)</li>
<li>Process the message within the visibility timeout</li>
<li>Delete the message using the message ID &amp; receipt handle</li>
</ul>
<h3 id="SQS-–Visibility-timeout"><a href="#SQS-–Visibility-timeout" class="headerlink" title="SQS –Visibility timeout"></a>SQS –Visibility timeout</h3><ul>
<li>When a consumer polls a message from a queue, the message is “invisible” to other
consumers for a defined period… the <strong>Visibility Timeout</strong>:<ul>
<li>Set between 0 seconds and 12 hours (default 30 seconds)</li>
<li>If too high (15 minutes) and consumer fails to process the message,  you must wait a long time before processing the message again</li>
<li>If too low (30 seconds) and consumer needs time to process the message (2 minutes), another
consumer will receive the message and the message will be processed more than once</li>
</ul>
</li>
<li><strong>ChangeMessageVisibility</strong> API to change the visibility while processing a message</li>
<li><strong>DeleteMessage</strong> API to tell SQS the message was successfully processed</li>
</ul>
<h3 id="AWS-SQS-–-Dead-Letter-Queue"><a href="#AWS-SQS-–-Dead-Letter-Queue" class="headerlink" title="AWS SQS – Dead Letter Queue"></a>AWS SQS – Dead Letter Queue</h3><ul>
<li>If a consumer fails to process a message within the Visibility Timeout…
the message goes back to the queue!</li>
<li>We can set a threshold of how many times a message can go back to the queue – it’s
called a “redrive policy”</li>
<li>After the threshold is exceeded, the message goes into a dead letter queue (DLQ)</li>
<li>We have to create a DLQ first and then designate it dead letter queue</li>
<li>Make sure to process the messages in the DLQ before they expire!</li>
</ul>
<h3 id="AWS-SQS-Long-Polling"><a href="#AWS-SQS-Long-Polling" class="headerlink" title="AWS SQS - Long Polling"></a>AWS SQS - Long Polling</h3><ul>
<li>When a consumer requests message from the queue, it can optionally “wait” for messages to arrive if there are none in the queue</li>
<li>This is called Long Polling</li>
<li><strong>LongPolling decreases the number of API calls made to SQS while increasing the efficiency and latency of your application.</strong></li>
<li>The wait time can be between 1 sec to 20 sec (20 sec preferable)</li>
<li>Long Polling is preferable to Short Polling</li>
<li>Long polling can be enabled at the queue level or at the API level using <strong>WaitTimeSeconds</strong></li>
</ul>
<h3 id="AWS-SQS-–-FIFO-Queue"><a href="#AWS-SQS-–-FIFO-Queue" class="headerlink" title="AWS SQS – FIFO Queue"></a>AWS SQS – FIFO Queue</h3><ul>
<li>Newer offering (First In - First out) – not available in all regions!</li>
<li>Name of the queue must end in .fifo - Lower throughput (up to 3,000 per second  with batching, 300&#x2F;s without)</li>
<li>Messages are processed in order by the consumer</li>
<li>Messages are sent exactly once</li>
<li>No per message delay (only per queue delay)</li>
<li>Ability to do content based  de-duplication</li>
<li>5-minute interval de-duplication using “Duplication ID”</li>
<li>Message Groups:<ul>
<li>Possibility to group messages for FIFO ordering using “Message GroupID”</li>
<li>Only one worker can be assigned per message group so that messages are processed in order</li>
<li>Message group is just an extra tag on the message!</li>
</ul>
</li>
</ul>
<h3 id="AWS-SNS"><a href="#AWS-SNS" class="headerlink" title="AWS SNS"></a>AWS SNS</h3><ul>
<li>The “event producer” only sends message to one SNS topic</li>
<li>As many “event receivers” (subscriptions) as we want to listen to the SNS topic notifications</li>
<li>Each subscriber to the topic will get all the messages (note: new feature to filter messages)</li>
<li>Up to 10,000,000 subscriptions per topic</li>
<li>100,000 topics limit</li>
<li>Subscribers can be:<ul>
<li>SQS</li>
<li>HTTP &#x2F; HTTPS (with delivery retries – how many times)</li>
<li>Lambda</li>
<li>Emails</li>
<li>SMS messages</li>
<li>Mobile Notifications</li>
</ul>
</li>
</ul>
<h3 id="SNS-integrates-with-a-lot-of-Amazon-Products"><a href="#SNS-integrates-with-a-lot-of-Amazon-Products" class="headerlink" title="SNS integrates with a lot of Amazon Products"></a>SNS integrates with a lot of Amazon Products</h3><ul>
<li>Some services can send data directly to SNS for notifications</li>
<li>CloudWatch (for alarms)</li>
<li>Auto Scaling Groups notifications</li>
<li>Amazon S3 (on bucket events)</li>
<li>CloudFormation (upon state changes &#x3D;&gt; failed to build, etc)</li>
<li>Etc…</li>
</ul>
<h3 id="AWS-SNS-–-How-to-publish"><a href="#AWS-SNS-–-How-to-publish" class="headerlink" title="AWS SNS – How to publish"></a>AWS SNS – How to publish</h3><ul>
<li><p>Topic Publish (within your AWS Server – using the SDK)</p>
<ul>
<li>Create a topic</li>
<li>Create a subscription (or many)</li>
<li>Publish to the topic</li>
</ul>
</li>
<li><p>Direct Publish (for mobile apps SDK)</p>
<ul>
<li>Create a platform application</li>
<li>Create a platform endpoint</li>
<li>Publish to the platform endpoint</li>
<li>Works with Google GCM, Apple APNS, Amazon ADM…</li>
</ul>
</li>
</ul>
<h3 id="SNS-SQS-Fan-Out-Pattern"><a href="#SNS-SQS-Fan-Out-Pattern" class="headerlink" title="SNS + SQS: Fan Out Pattern"></a>SNS + SQS: Fan Out Pattern</h3><ul>
<li>Push once in SNS, receive in many SQS</li>
<li>Fully decoupled</li>
<li>No data loss</li>
<li>Ability to add receivers of data later</li>
<li>SQS allows for delayed processing</li>
<li>SQS allows for retries of work</li>
<li>May have many workers on one queue and one worker on the other queue</li>
</ul>
<h3 id="AWS-Kinesis-Overview"><a href="#AWS-Kinesis-Overview" class="headerlink" title="AWS Kinesis Overview"></a>AWS Kinesis Overview</h3><ul>
<li><p>Kinesis is a managed alternative to Apache Kafka.</p>
</li>
<li><p>Great for application logs, metrics, IoT, clickstreams</p>
</li>
<li><p><strong>Great for “real-time” big data</strong></p>
</li>
<li><p>Great for streaming processing frameworks (Spark, NiFi, etc…)</p>
</li>
<li><p>Data is automatically replicated to 3 AZ</p>
</li>
<li><p><strong>Kinesis Streams</strong>: low latency streaming ingest at scale</p>
</li>
<li><p><strong>Kinesis Analytics</strong>: perform real-time analytics on streams using SQL</p>
</li>
<li><p><strong>Kinesis Firehose</strong>: load streams into S3, Redshift, ElasticSearch…</p>
</li>
</ul>
<h3 id="Kinesis-Streams-Shards"><a href="#Kinesis-Streams-Shards" class="headerlink" title="Kinesis Streams Shards"></a>Kinesis Streams Shards</h3><ul>
<li>One stream is made of many different shards</li>
<li>1MB&#x2F;s or 1000 messages&#x2F;s at write PER SHARD</li>
<li>2MB&#x2F;s at read PER SHARD</li>
<li>Billing is per shard provisioned, can have as many shards as you want</li>
<li>Batching available or per message calls.</li>
<li>The number of shards can evolve over time (reshard &#x2F; merge)</li>
<li><strong>Records are ordered per shard</strong></li>
</ul>
<h3 id="SQS-vs-SNS-vs-Kinesis"><a href="#SQS-vs-SNS-vs-Kinesis" class="headerlink" title="SQS vs SNS vs Kinesis"></a>SQS vs SNS vs Kinesis</h3><p><strong>SQS</strong>:</p>
<ul>
<li>Consumer “pull data”</li>
<li>Data is deleted after being consumed</li>
<li>Can have as many workers (consumers) as we want</li>
<li>No need to provision throughput</li>
<li>No ordering guarantee (except FIFO queues)</li>
<li>Individual message delay capability
<strong>SNS</strong>:</li>
<li>Push data to many subscribers</li>
<li>Up to 10,000,000 subscribers</li>
<li>Data is not persisted (lost if not delivered)</li>
<li>Pub&#x2F;Sub</li>
<li>Up to 100,000 topics</li>
<li>No need to provision throughput</li>
<li>Integrates with SQS for fan-out architecture pattern
<strong>Kinesis</strong>:</li>
<li>Consumers “pull data”</li>
<li>As many consumers as we want</li>
<li>Possibility to replay data</li>
<li>Meant for real-time big data, analytics and ETL</li>
<li>Ordering at the shard level</li>
<li>Data expires after X days</li>
<li>Must provision throughput</li>
</ul>
<h2 id="Amazon-MQ"><a href="#Amazon-MQ" class="headerlink" title="Amazon MQ"></a>Amazon MQ</h2><ul>
<li><p>SQS, SNS are “cloud-native” services, and they’re using proprietary protocols from AWS.</p>
</li>
<li><p>Traditional applications running from on-premise may use open protocols such as: MQTT, AMQP, STOMP, Openwire, WSS</p>
</li>
<li><p>When migrating to the cloud, instead of re-engineering the application to use SQS and SNS, we can use Amazon MQ</p>
</li>
<li><p><strong>Amazon MQ &#x3D; managed Apache ActiveMQ</strong></p>
</li>
<li><p>Amazon MQ doesn’t “scale” as much as SQS &#x2F; SNS</p>
</li>
<li><p>Amazon MQ runs on a dedicated machine, can run in HA with failover</p>
</li>
<li><p>Amazon MQ has both queue feature (<del>SQS) and topic features (</del>SNS)</p>
</li>
</ul>
<hr>
<p>Messaging and Integration Quiz
Quiz 13|12 questions</p>
<p>Question 1:
You are preparing for the biggest day of sale of the year, where your traffic will increase by 100x. You have already setup SQS standard queue. What should you do?
A: Do nothing, SQS scales automatically</p>
<p>Question 2:
You would like messages to be processed by SQS consumers only after 5 minutes of being published to SQS. What should you do?</p>
<p>Question 4:
Your SQS costs are extremely high. Upon closer look, you notice that your consumers are polling SQS too often and getting empty data as a result. What should you do?
A: Enable Long Polling</p>
<p>  Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren’t included in a response)</p>
<p>Question 5:
You’d like your messages to be processed exactly once and in order. Which do you need?
A: SQS FIFO Queue</p>
<p>  FIFO (First-In-First-Out) queues are designed to enhance messaging between applications when the order of operations and events is critical, or where duplicates can’t be tolerated. FIFO queues also provide exactly-once processing but have a limited number of transactions per second (TPS).</p>
<p>Question 6:
You’d like to send a message to 3 different applications all using SQS. You should
A: Use SNS + SQS Fan Out pattern</p>
<p>  This is a common pattern as only one message is sent to SNS and then “fan out” to multiple SQS queuesee</p>
<p>Question 7:
You have a Kinesis stream usually receiving 5MB&#x2F;s of data and sending out 8 MB&#x2F;s of data. You have provisioned 6 shards. Some days, your traffic spikes up to 2 times and you get a throughput exception. You should
A: Add more shards</p>
<p>  Each shard allows for 1MB&#x2F;s incoming and 2MB&#x2F;s outgoing of data</p>
<p>Question 8:
You are sending a clickstream for your users navigating your website, all the way to Kinesis. It seems that the users data is not ordered in Kinesis, and the data for one individual user is spread across many shards. How to fix that problem?
A: You should use a partition key that represents the identity of the user</p>
<p>  By providing a partition key we ensure the data is ordered for our users</p>
<p>Question 9:
We’d like to perform real time analytics on streams of data. The most appropriate product will be
A: Kinesis</p>
<p>  Kinesis Analytics is the product to use, with Kinesis Streams as the underlying source of data</p>
<p>Question 10:
We’d like for our big data to be loaded near real time to S3 or Redshift. We’d like to convert the data along the way. What should we use?
A: Kinesis Streams + Kinesis Firehose</p>
<p>  This is a perfect combo of technology for loading data near real-time in S3 and Redshift</p>
<p>Question 11:
You want to send email notifications to your users. You should use
A: SNS</p>
<p>  Has that feature by default</p>
<p>Question 12:
You have many microservices running on-premise and they currently communicate using a message broker that supports the MQTT protocol. You would like to migrate these applications and the message broker to the cloud without changing the application logic. Which technology allows you to get a managed message broker that supports the MQTT protocol?
A: Amazon MQ</p>
<p>  Supports JMS, NMS, AMQP, STOMP, MQTT, and WebSocket</p>
<hr>
<h2 id="Section-15-Serverless-Overviews-from-a-Solution-Architect-Perspective"><a href="#Section-15-Serverless-Overviews-from-a-Solution-Architect-Perspective" class="headerlink" title="Section 15: Serverless Overviews from a Solution Architect Perspective"></a>Section 15: Serverless Overviews from a Solution Architect Perspective</h2><p>What’s serverless?</p>
<ul>
<li>Serverless is a new paradigm in which the developers don’t have to manage servers anymore…</li>
<li>They just deploy code</li>
<li>They just deploy… functions !</li>
<li>Initially… <strong>Serverless &#x3D;&#x3D; FaaS (Function as a Service)</strong></li>
<li>Serverless was pioneered by AWS Lambda but now also includes
anything that’s managed: “databases, messaging, storage, etc.”</li>
<li><strong>Serverless does not mean there are no servers…</strong>
it means you just don’t manage &#x2F; provision &#x2F; see them</li>
</ul>
<h3 id="Serverless-in-AWS"><a href="#Serverless-in-AWS" class="headerlink" title="Serverless in AWS"></a>Serverless in AWS</h3><ul>
<li>AWS Lambda</li>
<li>DynamoDB</li>
<li>AWS Cognito</li>
<li>AWS API Gateway</li>
<li>Amazon S3</li>
<li>AWS SNS &amp; SQS</li>
<li>AWS Kinesis Data Firehose</li>
<li>Aurora Serverless</li>
<li>Step Functions</li>
<li>Fargate</li>
</ul>
<h3 id="Why-AWS-Lambda"><a href="#Why-AWS-Lambda" class="headerlink" title="Why AWS Lambda"></a>Why AWS Lambda</h3><p><strong>Amazon EC2</strong></p>
<ul>
<li>Virtual Servers in the Cloud</li>
<li>Limited by RAM and CPU</li>
<li>Continuously running</li>
<li>Scaling means intervention to add &#x2F; remove servers
<strong>Amazon Lambda</strong></li>
<li>Virtual <strong>functions</strong> – no servers to manage!</li>
<li>Limited by time - <strong>short executions</strong></li>
<li>Run <strong>on-demand</strong></li>
<li><strong>Scaling is automated!</strong></li>
</ul>
<h3 id="Benefits-of-AWS-Lambda"><a href="#Benefits-of-AWS-Lambda" class="headerlink" title="Benefits of AWS Lambda"></a>Benefits of AWS Lambda</h3><ul>
<li><p>Easy Pricing:</p>
<ul>
<li>Pay per request and compute time</li>
<li>Free tier of 1,000,000 AWS Lambda requests and 400,000 GBs of compute time</li>
</ul>
</li>
<li><p>Integrated with the whole AWS suite of services</p>
</li>
<li><p>Integrated with many programming languages</p>
</li>
<li><p>Easy monitoring through AWS CloudWatch</p>
</li>
<li><p>Easy to get more resources per functions (up to 3GB of RAM!)</p>
</li>
<li><p><strong>Increasing RAM will also improve CPU and network!</strong></p>
</li>
</ul>
<h3 id="AWS-Lambda-language-support"><a href="#AWS-Lambda-language-support" class="headerlink" title="AWS Lambda language support"></a>AWS Lambda language support</h3><ul>
<li>Node.js (JavaScript)</li>
<li>Python</li>
<li>Java (Java 8 compatible)</li>
<li>C# (.NET Core)</li>
<li>Golang</li>
<li>C# &#x2F; Powershell</li>
<li>Ruby</li>
<li>Custom Runtime API (community supported, example Rust)</li>
<li>Important: Docker is not for AWS Lambda, it’s for ECS &#x2F; Fargate</li>
</ul>
<p>AWS Lambda Pricing: example</p>
<ul>
<li>You can find overall pricing information here:
<a target="_blank" rel="noopener" href="https://aws.amazon.com/lambda/pricing/">https://aws.amazon.com/lambda/pricing/</a></li>
<li>Pay per <strong>calls</strong>:<ul>
<li>First 1,000,000 requests are free</li>
<li>$0.20 per 1 million requests thereafter ($0.0000002 per request)</li>
</ul>
</li>
<li>Pay per <strong>duration</strong>: (in increment of 100ms)<ul>
<li>400,000 GB-seconds of compute time per month if FREE</li>
<li>&#x3D;&#x3D; 400,000 seconds if function is 1GB RAM</li>
<li>&#x3D;&#x3D; 3,200,000 seconds if function is 128 MB RAM</li>
<li>After that $1.00 for 600,000 GB-seconds</li>
</ul>
</li>
<li>It is usually <strong>very cheap</strong> to run AWS Lambda so it’s <strong>very popular</strong>.</li>
</ul>
<h3 id="AWS-Lambda-Limits-to-Know-per-region"><a href="#AWS-Lambda-Limits-to-Know-per-region" class="headerlink" title="AWS Lambda Limits to Know - per region"></a>AWS Lambda Limits to Know - per region</h3><ul>
<li><strong>Execution:</strong><ul>
<li>Memory allocation: 128 MB – 3008 MB (64 MB increments)</li>
<li>Maximum execution time: 900 seconds (15 minutes)</li>
<li>Environment variables (4 KB)</li>
<li>Disk capacity in the “function container” (in &#x2F;tmp): 512 MB</li>
<li>Concurrency executions: 1000 (can be increased)</li>
</ul>
</li>
<li><strong>Deployment</strong>:<ul>
<li>Lambda function deployment size (compressed .zip): 50 MB</li>
<li>Size of uncompressed deployment (code + dependencies): 250 MB</li>
<li>Can use the &#x2F;tmp directory to load other files at startup</li>
<li>Size of environment variables: 4 KB</li>
</ul>
</li>
</ul>
<h3 id="Lambda-Edge"><a href="#Lambda-Edge" class="headerlink" title="Lambda@Edge"></a>Lambda@Edge</h3><ul>
<li><p>You have deployed a CDN using CloudFront</p>
</li>
<li><p>What if you wanted to run a global AWS Lambda alongside?</p>
</li>
<li><p>Or how to implement request filtering before reaching your application?</p>
</li>
<li><p>For this, you can use Lambda@Edge:
deploy Lambda functions alongside your CloudFront CDN</p>
<ul>
<li>Build more responsive applications</li>
<li>You don’t manage servers, Lambda is deployed globally</li>
<li>Customize the CDN content</li>
<li>Pay only for what you use</li>
</ul>
</li>
</ul>
<h3 id="Lambda-Edge-1"><a href="#Lambda-Edge-1" class="headerlink" title="Lambda@Edge"></a>Lambda@Edge</h3><ul>
<li><p>You can use Lambda to change CloudFront requests and responses:</p>
<ul>
<li>After CloudFront receives a request from a viewer (viewer request)</li>
<li>Before CloudFront forwards the request to the origin (origin request)</li>
<li>After CloudFront receives the response from the origin (origin response)</li>
<li>Before CloudFront forwards the response to the viewer (viewer response)</li>
</ul>
</li>
<li><p>You can also generate responses to viewers without ever sending the request to the origin.</p>
</li>
</ul>
<h3 id="Lambda-Edge-Use-Cases"><a href="#Lambda-Edge-Use-Cases" class="headerlink" title="Lambda@Edge: Use Cases"></a>Lambda@Edge: Use Cases</h3><ul>
<li>Website Security and Privacy</li>
<li>Dynamic Web Application at the Edge</li>
<li>Search Engine Optimization (SEO)</li>
<li>Intelligently Route Across Origins and Data Centers</li>
<li>Bot Mitigation at the Edge</li>
<li>Real-time Image Transformation</li>
<li>A&#x2F;B Testing</li>
<li>User Authentication and Authorization</li>
<li>User Prioritization</li>
<li>User Tracking and Analytics</li>
</ul>
<h3 id="AWS-API-Gateway"><a href="#AWS-API-Gateway" class="headerlink" title="AWS API Gateway"></a>AWS API Gateway</h3><ul>
<li>AWS Lambda + API Gateway: No infrastructure to manage</li>
<li>Support for the WebSocket Protocol</li>
<li>Handle API versioning (v1, v2…)</li>
<li>Handle different environments (dev, test, prod…)</li>
<li>Handle security (Authentication and Authorization)</li>
<li>Create API keys, handle request throttling</li>
<li>Swagger &#x2F; Open API import to quickly define APIs</li>
<li>Transform and validate requests and responses</li>
<li>Generate SDK and API specifications</li>
<li>Cache API responses</li>
</ul>
<h3 id="API-Gateway-–-Integrations-High-Level"><a href="#API-Gateway-–-Integrations-High-Level" class="headerlink" title="API Gateway – Integrations High Level"></a>API Gateway – Integrations High Level</h3><ul>
<li><strong>Lambda Function</strong><ul>
<li>Invoke Lambda function</li>
<li>Easy way to expose REST API backed by AWS Lambda</li>
</ul>
</li>
<li><strong>HTTP</strong><ul>
<li>Expose HTTP endpoints in the backend</li>
<li>Example: internal HTTP API on premise, Application Load Balancer…</li>
<li>Why? Add rate limiting, caching, user authentications, API keys, etc…</li>
</ul>
</li>
<li><strong>AWS Service</strong><ul>
<li>Expose any AWS API through the API Gateway?</li>
<li>Example: start an AWS Step Function workflow, post a message to SQS</li>
<li>Why? Add authentication, deploy publicly, rate control…</li>
</ul>
</li>
</ul>
<h3 id="API-Gateway-Endpoint-Types"><a href="#API-Gateway-Endpoint-Types" class="headerlink" title="API Gateway - Endpoint Types"></a>API Gateway - Endpoint Types</h3><ul>
<li>Edge-Optimized (default): For global clients<ul>
<li>Requests are routed through the CloudFront Edge locations (improves latency)</li>
<li>The API Gateway still lives in only one region</li>
</ul>
</li>
<li>Regional:<ul>
<li>For clients within the same region</li>
<li>Could manually combine with CloudFront (more control over the caching
strategies and the distribution)</li>
</ul>
</li>
<li>Private:<ul>
<li>Can only be accessed from your VPC using an interface VPC endpoint (ENI)</li>
<li>Use a resource policy to define access</li>
</ul>
</li>
</ul>
<h2 id="API-Gateway-–-Security"><a href="#API-Gateway-–-Security" class="headerlink" title="API Gateway – Security"></a>API Gateway – Security</h2><p>IAM Permissions</p>
<ul>
<li>Create an IAM policy authorization and attach to User &#x2F; Role</li>
<li>API Gateway verifies IAM permissions passed by the calling application</li>
<li>Good to provide access within your own infrastructure</li>
<li>Leverages “<strong>Sig v4</strong>” capability where IAM credential are in headers</li>
</ul>
<h3 id="API-Gateway-–-Security-1"><a href="#API-Gateway-–-Security-1" class="headerlink" title="API Gateway – Security"></a>API Gateway – Security</h3><p><strong>Lambda Authorizer (formerly Custom Authorizers)</strong></p>
<ul>
<li>Uses AWS Lambda to validate the token in header being passed</li>
<li>Option to cache result of authentication</li>
<li>Helps to use OAuth &#x2F; SAML &#x2F; 3rd party type of authentication</li>
<li>Lambda must return an IAM policy for the user</li>
</ul>
<h3 id="API-Gateway-–-Security-2"><a href="#API-Gateway-–-Security-2" class="headerlink" title="API Gateway – Security"></a>API Gateway – Security</h3><p>Cognito User Pools</p>
<ul>
<li>Cognito fully manages user lifecycle</li>
<li>API gateway verifies identity automatically from AWS Cognito</li>
<li>No custom implementation required</li>
<li><strong>Cognito only helps with authentication, not authorization</strong></li>
</ul>
<h2 id="API-Gateway-–-Security-–-Summary"><a href="#API-Gateway-–-Security-–-Summary" class="headerlink" title="API Gateway – Security – Summary"></a>API Gateway – Security – Summary</h2><ul>
<li><strong>IAM:</strong><ul>
<li>Great for users &#x2F; roles already within your AWS account</li>
<li>Handle authentication + authorization</li>
<li>Leverages Sig v4</li>
</ul>
</li>
<li><strong>Custom Authorizer:</strong><ul>
<li>Great for 3rd party tokens</li>
<li>Very flexible in terms of what IAM policy is returned</li>
<li>Handle Authentication + Authorization</li>
<li>Pay per Lambda invocation</li>
</ul>
</li>
<li><strong>Cognito User Pool:</strong><ul>
<li>You manage your own user pool (can be backed by Facebook, Google login etc…)</li>
<li>No need to write any custom code</li>
<li>Must implement authorization in the backend</li>
</ul>
</li>
</ul>
<h2 id="AWS-Cognito-–-Federated-Identity-Pools"><a href="#AWS-Cognito-–-Federated-Identity-Pools" class="headerlink" title="AWS Cognito – Federated Identity Pools"></a>AWS Cognito – Federated Identity Pools</h2><ul>
<li>Goal:<ul>
<li>Provide direct access to AWS Resources from the Client Side</li>
</ul>
</li>
<li>How:<ul>
<li>Log in to federated identity provider – or remain anonymous</li>
<li>Get temporary AWS credentials back from the Federated Identity Pool</li>
<li>These credentials come with a pre-defined IAM policy stating their permissions</li>
</ul>
</li>
<li>Example:<ul>
<li>provide (temporary) access to write to S3 bucket using Facebook Login</li>
</ul>
</li>
</ul>
<h3 id="AWS-SAM-Serverless-Application-Model"><a href="#AWS-SAM-Serverless-Application-Model" class="headerlink" title="AWS SAM - Serverless Application Model"></a>AWS SAM - Serverless Application Model</h3><ul>
<li><strong>SAM &#x3D; Serverless Application Model</strong></li>
<li>Framework for developing and deploying serverless applications</li>
<li>All the configuration is YAML code<ul>
<li>Lambda Functions</li>
<li>DynamoDB tables</li>
<li>API Gateway</li>
<li>Cognito User Pools</li>
</ul>
</li>
<li>SAM can help you to run Lambda, API Gateway, DynamoDB locally</li>
<li>SAM can use <strong>CodeDeploy</strong> to deploy Lambda functions</li>
</ul>
<hr>
<p>Serverless Quiz</p>
<p>Question 1:
You have a Lambda function that will process data for 25 minutes before successfully completing. The code is working fine in your machine, but in AWS Lambda it just fails with a “timeout” issue after 3 seconds. What should you do?
A: Run your code somewhere else than Lambda - the maximum timeout is 15 minutes</p>
<p>Question 2:
You’d like to have a dynamic DB_URL variable loaded in your Lambda code
A: Place it in the environment variables</p>
<p>Question 3:
We have to provision the instance type for our DynamoDB database
A: False</p>
<p>  DynamoDB is a serverless service and as such we don’t provision an instance type for our database. We just say how much RCU and WCU we require for our table (or auto scaling)</p>
<p>Question 4:
A DynamoDB table has been provisioned with 10 RCU and 10 WCU. You would like to increase the RCU to sustain more read traffic. What is true about RCU and WCU?
A: RCU and WCU are decoupled, so WCU can stay the same</p>
<p>Question 5:
You are about to enter the Christmas sale and you know a few items in your website are very popular and will be read often. Last year you had a ProvisionedThroughputExceededException. What should you do this year?
A: Create a DAX cluter</p>
<p>  A DynamoDB Accelerator (DAX) cluster is a cache that fronts your DynamoDB tables and caches the most frequently read values. They help offload the heavy reads on hot keys off of DynamoDB itself, hence preventing the ProvisionedThroughputExceededException</p>
<p>Question 6:
You would like to automate sending welcome emails to the users who subscribe to the Users table in DynamoDB. How can you achieve that?
A: Enable DynamoDB Streams and have the Lambda function receive the events in real-time</p>
<p>Question 7:
To make a serverless API, I should integrate API Gateway with
A: Lambda</p>
<p>Question 8:
You would like to provide a Facebook login before your users call your API hosted by API Gateway. You need seamlessly authentication integration, you will use
a: Conigto User Pools</p>
<p>  Cognito User Pools directly integration with Facebook Logins</p>
<p>Question 9:
[SAA-C02] Your production application is leveraging DynamoDB as its backend and is experiencing smooth sustained usage. There is a need to make the application run in development as well, where it will experience unpredictable, sometimes high, sometimes low volume of requests. You would like to make sure you optimize for cost. What do you recommend?
A: OJO X</p>
<p>  Provision WCU &amp;&amp;nbsp;RCU&amp;nbsp;and enable auto-scaling for production and use on-demand capacity for development</p>
<hr>
<h2 id="Serverless-Architectures"><a href="#Serverless-Architectures" class="headerlink" title="Serverless Architectures"></a>Serverless Architectures</h2><h3 id="Mobile-application-MyTodoList"><a href="#Mobile-application-MyTodoList" class="headerlink" title="Mobile application: MyTodoList"></a>Mobile application: MyTodoList</h3><ul>
<li><p>We want to create a mobile application with the following requirements</p>
</li>
<li><p>Expose as REST API with HTTPS</p>
</li>
<li><p>Serverless architecture</p>
</li>
<li><p>Users should be able to directly interact with their own folder in S3</p>
</li>
<li><p>Users should authenticate through a managed serverless service</p>
</li>
<li><p>The users can write and read to-dos, but they mostly read them</p>
</li>
<li><p>The database should scale, and have some high read throughput</p>
</li>
</ul>
<p>[*] Store files on S3 from mobile, use Amazon Cognito to generate temp credentials via AWS STS.</p>
<p>[*] Improve hight read throughput, static data. Use DAX caching layer &#x2F; CACHING OR RESPONSES of API Gateway</p>
<h3 id="In-this-lecture"><a href="#In-this-lecture" class="headerlink" title="In this lecture"></a>In this lecture</h3><ul>
<li>Serverless REST API: HTTPS, API Gateway, Lambda, DynamoDB</li>
<li>Using <strong>Cognito to generate temporary credentials with STS</strong> to access S3 bucket with restricted policy. App users can directly access AWS resources this way. Pattern can be applied to DynamoDB, Lambda…</li>
<li><strong>Caching the reads on DynamoDB using DAX</strong></li>
<li><strong>Caching the REST requests at the API Gateway level</strong></li>
<li>Security for authentication and authorization with Cognito, STS</li>
</ul>
<h3 id="Serverless-hosted-website-MyBlog-com"><a href="#Serverless-hosted-website-MyBlog-com" class="headerlink" title="Serverless hosted website: MyBlog.com"></a>Serverless hosted website: MyBlog.com</h3><ul>
<li>This website should scale globally</li>
<li>Blogs are rarely written, but often read</li>
<li>Some of the website is purely static files, the rest is a dynamic REST API</li>
<li>Caching must be implement where possible</li>
<li>Any new users that subscribes should receive a welcome email</li>
<li>Any photo uploaded to the blog should have a thumbnail generated</li>
</ul>
<p>[*] Provide securit with CloudFront: OAI, origin access identity, + Bucket Policy</p>
<p>client &lt;—&gt; CloudFront &lt;———————–&gt; S3 (Policy only authorize from OAI)
                          Origin Access Identity</p>
<h3 id="AWS-Hosted-Website-Summary"><a href="#AWS-Hosted-Website-Summary" class="headerlink" title="AWS Hosted Website Summary"></a>AWS Hosted Website Summary</h3><ul>
<li>We’ve seen static content being distributed using CloudFront with S3</li>
<li>The REST API was serverless, didn’t need Cognito because public</li>
<li>We leveraged a Global DynamoDB table to serve the data globally</li>
<li>(we could have used Aurora Global Tables)</li>
<li>We enabled DynamoDB streams to trigger a Lambda function</li>
<li>The lambda function had an IAM role which could use SES</li>
<li>SES (Simple Email Service) was used to send emails in a serverless way</li>
<li>S3 can trigger SQS &#x2F; SNS &#x2F; Lambda to notify of events</li>
</ul>
<h3 id="Micro-Services-architecture"><a href="#Micro-Services-architecture" class="headerlink" title="Micro Services architecture"></a>Micro Services architecture</h3><ul>
<li><p>We want to switch to a micro service architecture</p>
</li>
<li><p>Many services interact with each other directly using a REST API</p>
</li>
<li><p>Each architecture for each micro service may vary in form and shape</p>
</li>
<li><p>We want a micro-service architecture so we can have a leaner
development lifecycle for each service</p>
</li>
</ul>
<h3 id="Discussions-on-Micro-Services"><a href="#Discussions-on-Micro-Services" class="headerlink" title="Discussions on Micro Services"></a>Discussions on Micro Services</h3><ul>
<li><strong>You are free to design each micro-service the way you want</strong></li>
<li>Synchronous patterns: API Gateway, Load Balancers</li>
<li>Asynchronous patterns: SQS, Kinesis, SNS, Lambda triggers (S3)</li>
<li>Challenges with micro-services:<ul>
<li>repeated overhead for creating each new microservice,</li>
<li>issues with optimizing server density&#x2F;utilization</li>
<li>complexity of running multiple versions of multiple microservices simultaneously</li>
<li>proliferation of client-side code requirements to integrate with many separate services.</li>
</ul>
</li>
<li>Some of the challenges are solved by Serverless patterns:<ul>
<li>API Gateway, Lambda scale automatically and you pay per usage</li>
<li>You can easily clone API, reproduce environments</li>
<li>Generated client SDK through Swagger integration for the API Gateway</li>
</ul>
</li>
</ul>
<h3 id="Distributing-paid-content"><a href="#Distributing-paid-content" class="headerlink" title="Distributing paid content"></a>Distributing paid content</h3><ul>
<li>We sell videos online and users have to paid to buy videos</li>
<li>Each videos can be bought by many different customers</li>
<li>We only want to distribute videos to users who are premium users</li>
<li>We have a database of premium users</li>
<li>Links we send to premium users should be short lived</li>
<li>Our application is global</li>
<li>We want to be fully serverles</li>
</ul>
<h3 id="Premium-User-Video-service"><a href="#Premium-User-Video-service" class="headerlink" title="Premium User Video service"></a>Premium User Video service</h3><ul>
<li>We have implemented a fully serverless solution:<ul>
<li>Cognito for authentication</li>
<li>DynamoDB for storing users that are premium</li>
<li>2 serverless applications<ul>
<li>Premium User registration</li>
<li>CloudFront Signed URL generator</li>
</ul>
</li>
<li>Content is stored in S3 (serverless and scalable)</li>
<li>Integrated with CloudFront with OAI for security (users can’t bypass)</li>
<li>CloudFront can only be used using Signed URLs to prevent unauthorized users</li>
<li>What about S3 Signed URL? They’re not efficient for global access</li>
</ul>
</li>
</ul>
<h3 id="Software-updates-offloading"><a href="#Software-updates-offloading" class="headerlink" title="Software updates offloading"></a>Software updates offloading</h3><ul>
<li>We have an application running on EC2, that distributes software updates once in a while</li>
<li>When a new software update is out, we get a lot of request and the content is distributed in mass over the network. It’s very costly</li>
<li>We don’t want to change our application, but want to optimize our cost and CPU, how can we do it?</li>
</ul>
<h3 id="Why-CloudFront"><a href="#Why-CloudFront" class="headerlink" title="Why CloudFront?"></a>Why CloudFront?</h3><ul>
<li>No changes to architecture</li>
<li>Will cache software update files at the edge</li>
<li>Software update files are not dynamic, they’re static (never changing)</li>
<li>Our EC2 instances aren’t serverless</li>
<li>But CloudFront is, and will scale for us</li>
<li>Our ASG will not scale as much, and we’ll save tremendously in EC2</li>
<li>We’ll also save in availability, network bandwidth cost, etc</li>
<li>Easy way to make an existing application more scalable and cheaper!</li>
</ul>
<h3 id="Big-Data-Ingestion-Pipeline"><a href="#Big-Data-Ingestion-Pipeline" class="headerlink" title="Big Data Ingestion Pipeline"></a>Big Data Ingestion Pipeline</h3><ul>
<li>We want the ingestion pipeline to be fully serverless</li>
<li>We want to collect data in real time</li>
<li>We want to transform the data</li>
<li>We want to query the transformed data using SQL</li>
<li>The reports created using the queries should be in S3</li>
<li>We want to load that data into a warehouse and create dashboards</li>
</ul>
<hr>
<p>Serverless Architectures Quiz</p>
<p>Question 1:
As a solutions architect, you have been tasked to implement a fully Serverless REST API. Which technology choices do you recommend?
A: API Gateway + AWS Lambda</p>
<p>Question 2:
Which technology does not have an out of the box caching feature?
A: Lambda</p>
<p>  Lambda does not have an out of the box caching feature (it’s often paired with API gateway for that)</p>
<p>Question 3:
Which service allows to federate mobile users and generate temporary credentials so that they can access their own S3 bucket sub-folder?
A: Cognito</p>
<p>  in combination with STS</p>
<p>Question 4:
You would like to distribute your static content which currently lives in Amazon S3 to multiple regions around the world, such as the US, France and Australia. What do you recommend?
A: CloudFront</p>
<p>  This is a perfect use case for CloudFront</p>
<p>Question 5:
You have hosted a DynamoDB table in ap-northeast-1 and would like to make it available in eu-west-1. What must be enabled first to create a DynamoDB Global Table?
A: DynamoDB Streams.</p>
<p>  Streams enable DynamoDB to get a changelog and use that changelog to replicate data across regions</p>
<p>Question 6:
A Lambda function is triggered by a DynamoDB stream and is meant to insert data into SQS for further long processing jobs. The Lambda function does seem able to read from the DynamoDB stream but isn’t able to store messages in SQS. What’s the problem?
A: The Lambda IAM role is missing permissions</p>
<p>Question 7:
You would like to create a micro service whose sole purpose is to encode video files with your specific algorithm from S3 back into S3. You would like to make that micro-service reliable and retry upon failure. Processing a video may take over 25 minutes. The service is asynchronous and it should be possible for the service to be stopped for a day and resume the next day from the videos that haven’t been encoded yet. Which of the following service would you recommend to implement this service?
A: X SQS + EC2</p>
<p>  SQS allows you to retain messages for days and process them later, while we take down our EC2 instances</p>
<p>Question 8:
You would like to distribute paid software installation files globally for your customers that have indeed purchased the content. The software may be purchased by different users, and you want to protect the download URL with security including IP restriction. Which solution do you recommend?
A: CloudFront Signed URL</p>
<p>  This will have security including IP restriction</p>
<p>Question 9:
You are a photo hosting service and publish every month a master pack of beautiful mountains images, that are over 50 GB in size and downloaded from all around the world. The content is currently hosted on EFS and distributed by ELB and EC2 instances. You are experiencing high load each month and very high network costs. What can you recommend that won’t force an application refactor and reduce network costs and EC2 load dramatically?
A: Create a CloudFront distribution</p>
<p>  CloudFront can be used in front of an ELB</p>
<p>Question 10:
You would like to deliver big data streams in real time to multiple consuming applications, with replay features. Which technology do you recommend?
A: Kinesis Data Streams</p>
<hr>
<h2 id="Databases"><a href="#Databases" class="headerlink" title="Databases"></a>Databases</h2><h3 id="Choosing-the-Right-Database"><a href="#Choosing-the-Right-Database" class="headerlink" title="Choosing the Right Database"></a>Choosing the Right Database</h3><ul>
<li>We have a lot of managed databases on AWS to choose from</li>
<li>Questions to choose the right database based on your architecture:<ul>
<li>Read-heavy, write-heavy, or balanced workload? Throughput needs? Will it
change, does it need to scale or fluctuate during the day?</li>
<li>How much data to store and for how long? Will it grow? Average object size?
How are they accessed?</li>
<li>Data durability? Source of truth for the data ?</li>
<li>Latency requirements? Concurrent users?</li>
<li>Data model? How will you query the data? Joins? Structured? Semi-Structured?</li>
<li>Strong schema? More flexibility? Reporting? Search? RDBMS &#x2F; NoSQL?</li>
<li>License costs? Switch to Cloud Native DB such as Aurora?</li>
</ul>
</li>
</ul>
<h3 id="Database-Types"><a href="#Database-Types" class="headerlink" title="Database Types"></a>Database Types</h3><ul>
<li><strong>RDBMS (&#x3D; SQL &#x2F; OLTP)</strong>: RDS, Aurora – great for joins</li>
<li><strong>NoSQL database</strong>: DynamoDB (~JSON), ElastiCache (key &#x2F; value pairs),
Neptune (graphs) – no joins, no SQL</li>
<li><strong>Object Store</strong>: S3 (for big objects) &#x2F; Glacier (for backups &#x2F; archives)</li>
<li><strong>Data Warehouse</strong> (&#x3D; SQL Analytics &#x2F; BI): Redshift (OLAP), Athena</li>
<li><strong>Search</strong>: ElasticSearch (JSON) – free text, unstructured searches</li>
<li><strong>Graphs</strong>: Neptune – displays relationships between data</li>
</ul>
<h3 id="RDS-Overview"><a href="#RDS-Overview" class="headerlink" title="RDS Overview"></a>RDS Overview</h3><ul>
<li><p>Managed PostgreSQL &#x2F; MySQL &#x2F; Oracle &#x2F; SQL Server</p>
</li>
<li><p>Must <strong>provision an EC2 instance &amp; EBS Volume type and size</strong></p>
</li>
<li><p>Support for Read Replicas and Multi AZ</p>
</li>
<li><p>Security through IAM, Security Groups, KMS, SSL in transit</p>
</li>
<li><p>Backup &#x2F; Snapshot &#x2F; Point in time restore feature</p>
</li>
<li><p>Managed and Scheduled maintenance</p>
</li>
<li><p>Monitoring through CloudWatch</p>
</li>
<li><p><strong>Use case</strong>: Store relational datasets (RDBMS &#x2F; OLTP), perform SQL queries,
transactional inserts &#x2F; update &#x2F; delete is available</p>
</li>
</ul>
<h3 id="RDS-for-Solutions-Architect"><a href="#RDS-for-Solutions-Architect" class="headerlink" title="RDS for Solutions Architect"></a>RDS for Solutions Architect</h3><ul>
<li><strong>Operations</strong>: small downtime when failover happens, when maintenance happens, scaling in read replicas &#x2F; ec2 instance &#x2F; restore EBS implies manual intervention, application changes</li>
<li><strong>Security</strong>: AWS responsible for OS security, we are responsible for setting
up KMS, security groups, IAM policies, authorizing users in DB, using SSL</li>
<li><strong>Reliability</strong>: Multi AZ feature, failover in case of failures</li>
<li><strong>Performance</strong>: depends on EC2 instance type, EBS volume type, ability to
add Read Replicas. Doesn’t auto-scale</li>
<li><strong>Cost</strong>: Pay per hour based on provisioned EC2 and EBS</li>
</ul>
<h3 id="Aurora-Overview"><a href="#Aurora-Overview" class="headerlink" title="Aurora Overview"></a>Aurora Overview</h3><ul>
<li><p>Compatible API for PostgreSQL &#x2F; MySQL</p>
</li>
<li><p>Data is held in 6 replicas, across 3 AZ</p>
</li>
<li><p>Auto healing capability</p>
</li>
<li><p>Multi AZ, Auto Scaling Read Replicas</p>
</li>
<li><p>Read Replicas can be Global</p>
</li>
<li><p>Aurora database can be Global for DR or latency purposes</p>
</li>
<li><p>Auto scaling of storage from 10GB to 64 TB</p>
</li>
<li><p>Define EC2 instance type for aurora instances</p>
</li>
<li><p>Same security &#x2F; monitoring &#x2F; maintenance features as RDS</p>
</li>
<li><p>“Aurora Serverless” option</p>
</li>
<li><p><strong>Use case</strong>: same as RDS, but with less maintenance &#x2F; more flexibility &#x2F; more performance</p>
</li>
</ul>
<h3 id="Aurora-for-Solutions-Architect"><a href="#Aurora-for-Solutions-Architect" class="headerlink" title="Aurora for Solutions Architect"></a>Aurora for Solutions Architect</h3><ul>
<li><strong>Operations</strong>: less operations, auto scaling storage</li>
<li><strong>Security</strong>: AWS responsible for OS security, we are responsible for setting
up KMS, security groups, IAM policies, authorizing users in DB, using SSL</li>
<li><strong>Reliability</strong>: Multi AZ, highly available, possibly more than RDS, Aurora
Serverless option.</li>
<li><strong>Performance</strong>: 5x performance (according to AWS) due to architectural
optimizations. Up to 15 Read Replicas (only 5 for RDS)</li>
<li><strong>Cost</strong>: Pay per hour based on EC2 and storage usage. Possibly lower
costs compared to Enterprise grade databases such as Oracle</li>
</ul>
<h3 id="ElastiCache-Overview"><a href="#ElastiCache-Overview" class="headerlink" title="ElastiCache Overview"></a>ElastiCache Overview</h3><ul>
<li><p>Managed Redis &#x2F; Memcached (similar offering as RDS, but for caches)</p>
</li>
<li><p>In-memory data store, sub-millisecond latency</p>
</li>
<li><p>Must provision an EC2 instance type</p>
</li>
<li><p>Support for Clustering (Redis) and Multi AZ, Read Replicas (sharding)</p>
</li>
<li><p>Security through IAM, Security Groups, KMS, Redis Auth</p>
</li>
<li><p>Backup &#x2F; Snapshot &#x2F; Point in time restore feature</p>
</li>
<li><p>Managed and Scheduled maintenance</p>
</li>
<li><p>Monitoring through CloudWatch</p>
</li>
<li><p><strong>Use Case</strong>: Key&#x2F;Value store, Frequent reads, less writes, cache results for DB queries, store session data for websites, cannot use SQL.</p>
</li>
</ul>
<h2 id="ElastiCache-for-Solutions-Architect"><a href="#ElastiCache-for-Solutions-Architect" class="headerlink" title="ElastiCache for Solutions Architect"></a>ElastiCache for Solutions Architect</h2><ul>
<li><strong>Operations</strong>: same as RDS</li>
<li><strong>Security</strong>: AWS responsible for OS security, we are responsible for setting
up KMS, security groups, IAM policies, users (Redis Auth), using SSL</li>
<li><strong>Reliability</strong>: Clustering, Multi AZ</li>
<li><strong>Performance</strong>: Sub-millisecond performance, in memory, read replicas for
sharding, very popular cache option</li>
<li><strong>Cost</strong>: Pay per hour based on EC2 and storage usage</li>
</ul>
<h3 id="DynamoDB-Overview"><a href="#DynamoDB-Overview" class="headerlink" title="DynamoDB Overview"></a>DynamoDB Overview</h3><ul>
<li><p>AWS proprietary technology, managed NoSQL database</p>
</li>
<li><p>Serverless, provisioned capacity, auto scaling, on demand capacity (Nov 2018)</p>
</li>
<li><p>Can replace ElastiCache as a key&#x2F;value store (storing session data for example)</p>
</li>
<li><p>Highly Available, Multi AZ by default, Read and Writes are decoupled, DAX for read cache</p>
</li>
<li><p>Reads can be eventually consistent or strongly consistent</p>
</li>
<li><p>Security, authentication and authorization is done through IAM</p>
</li>
<li><p>DynamoDB Streams to integrate with AWS Lambda</p>
</li>
<li><p>Backup &#x2F; Restore feature, Global Table feature</p>
</li>
<li><p>Monitoring through CloudWatch</p>
</li>
<li><p>Can only query on primary key, sort key, or indexes</p>
</li>
<li><p><strong>Use Case</strong>: Serverless applications development (small documents 100s KB), distributed serverless cache, doesn’t have SQL query language available, has transactions capability from Nov 2018</p>
</li>
</ul>
<h3 id="DynamoDB-for-Solutions-Architect"><a href="#DynamoDB-for-Solutions-Architect" class="headerlink" title="DynamoDB for Solutions Architect"></a>DynamoDB for Solutions Architect</h3><ul>
<li><strong>Operations</strong>: no operations needed, auto scaling capability, serverless</li>
<li><strong>Security</strong>: full security through IAM policies, KMS encryption, SSL in flight</li>
<li><strong>Reliability</strong>: Multi AZ, Backups</li>
<li><strong>Performance</strong>: single digit millisecond performance, DAX for caching
reads, performance doesn’t degrade if your application scales</li>
<li><strong>Cost</strong>: Pay per provisioned capacity and storage usage (no need to guess
in advance any capacity – can use auto scaling)</li>
</ul>
<h2 id="S3-Overview"><a href="#S3-Overview" class="headerlink" title="S3 Overview"></a>S3 Overview</h2><ul>
<li><p>S3 is a… key &#x2F; value store for objects</p>
</li>
<li><p>Great for big objects, not so great for small objects</p>
</li>
<li><p>Serverless, scales infinitely, max object size is 5 TB</p>
</li>
<li><p>Eventually consistency for overwrites and deletes</p>
</li>
<li><p>Tiers: S3 Standard, S3 IA, S3 One Zone IA, Glacier for backups</p>
</li>
<li><p>Features: Versioning, Encryption, Cross Region Replication, etc…</p>
</li>
<li><p>Security: IAM, Bucket Policies, ACL</p>
</li>
<li><p>Encryption: SSE-S3, SSE-KMS, SSE-C, client side encryption, SSL in transit</p>
</li>
<li><p><strong>Use Case</strong>: static files, key value store for big files, website hosting</p>
</li>
</ul>
<h2 id="S3-for-Solutions-Architect"><a href="#S3-for-Solutions-Architect" class="headerlink" title="S3 for Solutions Architect"></a>S3 for Solutions Architect</h2><ul>
<li><strong>Operations</strong>: no operations needed</li>
<li><strong>Security</strong>: IAM, Bucket Policies, ACL, Encryption (Server&#x2F;Client), SSL</li>
<li><strong>Reliability</strong>: 99.999999999% durability &#x2F; 99.99% availability, Multi AZ, CRR</li>
<li><strong>Performance</strong>: scales to thousands of read &#x2F; writes per second, transfer
acceleration &#x2F; multi-part for big files</li>
<li><strong>Cost</strong>: pay per storage usage, network cost, requests number</li>
</ul>
<h3 id="Athena-Overview"><a href="#Athena-Overview" class="headerlink" title="Athena Overview"></a>Athena Overview</h3><ul>
<li><p>Fully Serverless database with SQL capabilities</p>
</li>
<li><p>Used to query data in S3</p>
</li>
<li><p>Pay per query</p>
</li>
<li><p>Output results back to S3</p>
</li>
<li><p>Secured through IAM</p>
</li>
<li><p><strong>Use Case</strong>: one time SQL queries, serverless queries on S3, log analytics</p>
</li>
</ul>
<h3 id="Athena-for-Solutions-Architect"><a href="#Athena-for-Solutions-Architect" class="headerlink" title="Athena for Solutions Architect"></a>Athena for Solutions Architect</h3><ul>
<li><strong>Operations</strong>: no operations needed, serverless</li>
<li><strong>Security</strong>: IAM + S3 security</li>
<li><strong>Reliability</strong>: managed service, uses Presto engine, highly available</li>
<li><strong>Performance</strong>: queries scale based on data size</li>
<li><strong>Cost</strong>: pay per query &#x2F; per TB of data scanned, serverless</li>
</ul>
<h3 id="Redshift-Overview"><a href="#Redshift-Overview" class="headerlink" title="Redshift Overview"></a>Redshift Overview</h3><ul>
<li>Redshift is based on PostgreSQL, but <strong>it’s not used for OLTP</strong></li>
<li><strong>It’s OLAP – online analytical processing (analytics and data warehousing)</strong></li>
<li>10x better performance than other data warehouses, scale to PBs of data</li>
<li><strong>Columnar</strong> storage of data (instead of row based)</li>
<li>Massively Parallel Query Execution (MPP), highly available</li>
<li>Pay as you go based on the instances provisioned</li>
<li>Has a SQL interface for performing the queries</li>
<li>BI tools such as AWS Quicksight or Tableau integrate with it</li>
</ul>
<h3 id="Redshift-Continued…"><a href="#Redshift-Continued…" class="headerlink" title="Redshift Continued…"></a>Redshift Continued…</h3><ul>
<li>Data is loaded from S3, DynamoDB, DMS, other DBs…</li>
<li>From 1 node to 128 nodes, up to 160 GB of space per node</li>
<li><strong>Leader node</strong>: for query planning, results aggregation</li>
<li><strong>Compute node</strong>: for performing the queries, send results to leader</li>
<li><strong>Redshift Spectrum</strong>: perform queries directly against S3 (no need to load)</li>
<li>Backup &amp; Restore, Security VPC &#x2F; IAM &#x2F; KMS, Monitoring</li>
<li>Redshift Enhanced VPC Routing: COPY &#x2F; UNLOAD goes through VPC</li>
</ul>
<h3 id="Redshift-–-Snapshots-amp-DR"><a href="#Redshift-–-Snapshots-amp-DR" class="headerlink" title="Redshift – Snapshots &amp; DR"></a>Redshift – Snapshots &amp; DR</h3><ul>
<li><p>Snapshots are point-in-time backups of a cluster, stored internally in S3</p>
</li>
<li><p>Snapshots are incremental (only what has changed is saved)</p>
</li>
<li><p>You can restore a snapshot into a <strong>new cluster</strong></p>
</li>
<li><p>Automated: every 8 hours, every 5 GB, or on a schedule. Set retention</p>
</li>
<li><p>Manual: snapshot is retained until you delete it</p>
</li>
<li><p><strong>You can configure Amazon Redshift to automatically copy snapshots (automated or manual) of a cluster to another AWS Region</strong></p>
</li>
</ul>
<p>Redshift Spectrum</p>
<ul>
<li>Query data that is already in S3 without loading it</li>
<li><strong>Must have a Redshift cluster available to start the query</strong></li>
<li>The query is then submitted to thousands of Redshift Spectrum nodes</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required">https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required</a></p>
<h3 id="Redshift-for-Solutions-Architect"><a href="#Redshift-for-Solutions-Architect" class="headerlink" title="Redshift for Solutions Architect"></a>Redshift for Solutions Architect</h3><ul>
<li><p><strong>Operations</strong>: similar to RDS</p>
</li>
<li><p><strong>Security</strong>: IAM, VPC, KMS, SSL (similar to RDS)</p>
</li>
<li><p><strong>Reliability</strong>: highly available, auto healing features</p>
</li>
<li><p><strong>Performance</strong>: 10x performance vs other data warehousing, compression</p>
</li>
<li><p><strong>Cost</strong>: pay per node provisioned, 1&#x2F;10th of the cost vs other warehouses</p>
</li>
<li><p><strong>Remember: Redshift &#x3D; Analytics &#x2F; BI &#x2F; Data Warehouse</strong></p>
</li>
</ul>
<h3 id="Neptune"><a href="#Neptune" class="headerlink" title="Neptune"></a>Neptune</h3><ul>
<li>Fully managed <strong>graph</strong> database</li>
<li><strong>When do we use Graphs?</strong><ul>
<li>High relationship data</li>
<li>Social Networking: Users friends with Users, replied to comment on post of user and likes other comments.</li>
<li>Knowledge graphs (Wikipedia)</li>
</ul>
</li>
<li>Highly available across 3 AZ, with up to 15 read replicas</li>
<li>Point-in-time recovery, continuous backup to Amazon S3</li>
<li>Support for KMS encryption at rest + HTTPS</li>
</ul>
<h3 id="Neptune-for-Solutions-Architect"><a href="#Neptune-for-Solutions-Architect" class="headerlink" title="Neptune for Solutions Architect"></a>Neptune for Solutions Architect</h3><ul>
<li><p><strong>Operations</strong>: similar to RDS</p>
</li>
<li><p><strong>Security</strong>: IAM, VPC, KMS, SSL (similar to RDS) + IAM Authentication</p>
</li>
<li><p><strong>Reliability</strong>: Multi-AZ, clustering</p>
</li>
<li><p><strong>Performance</strong>: best suited for graphs, clustering to improve performance</p>
</li>
<li><p><strong>Cost</strong>: pay per node provisioned (similar to RDS)</p>
</li>
<li><p><strong>Remember: Neptune &#x3D; Graphs</strong></p>
</li>
</ul>
<h3 id="ElasticSearch"><a href="#ElasticSearch" class="headerlink" title="ElasticSearch"></a>ElasticSearch</h3><ul>
<li>Example: In DynamoDB, you can only find by primary key or indexes.</li>
<li>With ElasticSearch, you can search any field, even partially matches</li>
<li>It’s common to use ElasticSearch as a complement to another database</li>
<li>ElasticSearch also has some usage for Big Data applications</li>
<li>You can provision a cluster of instances</li>
<li>Built-in integrations: Amazon Kinesis Data Firehose, AWS IoT, and
Amazon CloudWatch Logs for data ingestion</li>
<li>Security through Cognito &amp; IAM, KMS encryption, SSL &amp; VPC</li>
<li>Comes with Kibana (visualization) &amp; Logstash (log ingestion) – ELK stack</li>
</ul>
<h3 id="ElasticSearch-for-Solutions-Architect"><a href="#ElasticSearch-for-Solutions-Architect" class="headerlink" title="ElasticSearch for Solutions Architect"></a>ElasticSearch for Solutions Architect</h3><ul>
<li><p><strong>Operations</strong>: similar to RDS</p>
</li>
<li><p><strong>Security</strong>: Cognito, IAM, VPC, KMS, SSL</p>
</li>
<li><p><strong>Reliability</strong>: Multi-AZ, clustering</p>
</li>
<li><p><strong>Performance</strong>: based on ElasticSearch project (open source), petabyte scale</p>
</li>
<li><p><strong>Cost</strong>: pay per node provisioned (similar to RDS)</p>
</li>
<li><p><strong>Remember: ElasticSearch &#x3D; Search &#x2F; Indexing</strong></p>
</li>
</ul>
<hr>
<p>Databases Quiz</p>
<p>Question 1:
Which database helps you store data in a relational format, with SQL language compatibility and capability of processing transactions?
A: RDS</p>
<p>Question 2:
Which database do you suggest to have caching capability with a Redis compatible API?
A: ElascticCache</p>
<p>  ElastiCache can create a Redis cache or a Memcached cache</p>
<p>Question 3:
You are looking to perform OLTP, and would like to have the underlying storage with the maximum amount of replication and auto-scaling capability. What do you recommend?
A: Aurora</p>
<p>Question 4:
As a solution architect, you plan on creating a social media website where users can be friends with each other, and like each other’s posts. You plan on performing some complicated queries such as “What are the number of likes on the posts that have been posted by the friends of Mike?”. What database do you suggest?
A: Neptune</p>
<p>  This is AWS’ managed graph database</p>
<p>Question 5:
You would like to store big objects of 100 MB into a reliable and durable Key Value store. What do you recommend?
A: S3</p>
<p>  S3 is indeed a key value store! (where the key is the full path of the object in the bucket)</p>
<p>Question 6:
You would like to have a database which is efficient at performing analytical queries on large sets of columnar data. You would like to connect that Data Warehouse to a reporting and dashboard tool such as Amazon Quicksight. Which technology do you recommend?
A: Redshift</p>
<p>Question 7:
Your log data is currently stored in S3 and you would like to perform a quick analysis if possible serverless to filter the logs and find a user which may have completed an unauthorized action. Which technology do you recommend?
A: Athena</p>
<p>Question 8:
Your gaming website is currently running on top of DynamoDB. Users have been asking for a search feature to find other gamers by name, with partial matches if possible. Which technology do you recommend to implement that feature?
A: ElasticSearch</p>
<p>  Anytime you see “search”, think ElasticSearch</p>
<hr>
<h2 id="AWS-Monitoring-Audit-and-Performance"><a href="#AWS-Monitoring-Audit-and-Performance" class="headerlink" title="AWS Monitoring, Audit and Performance"></a>AWS Monitoring, Audit and Performance</h2><h3 id="AWS-CloudWatch-Metrics"><a href="#AWS-CloudWatch-Metrics" class="headerlink" title="AWS CloudWatch Metrics"></a>AWS CloudWatch Metrics</h3><ul>
<li>CloudWatch provides metrics for every services in AWS</li>
<li><strong>Metric</strong> is a variable to monitor (CPUUtilization, NetworkIn…)</li>
<li>Metrics belong to <strong>namespaces</strong></li>
<li><strong>Dimension</strong> is an attribute of a metric (instance id, environment, etc…).</li>
<li>Up to 10 dimensions per metric</li>
<li>Metrics have <strong>timestamps</strong></li>
<li>Can create CloudWatch dashboards of metrics</li>
</ul>
<h3 id="AWS-CloudWatch-EC2-Detailed-monitoring"><a href="#AWS-CloudWatch-EC2-Detailed-monitoring" class="headerlink" title="AWS CloudWatch EC2 Detailed monitoring"></a>AWS CloudWatch EC2 Detailed monitoring</h3><ul>
<li><p>EC2 instance metrics have metrics “every 5 minutes”</p>
</li>
<li><p>With detailed monitoring (for a cost), you get data “every 1 minute”</p>
</li>
<li><p>Use detailed monitoring if you want to more prompt scale your ASG!</p>
</li>
<li><p>The AWS Free Tier allows us to have 10 detailed monitoring metrics</p>
</li>
<li><p><strong>Note: EC2 Memory usage is by default not pushed (must be pushed from inside the instance as a custom metric)</strong></p>
</li>
</ul>
<h3 id="AWS-CloudWatch-Custom-Metrics"><a href="#AWS-CloudWatch-Custom-Metrics" class="headerlink" title="AWS CloudWatch Custom Metrics"></a>AWS CloudWatch Custom Metrics</h3><ul>
<li>Possibility to define and send your own custom metrics to CloudWatch</li>
<li>Ability to use dimensions (attributes) to segment metrics<ul>
<li>Instance.id</li>
<li>Environment.name</li>
</ul>
</li>
<li>Metric resolution (<strong>StorageResolution</strong> API parameter – two possible value):<ul>
<li>Standard: 1 minute (60 seconds)</li>
<li>High Resolution: 1 second – Higher cost</li>
</ul>
</li>
<li>Use API call <strong>PutMetricData</strong></li>
<li>Use exponential back off in case of throttle errors</li>
</ul>
<h3 id="CloudWatch-Dashboards"><a href="#CloudWatch-Dashboards" class="headerlink" title="CloudWatch Dashboards"></a>CloudWatch Dashboards</h3><ul>
<li><p>Great way to setup dashboards for quick access to keys metrics</p>
</li>
<li><p><strong>Dashboards are global</strong></p>
</li>
<li><p><strong>Dashboards can include graphs from different regions</strong></p>
</li>
<li><p>You can change the time zone &amp; time range of the dashboards</p>
</li>
<li><p>You can setup automatic refresh (10s, 1m, 2m, 5m, 15m)</p>
</li>
<li><p>Pricing:</p>
</li>
<li><p>3 dashboards (up to 50 metrics) for free</p>
</li>
<li><p>$3&#x2F;dashboard&#x2F;month afterwards</p>
</li>
</ul>
<h3 id="AWS-CloudWatch-Logs"><a href="#AWS-CloudWatch-Logs" class="headerlink" title="AWS CloudWatch Logs"></a>AWS CloudWatch Logs</h3><ul>
<li>Applications can send logs to CloudWatch using the SDK</li>
<li>CloudWatch can collect log from:<ul>
<li>Elastic Beanstalk: collection of logs from application</li>
<li>ECS: collection from containers</li>
<li>AWS Lambda: collection from function logs</li>
<li>VPC Flow Logs: VPC specific logs</li>
<li>API Gateway</li>
<li>CloudTrail based on filter</li>
<li>CloudWatch log agents: for example on EC2 machines</li>
<li>Route53: Log DNS queries</li>
</ul>
</li>
<li>CloudWatch Logs can go to:<ul>
<li>Batch exporter to S3 for archival</li>
<li>Stream to ElasticSearch cluster for further analytics</li>
</ul>
</li>
</ul>
<h3 id="AWS-CloudWatch-Logs-1"><a href="#AWS-CloudWatch-Logs-1" class="headerlink" title="AWS CloudWatch Logs"></a>AWS CloudWatch Logs</h3><ul>
<li>Logs storage architecture:<ul>
<li>Log groups: arbitrary name, usually representing an application</li>
<li>Log stream: instances within application &#x2F; log files &#x2F; containers</li>
</ul>
</li>
<li>Can define log expiration policies (never expire, 30 days, etc..)</li>
<li>Using the AWS CLI we can tail CloudWatch logs</li>
<li>To send logs to CloudWatch, make sure IAM permissions are correct!</li>
<li>Security: encryption of logs using KMS at the Group Level</li>
</ul>
<h3 id="CloudWatch-Logs-Metric-Filter-amp-Insights"><a href="#CloudWatch-Logs-Metric-Filter-amp-Insights" class="headerlink" title="CloudWatch Logs Metric Filter &amp; Insights"></a>CloudWatch Logs Metric Filter &amp; Insights</h3><ul>
<li><p>CloudWatch Logs can use filter expressions</p>
<ul>
<li>For example, find a specific IP inside of a log</li>
<li>Metric filters can be used to trigger alarms</li>
</ul>
</li>
<li><p>CloudWatch Logs Insights (new – Nov 2018) can be used to query logs
and add queries to CloudWatch Dashboards</p>
</li>
</ul>
<h3 id="AWS-CloudWatch-Alarms"><a href="#AWS-CloudWatch-Alarms" class="headerlink" title="AWS CloudWatch Alarms"></a>AWS CloudWatch Alarms</h3><ul>
<li>Alarms are used to trigger notifications for any metric</li>
<li>Alarms can go to Auto Scaling, EC2 Actions, SNS notifications</li>
<li>Various options (sampling, %, max, min, etc…)</li>
<li>Alarm States:<ul>
<li>OK</li>
<li>INSUFFICIENT_DATA (Missing data points)</li>
<li>ALARM</li>
</ul>
</li>
<li>Period:<ul>
<li>Length of time in seconds to evaluate the metric</li>
<li>High resolution custom metrics: can only choose 10 sec or 30 sec</li>
</ul>
</li>
</ul>
<h3 id="AWS-CloudWatch-Events"><a href="#AWS-CloudWatch-Events" class="headerlink" title="AWS CloudWatch Events"></a>AWS CloudWatch Events</h3><ul>
<li><p>Source + Rule &#x3D;&gt; Target</p>
</li>
<li><p>Schedule: Cron jobs</p>
</li>
<li><p>Event Pattern: Event rules to react to a service doing something</p>
<ul>
<li>Ex: CodePipeline state changes!</li>
</ul>
</li>
<li><p>Triggers to Lambda functions, SQS&#x2F;SNS&#x2F;Kinesis Messages</p>
</li>
<li><p>CloudWatch Event creates a small JSON document to give information
about the change</p>
</li>
</ul>
<h3 id="AWS-CloudTrail"><a href="#AWS-CloudTrail" class="headerlink" title="AWS CloudTrail"></a>AWS CloudTrail</h3><ul>
<li>Provides governance, compliance and audit for your AWS Account</li>
<li>CloudTrail is enabled by default!</li>
<li>Get an history of events &#x2F; API calls made within your AWS Account by:<ul>
<li>Console</li>
<li>SDK</li>
<li>CLI</li>
<li>AWS Services</li>
</ul>
</li>
<li>Can put logs from CloudTrail into CloudWatch Logs</li>
<li><strong>If a resource is deleted in AWS, look into CloudTrail first!</strong></li>
</ul>
<h3 id="AWS-Config"><a href="#AWS-Config" class="headerlink" title="AWS Config"></a>AWS Config</h3><ul>
<li>Helps with auditing and recording compliance of your AWS resources</li>
<li>Helps record configurations and changes over time</li>
<li>Possibility of storing the configuration data into S3 (analyzed by Athena)</li>
<li>Questions that can be solved by AWS Config:<ul>
<li>Is there unrestricted SSH access to my security groups?</li>
<li>Do my buckets have any public access?</li>
<li>How has my ALB configuration changed over time?</li>
</ul>
</li>
<li>You can receive alerts (SNS notifications) for any changes</li>
<li>AWS Config is a per-region service</li>
<li>Can be aggregated across regions and accounts</li>
</ul>
<p>AWS Config Resource - View compliance of a resource over time</p>
<h3 id="AWS-Config-Rules"><a href="#AWS-Config-Rules" class="headerlink" title="AWS Config Rules"></a>AWS Config Rules</h3><ul>
<li>Can use AWS managed config rules (over 75)</li>
<li>Can make custom config rules (must be defined in AWS Lambda)<ul>
<li>Evaluate if each EBS disk is of type gp2</li>
<li>Evaluate if each EC2 instance is t2.micro</li>
</ul>
</li>
<li>Rules can be evaluated &#x2F; triggered:<ul>
<li>For each config change</li>
<li>And &#x2F; or: at regular time intervals</li>
<li>Can trigger CloudWatch Events if the rule is non-compliant (and chain with Lambda)</li>
</ul>
</li>
<li>Rules can have auto remediations:<ul>
<li>If a resource is not compliant, you can trigger an auto remediation</li>
<li>Ex: stop instances with non-approved tags</li>
</ul>
</li>
<li><strong>AWS Config Rules does not prevent actions from happening (no deny)</strong></li>
<li><strong>Pricing</strong>: no free tier, $2 per active rule per region per month</li>
</ul>
<h3 id="CloudWatch-vs-CloudTrail-vs-Config"><a href="#CloudWatch-vs-CloudTrail-vs-Config" class="headerlink" title="CloudWatch vs CloudTrail vs Config"></a>CloudWatch vs CloudTrail vs Config</h3><ul>
<li>CloudWatch<ul>
<li>Performance monitoring (metrics, CPU, network, etc…) &amp; dashboards</li>
<li>Events &amp; Alerting</li>
<li>Log Aggregation &amp; Analysis</li>
</ul>
</li>
<li>CloudTrail<ul>
<li>Record API calls made within your Account by everyone</li>
<li>Can define trails for specific resources</li>
<li>Global Service</li>
</ul>
</li>
<li>Config<ul>
<li>Record configuration changes</li>
<li>Evaluate resources against compliance rules</li>
<li>Get timeline of changes and compliance</li>
</ul>
</li>
</ul>
<h3 id="For-an-Elasctic-Load-Balancer"><a href="#For-an-Elasctic-Load-Balancer" class="headerlink" title="For an Elasctic Load Balancer"></a>For an Elasctic Load Balancer</h3><ul>
<li>CloudWatch:<ul>
<li>Monitoring Incoming connections metic</li>
<li>Visualize error codes as a % over time</li>
<li>Make a dashboard to get an idea of your load blanacer performance</li>
</ul>
</li>
<li>Config:<ul>
<li>Track security group rules for the load Balancer</li>
<li>Track configuration changes for the Load Balancer</li>
<li>Ensure an SSL certificate is alqways assigned to the Load Blanacer (compliace)</li>
</ul>
</li>
<li>CloudTrail:<ul>
<li>Track who made any changes to the Load Balancer with API calls.</li>
</ul>
</li>
</ul>
<hr>
<p>Monitoring Quiz</p>
<p>Question 1:
We’d like to have CloudWatch Metrics for EC2 at a 1 minute rate. What should we do?
A: Enable Detailed Monitoring</p>
<p>  This is a paid offering and gives you EC2 metrics at a 1 minute rate</p>
<p>Question 2:
High Resolution Custom Metrics can have a minimum resolution of
A: 1 second</p>
<p>Question 3:
Your CloudWatch alarm is triggered and controls an ASG. The alarm should trigger 1 instance being deleted from your ASG, but your ASG has already 2 instances running and the minimum capacity is 2. What will happen?
A: The alarm will remain in “ALARM” state but never decrease the number of instances in my ASG</p>
<p>  The number of instances in an ASG cannot go below the minimum, even if the alarm would in theory trigger an instance termination</p>
<p>Question 4:
An Alarm on a High Resolution Metric can be triggered as often as
A:</p>
<p>Question 5:
You have made a configuration change and would like to evaluate the impact of it on the performance of your application. Which service do you use?
A: CloudWatch</p>
<p>  CloudWatch is used to monitor the applications performance &#x2F; metrics</p>
<p>Question 6:
Someone has terminated an EC2 instance in your account last week, which was hosting a critical database. You would like to understand who did it and when, how can you achieve that?
A: Look a CloudTrail</p>
<p>  CloudTrail helps audit the API calls made within your account, so the database deletion API call will appear here (regardless if made from the console, the CLI, or an SDK)</p>
<p>Question 7:
You would like to ensure that over time, none of your EC2 instances expose the port 84 as it is known to have vulnerabilities with the OS you are using. What can you do to monitor this?
A: Setup Config Rules</p>
<p>Question 8:
You would like to evaluate the compliance of your resource’s configurations over time. Which technology do you choose?
A: Config</p>
<hr>
<h3 id="AWS-STS-–-Security-Token-Service"><a href="#AWS-STS-–-Security-Token-Service" class="headerlink" title="AWS STS – Security Token Service"></a>AWS STS – Security Token Service</h3><ul>
<li><strong>Allows to grant limited and temporary access to AWS resources.</strong></li>
<li>Token is valid for up to one hour (must be refreshed)</li>
<li><strong>AssumeRole</strong><ul>
<li>Within your own account: for enhanced security</li>
<li>Cross Account Access: assume role in target account to perform actions there</li>
</ul>
</li>
<li><strong>AssumeRoleWithSAML</strong><ul>
<li>return credentials for users logged with SAML</li>
</ul>
</li>
<li><strong>AssumeRoleWithWebIdentity</strong><ul>
<li>return creds for users logged with an IdP (Facebook Login, Google Login, OIDC compatible…)</li>
<li>AWS recommends against using this, and using Cognito instead</li>
</ul>
</li>
<li><strong>GetSessionToken</strong><ul>
<li>for MFA, from a user or AWS account root user</li>
</ul>
</li>
</ul>
<h3 id="Using-STS-to-Assume-a-Role"><a href="#Using-STS-to-Assume-a-Role" class="headerlink" title="Using STS to Assume a Role"></a>Using STS to Assume a Role</h3><ul>
<li>Define an IAM Role within your account or cross-account</li>
<li>Define which principals can access this IAM Role</li>
<li>Use AWS STS (Security Token Service) to retrieve credentials and impersonate the IAM Role you have access to <strong>(AssumeRole API</strong>)</li>
<li>Temporary credentials can be valid between <strong>15 minutes to 1 hour</strong></li>
</ul>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_aws-accounts.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_aws-accounts.html</a></p>
<p>Identity Federation in AWS</p>
<ul>
<li><p>Federation lets users outside of AWS to assume
temporary role for accessing AWS resources.</p>
</li>
<li><p>These users assume identity provided access role.</p>
</li>
<li><p>Federations can have many flavors:</p>
</li>
<li><p>SAML 2.0</p>
</li>
<li><p>Custom Identity Broker</p>
</li>
<li><p>Web Identity Federation with Amazon Cognito</p>
</li>
<li><p>Web Identity Federation without Amazon Cognito</p>
</li>
<li><p>Single Sign On</p>
</li>
<li><p>Non-SAML with AWS Microsoft AD</p>
</li>
<li><p>Using federation, you don’t need to create IAM users
(user management is outside of AWS)</p>
</li>
</ul>
<p>SAML 2.0 Federation</p>
<ul>
<li>To integrate Active Directory &#x2F; ADFS with AWS (or any SAML 2.0)</li>
<li>Provides access to AWS Console or CLI (through temporary creds)</li>
<li>No need to create an IAM user for each of your employees</li>
</ul>
<h3 id="AWS-Directory-Services"><a href="#AWS-Directory-Services" class="headerlink" title="AWS Directory Services"></a>AWS Directory Services</h3><ul>
<li><strong>AWS Managed Microsoft AD</strong><ul>
<li>Create your own AD in AWS, manage users locally, supports MFA</li>
<li>Establish “trust” connections with your on- premise AD</li>
</ul>
</li>
<li><strong>AD Connector</strong><ul>
<li>Directory Gateway (proxy) to redirect to on- premise AD</li>
<li>Users are managed on the on-premise AD</li>
</ul>
</li>
<li><strong>Simple AD</strong><ul>
<li>AD-compatible managed directory on AWS</li>
<li>Cannot be joined with on-premise AD</li>
</ul>
</li>
</ul>
<h3 id="AWS-Organizations"><a href="#AWS-Organizations" class="headerlink" title="AWS Organizations"></a>AWS Organizations</h3><ul>
<li>Global service</li>
<li>Allows to manage multiple AWS accounts</li>
<li>The main account is the master account – you can’t change it</li>
<li>Other accounts are member accounts</li>
<li>Member accounts can only be part of one organization</li>
<li>Consolidated Billing across all accounts - single payment method</li>
<li>Pricing benefits from aggregated usage (volume discount for EC2, S3…)</li>
<li>API is available to automate AWS account creation</li>
</ul>
<h3 id="Multi-Account-Strategies"><a href="#Multi-Account-Strategies" class="headerlink" title="Multi Account Strategies"></a>Multi Account Strategies</h3><ul>
<li><p>Create accounts per department, per cost center, per dev &#x2F; test &#x2F;
prod, based on regulatory restrictions (using SCP), for better
resource isolation (ex: VPC), to have separate per-account service
limits, isolated account for logging</p>
</li>
<li><p>Multi Account vs One Account Multi VPC</p>
</li>
<li><p>Use tagging standards for billing purposes</p>
</li>
<li><p>Enable CloudTrail on all accounts, send logs to central S3 account</p>
</li>
<li><p>Send CloudWatch Logs to central logging account</p>
</li>
<li><p>Establish Cross Account Roles for Admin purposes</p>
</li>
</ul>
<h3 id="IAM-Conditions"><a href="#IAM-Conditions" class="headerlink" title="IAM Conditions"></a>IAM Conditions</h3><p><strong>aws:SourceIP</strong>: restrict the client IP from
which the API calls are being made</p>
<p><strong>Aws:RequestedRegion</strong>: restrict the region
The API calls are made to</p>
<p><strong>Restrict based on tags Force</strong>
<strong>MFA</strong></p>
<h3 id="IAM-for-S3"><a href="#IAM-for-S3" class="headerlink" title="IAM for S3"></a>IAM for S3</h3><ul>
<li><p>ListBucket permission applies to
arn:aws:s3:::test</p>
</li>
<li><p>&#x3D;&gt; bucket level permission</p>
</li>
<li><p>GetObject, PutObject,
DeleteObject applies to
arn:awn:s3:::test&#x2F;*</p>
</li>
<li><p>&#x3D;&gt; object level permission</p>
</li>
</ul>
<hr>
<p>Identity and Access Management (IAM) - Advanced - Quiz</p>
<p>Question 1:
We need to gain access to a Role in another AWS account. How is it done?
A: We should use the STS service to gain temporary credentials</p>
<p>  STS will allow us to get cross account access through the creation of a role in our account authorized to access a role in another account. See more here: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html</a></p>
<p>Question 2:
You have a mobile application and would like to give your users access to their own personal space in Amazon S3. How do you achieve that?
A: Use Cognito Identity Federation</p>
<p>  Cognito is made to federate mobile user accounts and provide them with their own IAM policy. As such, they should be able thanks to that policy to access their own personal space in Amazon S3.</p>
<p>  X: Use SAML Identity Federation</p>
<pre><code>SAML identity federation is used to integrate a service such as Active Directory with AWS. It does not work for mobile applciations
</code></pre>
<p>Question 3:
You have strong regulatory requirements to only allow fully internally audited AWS Services in production. You still want to allow your teams to experiment in development environments while services are being audited. How can you best set this up?
A: Create an AWS Organization and create two Prod and Dev OU. Apply a SCP on Prod</p>
<p>Question 4:
[SAA-C02] You have an on-premise active directory setup and would like to provide access for your on-premise users to the multiple accounts you have in AWS. The solution should scale to adding accounts in the future. What do you recommend?
A: Setup AWS Single Sign-On</p>
<h2 id="Question-5-Which-AWS-Directory-Service-allows-you-to-proxy-requests-to-your-on-premise-active-directory-A-AD-Connector"><a href="#Question-5-Which-AWS-Directory-Service-allows-you-to-proxy-requests-to-your-on-premise-active-directory-A-AD-Connector" class="headerlink" title="Question 5:
Which AWS Directory Service allows you to proxy requests to your on-premise active directory?
A: AD Connector"></a>Question 5:
Which AWS Directory Service allows you to proxy requests to your on-premise active directory?
A: AD Connector</h2><p>[*]</p>
<h2 id="Section-20-AWS-Security-amp-Encryption-KMS-SSM-Parameter-Store-CloudHSM-Shield-WAF"><a href="#Section-20-AWS-Security-amp-Encryption-KMS-SSM-Parameter-Store-CloudHSM-Shield-WAF" class="headerlink" title="Section 20: AWS Security &amp; Encryption: KMS, SSM Parameter Store, CloudHSM, Shield, WAF"></a>Section 20: AWS Security &amp; Encryption: KMS, SSM Parameter Store, CloudHSM, Shield, WAF</h2><h2 id="AWS-Security-amp-Encryption"><a href="#AWS-Security-amp-Encryption" class="headerlink" title="AWS Security &amp; Encryption"></a>AWS Security &amp; Encryption</h2><h2 id="Why-encryption"><a href="#Why-encryption" class="headerlink" title="Why encryption?"></a>Why encryption?</h2><h3 id="Encryption-in-flight-SSL"><a href="#Encryption-in-flight-SSL" class="headerlink" title="Encryption in flight (SSL)"></a>Encryption in flight (SSL)</h3><ul>
<li>Data is encrypted before sending and decrypted after receiving</li>
<li>SSL certificates help with encryption (HTTPS)</li>
<li>Encryption in flight ensures no MITM (man in the middle attack) can happen</li>
</ul>
<h2 id="Why-encryption-1"><a href="#Why-encryption-1" class="headerlink" title="Why encryption?"></a>Why encryption?</h2><h3 id="Server-side-encryption-at-rest"><a href="#Server-side-encryption-at-rest" class="headerlink" title="Server side encryption at rest"></a>Server side encryption at rest</h3><ul>
<li>Data is encrypted after being received by the server</li>
<li>Data is decrypted before being sent</li>
<li>It is stored in an encrypted form thanks to a key (usually a data key)</li>
<li>The encryption &#x2F; decryption keys must be managed somewhere and
the server must have access to it</li>
</ul>
<h2 id="Why-encryption-2"><a href="#Why-encryption-2" class="headerlink" title="Why encryption?"></a>Why encryption?</h2><h3 id="Client-side-encryption"><a href="#Client-side-encryption" class="headerlink" title="Client side encryption"></a>Client side encryption</h3><ul>
<li>Data is encrypted by the client and never decrypted by the server</li>
<li>Data will be decrypted by a receiving client</li>
<li>The server should not be able to decrypt the data</li>
<li>Could leverage Envelope Encryption</li>
</ul>
<h3 id="AWS-KMS-Key-Management-Service"><a href="#AWS-KMS-Key-Management-Service" class="headerlink" title="AWS KMS (Key Management Service)"></a>AWS KMS (Key Management Service)</h3><ul>
<li>Anytime you hear “encryption” for an AWS service, it’s most likely KMS</li>
<li>Easy way to control access to your data, AWS manages keys for us</li>
<li>Fully integrated with IAM for authorization</li>
<li>Seamlessly integrated into:<ul>
<li>Amazon EBS: encrypt volumes</li>
<li>Amazon S3: Server side encryption of objects</li>
<li>Amazon Redshift: encryption of data</li>
<li>Amazon RDS: encryption of data</li>
<li>Amazon SSM: Parameter store</li>
<li>Etc…</li>
</ul>
</li>
<li>But you can also use the CLI &#x2F; SDK</li>
</ul>
<h3 id="KMS-–-Customer-Master-Key-CMK-Types"><a href="#KMS-–-Customer-Master-Key-CMK-Types" class="headerlink" title="KMS – Customer Master Key (CMK) Types"></a>KMS – Customer Master Key (CMK) Types</h3><ul>
<li><strong>Symmetric (AES-256 keys)</strong><ul>
<li>First offering of KMS, single encryption key that is used to Encrypt and Decrypt</li>
<li>AWS services that are integrated with KMS use Symmetric CMKs</li>
<li>Necessary for envelope encryption</li>
<li>You never get access to the Key unencrypted (must call KMS API to use)</li>
</ul>
</li>
<li><strong>Asymmetric (RSA &amp; ECC key pairs)</strong><ul>
<li>Public (Encrypt) and Private Key (Decrypt) pair</li>
<li>Used for Encrypt&#x2F;Decrypt, or Sign&#x2F;Verify operations</li>
<li>The public key is downloadable, but you access the Private Key unencrypted</li>
<li>Use case: encryption outside of AWS by users who can’t call the KMS API</li>
</ul>
</li>
</ul>
<h3 id="AWS-KMS-Key-Management-Service-1"><a href="#AWS-KMS-Key-Management-Service-1" class="headerlink" title="AWS KMS (Key Management Service)"></a>AWS KMS (Key Management Service)</h3><ul>
<li>Able to fully manage the keys &amp; policies:<ul>
<li>Create</li>
<li>Rotation policies</li>
<li>Disable</li>
<li>Enable</li>
</ul>
</li>
<li>Able to audit key usage (using CloudTrail)</li>
<li>Three types of Customer Master Keys (CMK):</li>
<li>AWS Managed Service Default CMK: free<ul>
<li>User Keys created in KMS: $1 &#x2F; month</li>
<li>User Keys imported (must be 256-bit symmetric key): $1 &#x2F; month</li>
<li><ul>
<li>pay for API call to KMS ($0.03 &#x2F; 10000 calls)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="AWS-KMS-101"><a href="#AWS-KMS-101" class="headerlink" title="AWS KMS 101"></a>AWS KMS 101</h3><ul>
<li>Anytime you need to share sensitive information… use KMS</li>
<li>Database passwords</li>
<li>Credentials to external service</li>
<li>Private Key of SSL certificates</li>
<li>The value in KMS is that the CMK used to encrypt data can never be
retrieved by the user, and the CMK can be rotated for extra security</li>
</ul>
<h3 id="AWS-KMS-101-1"><a href="#AWS-KMS-101-1" class="headerlink" title="AWS KMS 101"></a>AWS KMS 101</h3><ul>
<li><p><strong>Never ever store your secrets in plaintext, especially in your code!</strong></p>
</li>
<li><p>Encrypted secrets can be stored in the code &#x2F; environment variables</p>
</li>
<li><p><strong>KMS can only help in encrypting up to 4KB of data per call</strong></p>
</li>
<li><p>If data &gt; 4 KB, use envelope encryption</p>
</li>
<li><p>To give access to KMS to someone:</p>
<ul>
<li>Make sure the Key Policy allows the user</li>
<li>Make sure the IAM Policy allows the API calls</li>
</ul>
</li>
</ul>
<h3 id="KMS-Key-Policies"><a href="#KMS-Key-Policies" class="headerlink" title="KMS Key Policies"></a>KMS Key Policies</h3><ul>
<li>Control access to KMS keys, “similar” to S3 bucket policies</li>
<li>Difference: you cannot control access without them</li>
<li><strong>Default KMS Key Policy:</strong><ul>
<li>Created if you don’t provide a specific KMS Key Policy</li>
<li>Complete access to the key to the root user &#x3D; entire AWS account</li>
<li>Gives access to the IAM policies to the KMS key</li>
</ul>
</li>
<li><strong>Custom KMS Key Policy:</strong><ul>
<li>Define users, roles that can access the KMS key</li>
<li>Define who can administer the key</li>
<li>Useful for cross-account access of your KMS key</li>
</ul>
</li>
</ul>
<h3 id="Copying-Snapshots-across-accounts"><a href="#Copying-Snapshots-across-accounts" class="headerlink" title="Copying Snapshots across accounts"></a>Copying Snapshots across accounts</h3><ol>
<li>Create a Snapshot, encrypted with your own CMK</li>
<li>Attach a KMS Key Policy to authorize cross-account access</li>
<li>Share the encrypted snapshot</li>
<li>(in target) Create a copy of the Snapshot, encrypt it with a KMS Key in your account</li>
<li>Create a volume from the snapshot KMS Key Policy</li>
</ol>
<h3 id="SSM-Parameter-Store"><a href="#SSM-Parameter-Store" class="headerlink" title="SSM Parameter Store"></a>SSM Parameter Store</h3><ul>
<li>Secure storage for configuration and secrets</li>
<li>Optional Seamless Encryption using KMS</li>
<li>Serverless, scalable, durable, easy SDK</li>
<li>Version tracking of configurations &#x2F; secrets</li>
<li>Configuration management using path &amp; IAM</li>
<li>Notifications with CloudWatch Events</li>
<li>Integration with CloudFormation</li>
</ul>
<h3 id="SSM-Parameter-Store-Hierarchy"><a href="#SSM-Parameter-Store-Hierarchy" class="headerlink" title="SSM Parameter Store Hierarchy"></a>SSM Parameter Store Hierarchy</h3><h3 id="AWS-Secrets-Manager"><a href="#AWS-Secrets-Manager" class="headerlink" title="AWS Secrets Manager"></a>AWS Secrets Manager</h3><ul>
<li><p>Newer service, meant for storing secrets</p>
</li>
<li><p>Capability to <strong>force rotation</strong> of secrets every X days</p>
</li>
<li><p>Automate generation of secrets on rotation (uses Lambda)</p>
</li>
<li><p>Integration with <strong>Amazon RDS</strong> (MySQL, PostgreSQL, Aurora)</p>
</li>
<li><p>Secrets are encrypted using KMS</p>
</li>
<li><p>Mostly meant for RDS integration</p>
</li>
</ul>
<h3 id="CloudHSM"><a href="#CloudHSM" class="headerlink" title="CloudHSM"></a>CloudHSM</h3><ul>
<li>KMS &#x3D;&gt; AWS manages the software for encryption</li>
<li>CloudHSM &#x3D;&gt; AWS provisions encryption <strong>hardware</strong></li>
<li>Dedicated Hardware (HSM &#x3D; Hardware Security Module)</li>
<li>You manage your own encryption keys entirely (not AWS)</li>
<li>HSM device is tamper resistant, FIPS 140-2 Level 3 compliance</li>
<li><strong>CloudHSM clusters are spread across Multi AZ (HA) – must setup</strong></li>
<li>Supports both symmetric and <strong>asymmetric</strong> encryption (SSL&#x2F;TLS keys)</li>
<li>No free tier available</li>
<li>Must use the CloudHSM Client Software</li>
<li>Redshift supports CloudHSM for database encryption and key management</li>
<li>Good option to use with SSE-C encryption</li>
</ul>
<h3 id="AWS-Shield"><a href="#AWS-Shield" class="headerlink" title="AWS Shield"></a>AWS Shield</h3><ul>
<li><strong>AWS Shield Standard:</strong><ul>
<li>Free service that is activated for every AWS customer</li>
<li>Provides protection from attacks such as SYN&#x2F;UDP Floods, Reflection attacks
and other layer 3&#x2F;layer 4 attacks</li>
</ul>
</li>
<li><strong>AWS Shield Advanced</strong>:<ul>
<li>Optional DDoS mitigation service ($3,000 per month per organization)</li>
<li>Protect against more sophisticated attack on Amazon EC2, Elastic Load
Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, and Route 53</li>
<li>24&#x2F;7 access to AWS DDoS response team (DRP)</li>
<li>Protect against higher fees during usage spikes due to DDoS</li>
</ul>
</li>
</ul>
<h3 id="AWS-WAF-–-Web-Application-Firewall"><a href="#AWS-WAF-–-Web-Application-Firewall" class="headerlink" title="AWS WAF – Web Application Firewall"></a>AWS WAF – Web Application Firewall</h3><ul>
<li><p>Protects your web applications from common web exploits (Layer 7)</p>
</li>
<li><p><strong>Layer 7 is HTTP</strong> (vs Layer 4 is TCP)</p>
</li>
<li><p>Deploy on <strong>Application Load Balancer, API Gateway, CloudFront</strong></p>
</li>
<li><p>Define Web ACL (Web Access Control List):</p>
<ul>
<li>Rules can include: <strong>IP addresses</strong>, HTTP headers, HTTP body, or URI strings</li>
<li>Protects from common attack - <strong>SQL injection</strong> and <strong>Cross-Site Scripting (XSS)</strong></li>
<li>Size constraints, <strong>geo-match (block countries)</strong></li>
<li><strong>Rate-based rules</strong> (to count occurrences of events) – for DDoS protection</li>
</ul>
</li>
</ul>
<h3 id="AWS-Firewall-Manager"><a href="#AWS-Firewall-Manager" class="headerlink" title="AWS Firewall Manager"></a>AWS Firewall Manager</h3><ul>
<li><p>Manage rules in all accounts of an AWS Organization</p>
</li>
<li><p>Common set of security rules</p>
</li>
<li><p>WAF rules (Application Load Balancer, API Gateways, CloudFront)</p>
</li>
<li><p>AWS Shield Advanced (ALB, CLB, Elastic IP, CloudFront)</p>
</li>
<li><p>Security Groups for EC2 and ENI resources in VPC</p>
</li>
</ul>
<h3 id="AWS-Shared-Responsibility-Model"><a href="#AWS-Shared-Responsibility-Model" class="headerlink" title="AWS Shared Responsibility Model"></a>AWS Shared Responsibility Model</h3><ul>
<li>AWS responsibility - Security of the Cloud</li>
<li>Protecting infrastructure (hardware, software, facilities, and networking) that runs
all of the AWS services</li>
<li>Managed services like S3, DynamoDB, RDS etc</li>
<li>Customer responsibility - Security in the Cloud</li>
<li>For EC2 instance, customer is responsible for management of the guest OS
(including security patches and updates), firewall &amp; network configuration, IAM
etc</li>
</ul>
<h3 id="Example-for-RDS"><a href="#Example-for-RDS" class="headerlink" title="Example, for RDS"></a>Example, for RDS</h3><ul>
<li>AWS responsibility:<ul>
<li>Manage the underlying EC2 instance, disable SSH access</li>
<li>Automated DB patching</li>
<li>Automated OS patching</li>
<li>Audit the underlying instance and disks &amp; guarantee it functions</li>
</ul>
</li>
<li>Your responsibility:<ul>
<li>Check the ports &#x2F; IP &#x2F; security group inbound rules in DB’s SG</li>
<li>In-database user creation and permissions</li>
<li>Creating a database with or without public access</li>
<li>Ensure parameter groups or DB is configured to only allow SSL connections</li>
<li>Database encryption setting</li>
</ul>
</li>
</ul>
<h3 id="Example-for-S3"><a href="#Example-for-S3" class="headerlink" title="Example, for S3"></a>Example, for S3</h3><ul>
<li>AWS responsibility:<ul>
<li>Guarantee you get unlimited storage</li>
<li>Guarantee you get encryption</li>
<li>Ensure separation of the data between different customers</li>
<li>Ensure AWS employees can’t access your data</li>
</ul>
</li>
<li>Your responsibility:<ul>
<li>Bucket configuration</li>
<li>Bucket policy &#x2F; public setting</li>
<li>IAM user and roles</li>
<li>Enabling encryption</li>
</ul>
</li>
</ul>
<h3 id="Shared-Responsibility-Model-diagram"><a href="#Shared-Responsibility-Model-diagram" class="headerlink" title="Shared Responsibility Model diagram"></a>Shared Responsibility Model diagram</h3><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/compliance/shared-responsibility-model/">https://aws.amazon.com/compliance/shared-responsibility-model/</a></p>
<hr>
<p>Security &amp; Encryption Quiz</p>
<p>Question 1:
To enable encryption in flight, we need to have
A: an HTTPS endpoint with a SSL certificate</p>
<p>  encryption in flight &#x3D; HTTPS, and HTTPs cannot be enabled without an SSL certificate</p>
<p>Question 2:
Server side encryption means that the data is sent encrypted to the server first
A: False</p>
<p>  Server side encryptions means the server will encrypt the data for us. We don’t need to encrypt it beforehand</p>
<p>Question 3:
In server side encryption, only the encryption happens on the server. Where does the decryption happen?
A: The server</p>
<p>  In server side encryption, the decryption also happens on the server (in AWS, we wouldn’t be able to decrypt the data ourselves as we can’t have access to the corresponding encryption key)</p>
<p>Question 4:
In client side encryption, the server must know our encryption scheme to accept the data
A: False</p>
<p>  With client side encryption, the server does not need to know any information about the encryption being used, as the server won’t perform any encryption or decryption tasks</p>
<p>Question 5:
We need to create User Keys in KMS before using the encryption features for EBS, S3, etc…
A: False</p>
<p>  we can use the AWS Managed Service Keys in KMS, therefore we don’t need to create our own keys</p>
<p>Question 6:
We’d like our Lambda function to have access to a database password. We should
A: Have it as an encrypted environment variable and decrypt it at runtime</p>
<p>  This is the most secure solution amongst the options</p>
<p>Question 7:
We would like to audit the values of an encryption value over time
A: We should use SSM Parameter Store</p>
<p>  SSM Parameter Store has versioning and audit of values built-in directly</p>
<p>Question 8:
Under the shared responsibility model, what are you responsible for in RDS?
A: Security Group Rules</p>
<p>  This are configured by us and we’ve done that extensively in the course</p>
<p>Question 9:
Your user-facing website is a high risk target for DDoS attack and you would like to get 24&#x2F;7 support in case they happen, as well as AWS bill reimbursement for the incurred costs during the attacks. What service should you use?
A: AWS Shield Advanced</p>
<p>Question 10:
You need an encryption service that supports asymmetric encryption schemes, and you want to manage the security keys yourself. Which service could you use?
A: CloudHSM</p>
<hr>
<h2 id="Networking-VPC"><a href="#Networking-VPC" class="headerlink" title="Networking - VPC"></a>Networking - VPC</h2><h3 id="Understanding-CIDR-IPv4"><a href="#Understanding-CIDR-IPv4" class="headerlink" title="Understanding CIDR - IPv4"></a>Understanding CIDR - IPv4</h3><h4 id="Classless-Inter-Domain-Routing"><a href="#Classless-Inter-Domain-Routing" class="headerlink" title="(Classless Inter-Domain Routing)"></a>(Classless Inter-Domain Routing)</h4><ul>
<li><p>CIDR are used for <strong>Security Groups rules</strong>, or <strong>AWS networking in general</strong></p>
</li>
<li><p><strong>They help to define an IP address range</strong></p>
<ul>
<li>We’ve seen WW.XX.YY.ZZ&#x2F;32 &#x3D;&#x3D; one IP</li>
<li>We’ve seen 0.0.0.0&#x2F;0 &#x3D;&#x3D; all IPs</li>
<li>But we can define for ex: 192.168.0.0&#x2F;26: 192.168.0.0 – 192.168.0.63 (64 IP)</li>
</ul>
</li>
</ul>
<h3 id="Understanding-CIDR"><a href="#Understanding-CIDR" class="headerlink" title="Understanding CIDR"></a>Understanding CIDR</h3><ul>
<li><p>A CIDR has two components:</p>
<ul>
<li>The base IP (XX.XX.XX.XX)</li>
<li>The Subnet Mask (&#x2F;26)</li>
</ul>
</li>
<li><p>The base IP represents an IP contained in the range</p>
</li>
<li><p>The subnet masks defines how many bits can change in the IP</p>
</li>
<li><p>The subnet mask can take two forms. Examples:</p>
<ul>
<li>255.255.255.0 &lt;- less common</li>
<li>&#x2F;24           &lt;- more common</li>
</ul>
</li>
</ul>
<h3 id="Understanding-CIDRs"><a href="#Understanding-CIDRs" class="headerlink" title="Understanding CIDRs"></a>Understanding CIDRs</h3><h4 id="Subnet-Masks"><a href="#Subnet-Masks" class="headerlink" title="Subnet Masks"></a>Subnet Masks</h4><ul>
<li>The subnet masks basically allows part of the underlying IP to get additional next values from the base IP</li>
<li>&#x2F;32 allows for 1 IP &#x3D; 2^0</li>
<li>&#x2F;31 allows for 2 IP &#x3D; 2^1</li>
<li>&#x2F;30 allows for 4 IP &#x3D; 2^2</li>
<li>&#x2F;29 allows for 8 IP &#x3D; 2^3</li>
<li>&#x2F;28 allows for 16 IP &#x3D; 2^4</li>
<li>&#x2F;27 allows for 32 IP &#x3D; 2^5</li>
<li>&#x2F;26 allows for 64 IP &#x3D; 2^6</li>
<li>&#x2F;25 allows for 128 IP &#x3D; 2^7</li>
<li>&#x2F;24 allows for 256 IP &#x3D; 2^8</li>
<li>&#x2F;16 allows for 65,536 IP &#x3D; 2^16</li>
<li>&#x2F;0 allows for all IPs &#x3D; 2^32</li>
</ul>
<h3 id="Understanding-CIDRs-1"><a href="#Understanding-CIDRs-1" class="headerlink" title="Understanding CIDRs"></a>Understanding CIDRs</h3><h4 id="Little-exercise"><a href="#Little-exercise" class="headerlink" title="Little exercise"></a>Little exercise</h4><ul>
<li>192.168.0.0&#x2F;24 &#x3D; … ?</li>
<li>192.168.0.0 – 192.168.0.255 (256 IP)</li>
<li>192.168.0.0&#x2F;16 &#x3D; … ?</li>
<li>192.168.0.0 – 192.168.255.255 (65,536 IP)</li>
<li>134.56.78.123&#x2F;32 &#x3D; … ?</li>
<li>Just 134.56.78.123</li>
<li>0.0.0.0&#x2F;0</li>
<li>All IP!</li>
<li>When in doubt, use this website: <a target="_blank" rel="noopener" href="https://www.ipaddressguide.com/cidr">https://www.ipaddressguide.com/cidr</a></li>
</ul>
<h3 id="Private-vs-Public-IP-IPv4-2"><a href="#Private-vs-Public-IP-IPv4-2" class="headerlink" title="Private vs Public IP (IPv4)"></a>Private vs Public IP (IPv4)</h3><h3 id="Allowed-ranges"><a href="#Allowed-ranges" class="headerlink" title="Allowed ranges"></a>Allowed ranges</h3><ul>
<li><p>The Internet Assigned Numbers Authority (IANA) established certain
blocks of IPV4 addresses for the use of private (LAN) and public
(Internet) addresses.</p>
</li>
<li><p>Private IP can only allow certain values</p>
<ul>
<li>10.0.0.0 – 10.255.255.255 (10.0.0.0&#x2F;8) &lt;&#x3D; in big networks</li>
<li>172.16.0.0 – 172.31.255.255 (172.16.0.0&#x2F;12) &lt;&#x3D; default AWS one</li>
<li>192.168.0.0 – 192.168.255.255 (192.168.0.0&#x2F;16) &lt;&#x3D; example: home networks</li>
</ul>
</li>
<li><p>All the rest of the IP on the internet are public IP</p>
</li>
</ul>
<h3 id="Default-VPC-Walkthrough"><a href="#Default-VPC-Walkthrough" class="headerlink" title="Default VPC Walkthrough"></a>Default VPC Walkthrough</h3><ul>
<li>All new accounts have a default VPC</li>
<li>New instances are launched into default VPC if no subnet is specified</li>
<li>Default VPC have internet connectivity and all instances have public IP</li>
<li>We also get a public and a private DNS name</li>
</ul>
<h3 id="VPC-in-AWS-–-IPv4"><a href="#VPC-in-AWS-–-IPv4" class="headerlink" title="VPC in AWS – IPv4"></a>VPC in AWS – IPv4</h3><ul>
<li>VPC &#x3D; Virtual Private Cloud</li>
<li>You can have multiple VPCs in a region (max 5 per region – soft limit)<ul>
<li>Max CIDR per VPC is 5. For each CIDR:</li>
<li>Min size is &#x2F;28 &#x3D; 16 IP Addresses</li>
</ul>
</li>
<li>Max size is &#x2F;16 &#x3D; 65536 IP Addresses<ul>
<li>Because VPC is private, only the Private IP ranges are allowed:</li>
<li>10.0.0.0 – 10.255.255.255 (10.0.0.0&#x2F;8)</li>
<li>172.16.0.0 – 172.31.255.255 (172.16.0.0&#x2F;12)</li>
<li>192.168.0.0 – 192.168.255.255 (192.168.0.0&#x2F;16)</li>
</ul>
</li>
<li><strong>Your VPC CIDR should not overlap with your other networks (ex: corporate)</strong></li>
</ul>
<h3 id="Subnets-IPv4"><a href="#Subnets-IPv4" class="headerlink" title="Subnets - IPv4"></a>Subnets - IPv4</h3><ul>
<li><p>AWS reserves 5 IPs address (first 4 and last 1 IP address) in each Subnet</p>
</li>
<li><p>These 5 IPs are not available for use and cannot be assigned to an instance</p>
</li>
<li><p>Ex, if CIDR block 10.0.0.0&#x2F;24, reserved IP are:</p>
<ul>
<li>10.0.0.0: Network address</li>
<li>10.0.0.1: Reserved by AWS for the VPC router</li>
<li>10.0.0.2: Reserved by AWS for mapping to Amazon-provided DNS</li>
<li>10.0.0.3: Reserved by AWS for future use</li>
<li>10.0.0.255: Network broadcast address. AWS does not support broadcast in a VPC,
therefore the address is reserved</li>
</ul>
</li>
<li><p>Exam Tip: [*]</p>
<ul>
<li>If you need 29 IP addresses for EC2 instances, you can’t choose a Subnet of size &#x2F;27 (32 IP)</li>
<li>You need at least 64 IP, Subnet size &#x2F;26 (64-5 &#x3D; 59 &gt; 29, but 32-5 &#x3D; 27 &lt; 29)</li>
</ul>
</li>
</ul>
<h3 id="Internet-Gateways"><a href="#Internet-Gateways" class="headerlink" title="Internet Gateways"></a>Internet Gateways</h3><ul>
<li><p>Internet gateways helps our VPC instances connect with the internet</p>
</li>
<li><p>It scales horizontally and is HA (High Ability) and redundant</p>
</li>
<li><p>Must be created separately from VPC</p>
</li>
<li><p>One VPC can only be attached to one IGW and vice versa [*]</p>
</li>
<li><p>Internet Gateway is also a NAT for the instances that have a public IPv4</p>
</li>
<li><p>Internet Gateways on their own do not allow internet access…</p>
</li>
<li><p>Route tables must also be edited!</p>
</li>
</ul>
<h3 id="NAT-Instances-–-Network-Address-Translation"><a href="#NAT-Instances-–-Network-Address-Translation" class="headerlink" title="NAT Instances – Network Address Translation"></a>NAT Instances – Network Address Translation</h3><p>(outdated but still at the exam)</p>
<ul>
<li>Allows instances in the private subnets to connect to the internet</li>
<li>Must be launched in a public subnet</li>
<li>Must disable EC2 flag: Source &#x2F; Destination Check</li>
<li>Must have Elastic IP attached to it</li>
<li>Route table must be configured to route traffic from private subnets to</li>
</ul>
<h3 id="NAT-Instances-–-Comments"><a href="#NAT-Instances-–-Comments" class="headerlink" title="NAT Instances – Comments"></a>NAT Instances – Comments</h3><ul>
<li>Amazon Linux AMI pre-configured are available</li>
<li>Not highly available &#x2F; resilient setup out of the box</li>
<li>&#x3D;&gt; Would need to create ASG in multi AZ + resilient user-data script</li>
<li>Internet traffic bandwidth depends on EC2 instance performance</li>
<li>Must manage security groups &amp; rules:<ul>
<li>Inbound:<ul>
<li>Allow HTTP &#x2F; HTTPS Traffic coming from Private Subnets</li>
<li>Allow SSH from your home network (access is provided through Internet Gateway)</li>
</ul>
</li>
<li>Outbound:<ul>
<li>Allow HTTP &#x2F; HTTPS traffic to the internet</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="NAT-Gateway"><a href="#NAT-Gateway" class="headerlink" title="NAT Gateway"></a>NAT Gateway</h3><ul>
<li>AWS managed NAT, higher bandwidth, better availability, no admin</li>
<li>Pay by the hour for usage and bandwidth</li>
<li>NAT is created in a specific AZ, uses an EIP</li>
<li>Cannot be used by an instance in that subnet (only from other subnets)</li>
<li>Requires an IGW (Private Subnet &#x3D;&gt; NAT &#x3D;&gt; IGW)</li>
<li>5 Gbps of bandwidth with automatic scaling up to 45 Gbps</li>
<li>No security group to manage &#x2F; required</li>
</ul>
<h3 id="NAT-Instance-vs-Gateway"><a href="#NAT-Instance-vs-Gateway" class="headerlink" title="NAT Instance vs Gateway"></a>NAT Instance vs Gateway</h3><ul>
<li>See: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-natcomparison.html">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-natcomparison.html</a></li>
</ul>
<h3 id="DNS-Resolution-in-VPC"><a href="#DNS-Resolution-in-VPC" class="headerlink" title="DNS Resolution in VPC"></a>DNS Resolution in VPC</h3><ul>
<li><strong>enableDnsSupport: (&#x3D; DNS Resolution setting)</strong><ul>
<li>Default True</li>
<li>Helps decide if DNS resolution is supported for the VPC</li>
<li>If True, queries the AWS DNS server at 169.254.169.253</li>
</ul>
</li>
<li><strong>enableDnsHostname: (&#x3D; DNS Hostname setting)</strong><ul>
<li>False by default for newly created VPC, True by default for Default VPC</li>
<li>Won’t do anything unless enableDnsSupport&#x3D;true</li>
<li>If True, Assign public hostname to EC2 instance if it has a public</li>
</ul>
</li>
<li><strong>If you use custom DNS domain names in a private zone in Route 53, you must set both these attributes to true</strong></li>
</ul>
<p>[*]
Network ACLs &amp; Security Group
Incoming Request</p>
<h3 id="Network-ACLs"><a href="#Network-ACLs" class="headerlink" title="Network ACLs"></a>Network ACLs</h3><ul>
<li>NACL are like a firewall which control traffic from and to subnet</li>
<li>Default NACL allows everything outbound and everything inbound</li>
<li><strong>One NACL per Subnet, new Subnets are assigned the Default NACL</strong></li>
<li>Define NACL rules:<ul>
<li>Rules have a number (1-32766) and higher precedence with a lower number</li>
<li>E.g. If you define #100 ALLOW <IP> and #200 DENY <IP> , IP will be allowed</li>
<li>Last rule is an asterisk (*) and denies a request in case of no rule match</li>
<li>AWS recommends adding rules by increment of 100</li>
</ul>
</li>
<li>Newly created NACL will deny everything</li>
<li>NACL are a great way of blocking a specific IP at the subnet level</li>
</ul>
<h4 id="Network-ACLs-vs-Security-Groups"><a href="#Network-ACLs-vs-Security-Groups" class="headerlink" title="Network ACLs vs Security Groups"></a>Network ACLs vs Security Groups</h4><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html#VPC_Security_Comparison">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html#VPC_Security_Comparison</a></p>
<h4 id="Example-Network-ACL-with-Ephemeral-Ports"><a href="#Example-Network-ACL-with-Ephemeral-Ports" class="headerlink" title="Example Network ACL with Ephemeral Ports"></a>Example Network ACL with Ephemeral Ports</h4><ul>
<li><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html</a></li>
</ul>
<h3 id="VPC-Peering"><a href="#VPC-Peering" class="headerlink" title="VPC Peering [*]"></a>VPC Peering [*]</h3><ul>
<li>Connect two VPC, privately using AWS’ network</li>
<li>Make them behave as if they were in the same network</li>
<li>Must not have overlapping CIDR</li>
<li>VPC Peering connection is not <strong>transitive</strong> (must be established for each VPC that need to communicate with one another) [*]</li>
<li>You can do VPC peering with another AWS account</li>
<li><strong>You must update route tables in <strong>each VPC’s subnets</strong> to ensure instances can communicate</strong> [*]</li>
</ul>
<h3 id="VPC-Peering-–-Good-to-know"><a href="#VPC-Peering-–-Good-to-know" class="headerlink" title="VPC Peering – Good to know"></a>VPC Peering – Good to know</h3><ul>
<li>VPC peering can work <strong>inter-region, cross-account</strong></li>
<li>You can reference a security group of a peered VPC (works cross account)</li>
</ul>
<h3 id="VPC-Endpoints"><a href="#VPC-Endpoints" class="headerlink" title="VPC Endpoints"></a>VPC Endpoints</h3><ul>
<li>Endpoints allow you to connect to AWS Services using a private network
instead of the public www network</li>
<li>They scale horizontally and are redundant</li>
<li>They remove the need of IGW, NAT, etc… to access AWS Services</li>
<li><strong><strong>Interface</strong></strong>: provisions an ENI (private IP address) as an entry point (must
attach security group) – most AWS services</li>
<li><strong><strong>Gateway</strong></strong>: provisions a target and must be used in a route table – S3 and
DynamoDB</li>
<li>In case of issues:<ul>
<li>Check DNS Setting Resolution in your VPC</li>
<li>Check Route Tables</li>
</ul>
</li>
</ul>
<h3 id="Flow-Logs"><a href="#Flow-Logs" class="headerlink" title="Flow Logs"></a>Flow Logs</h3><ul>
<li>Capture information about IP traffic going into your interfaces:<ul>
<li><strong>VPC Flow Logs</strong></li>
<li><strong>Subnet Flow Logs</strong></li>
<li><strong>Elastic Network Interface Flow Logs</strong></li>
</ul>
</li>
<li>Helps to monitor &amp; troubleshoot connectivity issues</li>
<li>Flow logs data can go to S3 &#x2F; CloudWatch Logs</li>
<li>Captures network information from AWS managed interfaces too: ELB,
RDS, ElastiCache, Redshift, WorkSpaces</li>
</ul>
<p>[*] To use private DNS names, ensure that the attributes ‘Enable DNS hostnames’ and ‘Enable DNS Support’ are set to ‘true’ for your VPC (vpc-f4858893). Learn more.</p>
<h3 id="Flow-Log-Syntax"><a href="#Flow-Log-Syntax" class="headerlink" title="Flow Log Syntax"></a>Flow Log Syntax</h3><ul>
<li><p><version> <account-id> <interface-id> <srcaddr> <dstaddr> <srcport>
<dstport> <protocol> <packets> <bytes> <start> <end> <action> <log- status></p>
</li>
<li><p>Srcaddr, dstaddr help identify problematic IP</p>
</li>
<li><p>Srcport, dstport help identity problematic ports</p>
</li>
<li><p>Action : success or failure of the request due to Security Group &#x2F; NACL</p>
</li>
<li><p>Can be used for analytics on usage patterns, or malicious behavior</p>
</li>
<li><p>Flow logs example: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html#flow-log-records">https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html#flow-log-records</a></p>
</li>
<li><p><strong>Query VPC flow logs using Athena on S3 or CloudWatch Logs Insights</strong></p>
</li>
</ul>
<h3 id="Bastion-Hosts"><a href="#Bastion-Hosts" class="headerlink" title="Bastion Hosts"></a>Bastion Hosts</h3><ul>
<li><p>We can use a Bastion Host to SSH into our private instances</p>
</li>
<li><p>The bastion is in the public subnet which is then connected to all other private
subnets</p>
</li>
<li><p><strong>Bastion Host security group must be tightened</strong></p>
</li>
<li><p>Exam Tip: Make sure the bastion host only has port 22 traffic from the IP you need, not from the security groups of your other instances [*]</p>
</li>
</ul>
<h3 id="Site-to-Site-VPN"><a href="#Site-to-Site-VPN" class="headerlink" title="Site to Site VPN"></a>Site to Site VPN</h3><ul>
<li>Virtual Private Gateway:<ul>
<li>VPN concentrator on the AWS side of the VPN connection</li>
<li>VGW is created and attached to the VPC from which you want to create the Site-toSite VPN connection</li>
<li>Possibility to customize the ASN</li>
</ul>
</li>
<li>Customer Gateway:<ul>
<li>Software application or physical device on customer side of the VPN connection</li>
<li><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/vpc/latest/adminguide/Introduction.html#DevicesTested">https://docs.aws.amazon.com/vpc/latest/adminguide/Introduction.html#DevicesTested</a></li>
<li>IP Address: [*]<ul>
<li>Use static, internet-routable IP address for your customer gateway device.</li>
<li>If behind a CGW behind NAT (with NAT-T), use the public IP address of the NAT</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Direct-Connect"><a href="#Direct-Connect" class="headerlink" title="Direct Connect"></a>Direct Connect</h3><ul>
<li>Provides a dedicated <strong><strong>private</strong></strong> connection from a remote network to your VPC</li>
<li>Dedicated connection must be setup between your DC and AWS Direct
Connect locations</li>
<li>You need to setup a Virtual Private Gateway on your VPC</li>
<li>Access public resources (S3) and private (EC2) on same connection</li>
<li>Use Cases:<ul>
<li>Increase bandwidth throughput - working with large data sets – lower cost</li>
<li>More consistent network experience - applications using real-time data feeds</li>
<li>Hybrid Environments (on prem + cloud)</li>
</ul>
</li>
<li>Supports both IPv4 and IPv6</li>
</ul>
<p>Direct Connect Gateway</p>
<ul>
<li>If you want to setup a Direct Connect to one or more VPC in many
different regions (same account), you must use a Direct Connect Gateway</li>
</ul>
<p>Direct Connect – Connection Types</p>
<ul>
<li>Dedicated Connections: 1Gbps and 10 Gbps capacity</li>
<li>Physical ethernet port dedicated to a customer</li>
<li>Request made to AWS first, then completed by AWS Direct Connect Partners</li>
<li>Hosted Connections: 50Mbps, 500 Mbps, to 10 Gbps</li>
<li>Connection requests are made via AWS Direct Connect Partners</li>
<li>Capacity can be added or removed on demand</li>
<li>1, 2, 5, 10 Gbps available at select AWS Direct Connect Partners</li>
<li>Lead times are often longer than 1 month to establish a new connection</li>
</ul>
<h3 id="Direct-Connect-1"><a href="#Direct-Connect-1" class="headerlink" title="Direct Connect"></a>Direct Connect</h3><p>– Encryption</p>
<ul>
<li>Data in transit is not encrypted but is
private</li>
<li>AWS Direct Connect + VPN
provides an IPsec
-encrypted private
connection</li>
<li>Good for an extra level of security,
but slightly more complex to put in
place</li>
</ul>
<h3 id="Egress-Only-Internet-Gateway"><a href="#Egress-Only-Internet-Gateway" class="headerlink" title="Egress Only Internet Gateway"></a>Egress Only Internet Gateway</h3><ul>
<li>Egress only Internet Gateway is for IPv6 only</li>
<li>Similar function as a NAT, but a NAT is for IPv4</li>
<li>Good to know: IPv6 are all public addresses</li>
<li>Therefore all our instances with IPv6 are publicly accessibly</li>
<li>Egress Only Internet Gateway gives our IPv6 instances access to the
internet, but they won’t be directly reachable by the internet</li>
<li>After creating an Egress Only Internet Gateway, edit the route tables</li>
</ul>
<p>[SAA-C02]</p>
<h2 id="AWS-PrivateLink-VPC-Endpoint-Services"><a href="#AWS-PrivateLink-VPC-Endpoint-Services" class="headerlink" title="AWS PrivateLink (VPC Endpoint Services) [*]"></a>AWS PrivateLink (VPC Endpoint Services) [*]</h2><ul>
<li>Most secure &amp; scalable way to expose a service to 1000s of VPC (own or other accounts)</li>
<li>Does not require VPC peering, internet gateway, NAT, route tables…</li>
<li>Requires a network load balancer (Service VPC) and ENI (Customer VPC)</li>
<li>If the NLB is in multiple AZ, and the ENI in multiple AZ, the solution is fault tolerant!</li>
</ul>
<p>[SAA-C02]</p>
<h3 id="EC2-Classic-amp-AWS-ClassicLink-deprecated"><a href="#EC2-Classic-amp-AWS-ClassicLink-deprecated" class="headerlink" title="EC2-Classic &amp; AWS ClassicLink (deprecated)"></a>EC2-Classic &amp; AWS ClassicLink (deprecated)</h3><ul>
<li><p>EC2-Classic: instances run in a single network shared with other customers</p>
</li>
<li><p>Amazon VPC: your instances run logically isolated to your AWS account</p>
</li>
<li><p>ClassicLink allows you to link EC2-Classic instances to a VPC in your account</p>
<ul>
<li>Must associate a security group</li>
<li>Enables communication using private IPv4 addresses</li>
<li>Removes the need to make use of public IPv4 addresses or Elastic IP addresses</li>
</ul>
</li>
<li><p>Likely to be distractors at the exam</p>
</li>
</ul>
<p>[SAA-C02]</p>
<h2 id="AWS-VPN-CloudHub"><a href="#AWS-VPN-CloudHub" class="headerlink" title="AWS VPN CloudHub"></a>AWS VPN CloudHub</h2><ul>
<li><p>Provide secure communication between sites, if you have multiple VPN connections</p>
</li>
<li><p>Low cost hub-and-spoke model for primary or secondary network connectivity between locations</p>
</li>
<li><p>It’s a VPN connection so it goes over the public internet</p>
</li>
</ul>
<p>[SAA-C02]</p>
<h3 id="Transit-Gateway"><a href="#Transit-Gateway" class="headerlink" title="Transit Gateway"></a>Transit Gateway</h3><ul>
<li><strong>For having transitive peering between thousands of VPC and on-premises, hub-and-spoke (star) connection</strong></li>
<li>Regional resource, can work cross -region</li>
<li>Share cross -account using Resource Access Manager (RAM)</li>
<li>You can peer Transit Gateways across regions</li>
<li>Route Tables: limit which VPC can talk with other VPC</li>
<li>Works with Direct Connect Gateway, VPN connections</li>
<li>Supports <strong>IP Multicast</strong> (not supported by any other AWS service)</li>
</ul>
<h2 id="VPC-Section-Summary-1-x2F-3"><a href="#VPC-Section-Summary-1-x2F-3" class="headerlink" title="VPC Section Summary (1&#x2F;3)"></a>VPC Section Summary (1&#x2F;3)</h2><ul>
<li>CIDR: IP Range</li>
<li>VPC: Virtual Private Cloud &#x3D;&gt; we define a list of IPv4 &amp; IPv6 CIDR</li>
<li>Subnets:Tied to an AZ, we define a CIDR</li>
<li>Internet Gateway: at the VPC level, provide IPv4 &amp; IPv6 Internet Access</li>
<li>Route Tables: must be edited to add routes from subnets to the IGW, VPC Peering Connections, VPC Endpoints, etc…</li>
<li>NAT Instances: gives internet access to instances in private subnets. Old, must be setup in a public subnet, disable Source &#x2F; Destination check flag</li>
<li>NAT Gateway: managed by AWS, provides scalable internet access to private instances, IPv4 only</li>
<li>Private DNS + Route 53: enable DNS Resolution + DNS hostnames (VPC)</li>
<li>NACL: Stateless, subnet rules for inbound and outbound, don’t forget ephemeral ports</li>
<li>Security Groups: Stateful, operate at the EC2 instance level</li>
</ul>
<h2 id="VPC-Section-Summary-2-x2F-3"><a href="#VPC-Section-Summary-2-x2F-3" class="headerlink" title="VPC Section Summary (2&#x2F;3)"></a>VPC Section Summary (2&#x2F;3)</h2><ul>
<li>VPC Peering: Connect two VPC with non overlapping CIDR, non transitive</li>
<li>VPC Endpoints: Provide private access to AWS Services (S3, DynamoDB, CloudFormation, SSM) within VPC</li>
<li>VPC Flow Logs: Can be setup at the VPC &#x2F; Subnet &#x2F; ENI Level, for ACCEPT and REJECT traffic, helps identifying attacks, analyze using Athena or CloudWatch Log Insights</li>
<li>Bastion Host: Public instance to SSH into, that has SSH connectivity to instances in private subnets</li>
<li>Site to Site VPN: setup a Customer Gateway on DC, a Virtual Private Gateway on VPC, and site-to-site VPN over public internet</li>
<li>Direct Connect: setup a Virtual Private Gateway on VPC, and establish a direct private connection to an AWS Direct Connect Location</li>
<li>Direct Connect Gateway: setup a Direct Connect to many VPC in different regions</li>
<li>Internet Gateway Egress: like a NAT Gateway, but for IPv6</li>
</ul>
<h2 id="VPC-Section-Summary-3-x2F-3"><a href="#VPC-Section-Summary-3-x2F-3" class="headerlink" title="VPC Section Summary (3&#x2F;3)"></a>VPC Section Summary (3&#x2F;3)</h2><ul>
<li>Private Link &#x2F; VPC Endpoint Services:</li>
<li>connect services privately from your service VPC to customers VPC</li>
<li>Doesn’t need VPC peering, public internet, NAT gateway, route tables</li>
<li>Must be used with Network Load Balancer &amp; ENI</li>
<li>ClassicLink: connect EC2-Classic instances privately to your VPC</li>
<li>VPN CloudHub: hub-and-spoke VPN model to connect your sites</li>
<li>Transit Gateway: transitive peering connections for VPC, VPN &amp; DX</li>
</ul>
<hr>
<p>VPC Quiz
Question 1:
What does this CIDR correspond to?
10.0.4.0&#x2F;28
A: 10.0.4.0 TO 10.0.4.15</p>
<p>  &#x2F;28 means 16 IPs (&#x3D;2^(32-28) &#x3D; 2^4), means only the last digit can change.</p>
<p>Question 2:
You have a corporate network of size 10.0.0.0&#x2F;8  and a satellite office of size 192.168.0.0&#x2F;16. Which CIDR is acceptable for your AWS VPC if you plan on connecting your networks later on?
a: 172.16.0.0&#x2F;16
  X: Lecture 222
  CIDR not should overlap, and the max CIDR size in AWS is &#x2F;16</p>
<p>Question 3:
You plan on creating a subnet and want it to have at least capacity for 28 EC2 instances. What’s the minimum size you need to have for your subnet?
A: &#x2F;26</p>
<p>  perfect size (64 IP)</p>
<p>Question 4:
You have set up an internet gateway in your VPC, but your EC2 instances still don’t have access to the internet. What is NOT a possible issue?
A: The security group does not allow network in.
  X: 228
  security groups are stateful and if traffic can go out, then it can go back in</p>
<p>Question 5:
You would like to provide internet access to your instances in private subnets with IPv4, while making sure this solution requires the least amount of administration and scales seamlessly. What should you use?
A: NAT Gateway</p>
<p>Question 6:
VPC Peering has been enabled between VPC A and VPC B, and the route tables have been updated for VPC A. Still, your instances cannot communicate. What is the likely issue?
A: Check the route tables in VPC B</p>
<p>  Route tables must be updated in both VPC that are peered</p>
<p>Question 7:
You have set-up a direct connection between your Corporate Data Center and your VPC A. You need to access VPC B in another region from your Corporate Data Center as well. What should you do?
A: Use a Direct Connect Gateway</p>
<p>  This is the main use case of Direct Connect Gateways</p>
<p>Question 8:
Which are the only two services that have a Gateway Endpoint instead of an Interface Endpoint as a VPC endpoint?
A: Amazon S3 &amp; DynamoDB</p>
<p>  these two services have a Gateway endpoint (remember it), all the other ones have an interface endpoint (powered by Private Link - means a private IP)</p>
<p>Question 9:
Your company has created a REST API that it will sell to hundreds of customers as a SaaS. Your customers are on AWS and are using their own VPC. You would like to allow your customers to access your SaaS without going through the public internet while ensuring your infrastructure is not left exposed to network attacks. What do you recommend?
A: AWS PrivateLink</p>
<p>Question 10:
Your company has several on-premise sites across the USA. These sites are currently linked using a private connection, but your private connection provider has been recently quite unstable, making your IT architecture partially offline. You would like to create a backup connection that will use the public internet to link your on-premise sites, that you can failover in case of issues with your provider. What do you recommend?
A: VPN CloudHub</p>
<hr>
<h3 id="Networking-Costs-in-AWS-per-GB-Simplified"><a href="#Networking-Costs-in-AWS-per-GB-Simplified" class="headerlink" title="Networking Costs in AWS per GB - Simplified"></a>Networking Costs in AWS per GB - Simplified</h3><ul>
<li>Use Private IP instead of Public IP for good savings and better network performance</li>
<li>Use same AZ for maximum savings (at the cost of high availability)</li>
</ul>
<h2 id="Disaster-Recovery-Overview"><a href="#Disaster-Recovery-Overview" class="headerlink" title="Disaster Recovery Overview"></a>Disaster Recovery Overview</h2><ul>
<li>Any event that has a negative impact on a company’s business continuity or finances is a disaster</li>
<li>Disaster recovery (DR) is about preparing for and recovering from a disaster</li>
<li>What kind of disaster recovery?</li>
<li>On-premise &#x3D;&gt; On-premise: traditional DR, and very expensive</li>
<li>On-premise &#x3D;&gt; AWS Cloud: hybrid recovery</li>
<li>AWS Cloud Region A &#x3D;&gt; AWS Cloud Region B</li>
<li>Need to define two terms:</li>
<li>RPO: Recovery Point Objective</li>
<li>RTO: Recovery Time Objective</li>
</ul>
<h3 id="Disaster-Recovery-Strategies"><a href="#Disaster-Recovery-Strategies" class="headerlink" title="Disaster Recovery Strategies"></a>Disaster Recovery Strategies</h3><ul>
<li>Backup and Restore</li>
<li>Pilot Light</li>
<li>Warm Standby</li>
<li>Hot Site &#x2F; Multi Site Approach</li>
</ul>
<h3 id="Disaster-Recovery-Tips"><a href="#Disaster-Recovery-Tips" class="headerlink" title="Disaster Recovery Tips"></a>Disaster Recovery Tips</h3><ul>
<li><strong>Backup</strong><ul>
<li>EBS Snapshots, RDS automated backups &#x2F; Snapshots, etc…</li>
<li>Regular pushes to S3 &#x2F; S3 IA &#x2F; Glacier, Lifecycle Policy, Cross Region Replication</li>
<li>From On-Premise: Snowball or Storage Gateway</li>
</ul>
</li>
<li><strong>High Availability</strong><ul>
<li>Use Route53 to migrate DNS over from Region to Region</li>
<li>RDS Multi-AZ, ElastiCache Multi-AZ, EFS, S3</li>
<li>Site to Site VPN as a recovery from Direct Connect</li>
</ul>
</li>
<li><strong>Replication</strong><ul>
<li>RDS Replication (Cross Region), AWS Aurora + Global Databases</li>
<li>Database replication from on-premise to RDS</li>
<li>Storage Gateway</li>
</ul>
</li>
<li><strong>Automation</strong><ul>
<li>CloudFormation &#x2F; Elastic Beanstalk to re-create a whole new environment</li>
<li>Recover &#x2F; Reboot EC2 instances with CloudWatch if alarms fail</li>
<li>AWS Lambda functions for customized automations</li>
</ul>
</li>
<li><strong>Chaos</strong><ul>
<li>Netflix has a “simian-army” randomly terminating EC2</li>
</ul>
</li>
</ul>
<h3 id="DMS-–-Database-Migration-Service"><a href="#DMS-–-Database-Migration-Service" class="headerlink" title="DMS – Database Migration Service"></a>DMS – Database Migration Service</h3><ul>
<li>Quickly and securely migrate databases to AWS, resilient, self healing</li>
<li>The source database remains available during the migration</li>
<li>Supports:<ul>
<li>Homogeneous migrations: ex Oracle to Oracle</li>
<li>Heterogeneous migrations: ex Microsoft SQL Server to Aurora</li>
</ul>
</li>
<li>Continuous Data Replication using CDC</li>
<li>You must create an EC2 instance to perform the replication tasks</li>
</ul>
<h3 id="DMS-Sources-and-Targets"><a href="#DMS-Sources-and-Targets" class="headerlink" title="DMS Sources and Targets"></a>DMS Sources and Targets</h3><p>SOURCES:</p>
<ul>
<li>On-Premise and EC2 instances databases: Oracle, MS SQL Server,
MySQL, MariaDB, PostgreSQL, MongoDB, SAP, DB2</li>
<li>Azure: Azure SQL Database</li>
<li>Amazon RDS: all including Aurora</li>
<li>Amazon S3
TARGETS:</li>
<li>On-Premise and EC2 instances databases: Oracle, MS SQL Server, MySQL, MariaDB, PostgreSQL, SAP</li>
<li>Amazon RDS</li>
<li>Amazon Redshift</li>
<li>Amazon DynamoDB</li>
<li>Amazon S3</li>
<li>ElasticSearch Service</li>
<li>Kinesis Data Streams</li>
<li>DocumentDB</li>
</ul>
<p>[SAA-C02]</p>
<h3 id="AWS-Schema-Conversion-Tool-SCT"><a href="#AWS-Schema-Conversion-Tool-SCT" class="headerlink" title="AWS Schema Conversion Tool (SCT)"></a>AWS Schema Conversion Tool (SCT)</h3><ul>
<li>Convert your Database’s Schema from one engine to another</li>
<li>Example OLTP: (SQL Server or Oracle) to MySQL, PostgreSQL, Aurora</li>
<li>Example OLAP: (Teradata or Oracle) to Amazon Redshift</li>
<li>You do not need to use SCT if you are migrating the same DB engine<ul>
<li>Ex: On-Premise PostgreSQL &#x3D;&gt; RDS PostgreSQL</li>
<li>The DB engine is still PostgreSQL (RDS is the platform)</li>
</ul>
</li>
</ul>
<p>[SAA-C02]</p>
<h3 id="On-Premise-strategy-with-AWS"><a href="#On-Premise-strategy-with-AWS" class="headerlink" title="On-Premise strategy with AWS"></a>On-Premise strategy with AWS</h3><ul>
<li>Ability to download Amazon Linux 2 AMI as a VM (.iso format)<ul>
<li>VMWare, KVM, VirtualBox (Oracle VM), Microsoft Hyper-V</li>
</ul>
</li>
<li>VM Import &#x2F; Export<ul>
<li>Migrate existing applications into EC2</li>
<li>Create a DR repository strategy for your on-premise VMs</li>
<li>Can export back the VMs from EC2 to on-premise</li>
</ul>
</li>
<li>AWS Application Discovery Service<ul>
<li>Gather information about your on-premise servers to plan a migration</li>
<li>Server utilization and dependency mappings</li>
<li>Track with AWS Migration Hub</li>
</ul>
</li>
<li>AWS Database Migration Service (DMS)<ul>
<li>replicate On-premise &#x3D;&gt; AWS , AWS &#x3D;&gt; AWS, AWS &#x3D;&gt; On-premise</li>
<li>Works with various database technologies (Oracle, MySQL, DynamoDB, etc..)</li>
</ul>
</li>
<li>AWS Server Migration Service (SMS)<ul>
<li>Incremental replication of on-premise live servers to AWS</li>
</ul>
</li>
</ul>
<p>[SAA-C02]</p>
<h3 id="AWS-DataSync"><a href="#AWS-DataSync" class="headerlink" title="AWS DataSync"></a>AWS DataSync</h3><ul>
<li>Move large amount of data from on- premise to AWS</li>
<li>Can synchronize to: <strong>Amazon S3, Amazon EFS, Amazon FSx for Windows</strong></li>
<li>Move data from your NAS or file system via <strong>NFS</strong> or <strong>SMB</strong></li>
<li>Replication tasks can be scheduled
hourly, daily, weekly</li>
<li>Leverage the DataSync agent to
connect to your systems
<a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/datasync/latest/userguide/how-datasync-works.html">https://docs.aws.amazon.com/datasync/latest/userguide/how-datasync-works.html</a>
NFS &#x2F; SMB to AWS (S3, EFS, FSx for Windows)
EFS to EFS</li>
</ul>
<p>[SAA-C02]</p>
<h3 id="Transferring-large-amount-of-data-into-AWS"><a href="#Transferring-large-amount-of-data-into-AWS" class="headerlink" title="Transferring large amount of data into AWS"></a>Transferring large amount of data into AWS</h3><ul>
<li>Example: transfer 200 TB of data in the cloud. We have a 100 Mbps internet
connection.</li>
<li>Over the internet &#x2F; Site-to-Site VPN:<ul>
<li>Immediate to setup</li>
<li>Will take 200(TB)*1000(GB)*1000(MB)*8(Mb)&#x2F;100 Mbps &#x3D; 16,000,000s &#x3D; 185d</li>
</ul>
</li>
<li>Over direct connect 1Gbps:<ul>
<li>Long for the one-time setup (over a month)</li>
<li>Will take 200(TB)*1000(GB)*8(Gb)&#x2F;1 Gbps &#x3D; 1,600,000s &#x3D; 18.5d</li>
</ul>
</li>
<li>Over Snowball:<ul>
<li>Will take 2 to 3 snowballs in parallel</li>
<li>Takes about 1 week for the end-to-end transfer</li>
<li>Can be combined with DMS</li>
</ul>
</li>
<li>For on-going replication &#x2F; transfers: Site-to-Site VPN or DX with DMS or DataSync</li>
</ul>
<hr>
<p>Disaster Recovery Quiz</p>
<p>Question 1:
As part of your disaster recovery strategy, you would like to have only the critical systems up and running in AWS. You don’t mind a longer RTO. Which DR strategy do you recommend?
A: Pilot Light</p>
<p>  If you’re interested into reading more about disaster recovery, the whitepaper is here: <a target="_blank" rel="noopener" href="https://d1.awsstatic.com/asset-repository/products/CloudEndure/CloudEndure_Affordable_Enterprise-Grade_Disaster_Recovery_Using_AWS.pdf">https://d1.awsstatic.com/asset-repository/products/CloudEndure/CloudEndure_Affordable_Enterprise-Grade_Disaster_Recovery_Using_AWS.pdf</a></p>
<p>Question 2:
You would like to get the DR strategy with the lowest RTO and RPO, regardless of the cost, which one do you recommend?
A: Multi Site</p>
<p>  If you’re interested into reading more about disaster recovery, the whitepaper is here: <a target="_blank" rel="noopener" href="https://d1.awsstatic.com/asset-repository/products/CloudEndure/CloudEndure_Affordable_Enterprise-Grade_Disaster_Recovery_Using_AWS.pdf">https://d1.awsstatic.com/asset-repository/products/CloudEndure/CloudEndure_Affordable_Enterprise-Grade_Disaster_Recovery_Using_AWS.pdf</a></p>
<p>Question 3:
Which of the following strategies has a potentially high RPO and RTO?
A: Backup and Restore</p>
<p>  If you’re interested into reading more about disaster recovery, the whitepaper is here: <a target="_blank" rel="noopener" href="https://d1.awsstatic.com/asset-repository/products/CloudEndure/CloudEndure_Affordable_Enterprise-Grade_Disaster_Recovery_Using_AWS.pdf">https://d1.awsstatic.com/asset-repository/products/CloudEndure/CloudEndure_Affordable_Enterprise-Grade_Disaster_Recovery_Using_AWS.pdf</a></p>
<hr>
<h2 id="Section-23-More-Solution-Architectures"><a href="#Section-23-More-Solution-Architectures" class="headerlink" title="Section 23: More Solution Architectures"></a>Section 23: More Solution Architectures</h2><h3 id="Extra-Solution-Architecture-discussions-249"><a href="#Extra-Solution-Architecture-discussions-249" class="headerlink" title="Extra Solution Architecture discussions - 249"></a>Extra Solution Architecture discussions - 249</h3><p>[SAA-C02]</p>
<h3 id="S3-Events"><a href="#S3-Events" class="headerlink" title="S3 Events"></a>S3 Events</h3><ul>
<li>S3:ObjectCreated, S3:ObjectRemoved,
S3:ObjectRestore, S3:Replication…</li>
<li>Object name filtering possible (*.jpg)</li>
<li>Use case: generate thumbnails of images uploaded to S3</li>
<li>Can create as many “S3 events” as desired</li>
<li>S3 event notifications typically deliver events in seconds but can sometimes take a minute or longer</li>
<li>If two writes are made to a single non- versioned object at the same time, it is
possible that only a single event notification will be sent</li>
<li>If you want to ensure that an event notification is sent for every successful write, you can enable versioning on your bucket</li>
</ul>
<p>[SAA-C02]</p>
<h3 id="High-Performance-Computing-HPC"><a href="#High-Performance-Computing-HPC" class="headerlink" title="High Performance Computing (HPC)"></a>High Performance Computing (HPC)</h3><ul>
<li><p>The cloud is the perfect place to perform HPC</p>
</li>
<li><p>You can create a very high number of resources in no time</p>
</li>
<li><p>You can speed up time to results by adding more resources</p>
</li>
<li><p>You can pay only for the systems you have used</p>
</li>
<li><p>Perform genomics, computational chemistry, financial risk modeling, weather prediction, machine learning, deep learning, autonomous driving</p>
</li>
<li><p>Which services help perform HPC?</p>
</li>
</ul>
<h3 id="Data-Management-amp-Transfer"><a href="#Data-Management-amp-Transfer" class="headerlink" title="Data Management &amp; Transfer"></a>Data Management &amp; Transfer</h3><ul>
<li><strong>AWS Direct Connect</strong>:<ul>
<li>Move GB&#x2F;s of data to the cloud, over a private secure network</li>
</ul>
</li>
<li><strong>Snowball &amp; Snowmobile</strong><ul>
<li>Move PB of data to the cloud</li>
</ul>
</li>
<li><strong>AWS DataSync</strong><ul>
<li>Move large amount of data between on-premise and S3, EFS, FSx for Windows</li>
</ul>
</li>
</ul>
<p>[SAA-C02][*]</p>
<h3 id="Compute-and-Networking"><a href="#Compute-and-Networking" class="headerlink" title="Compute and Networking"></a>Compute and Networking</h3><ul>
<li><p>EC2 Enhanced Networking (SR-IOV)</p>
<ul>
<li>Higher bandwidth, higher PPS (packet per second), lower latency</li>
<li>Option 1: Elastic Network Adapter (ENA) up to 100 Gbps</li>
<li>Option 2: Intel 82599 VF up to 10 Gbps – LEGACY</li>
</ul>
</li>
<li><p><strong>Elastic Fabric Adapter</strong> (EFA)</p>
<ul>
<li>Improved ENA for HPC, only works for Linux</li>
<li>Great for inter-node communications, tightly coupled workloads</li>
<li>Leverages Message Passing Interface (MPI) standard</li>
<li>Bypasses the underlying Linux OS to provide low-latency, reliable transport</li>
</ul>
</li>
</ul>
<p>[SAA-C02]</p>
<h3 id="Storage"><a href="#Storage" class="headerlink" title="Storage"></a>Storage</h3><ul>
<li>Instance-attached storage:<ul>
<li>EBS: scale up to 64000 IOPS with io1 Provisioned IOPS</li>
<li>Instance Store: scale to millions of IOPS, linked to EC2 instance, low latency</li>
</ul>
</li>
<li>Network storage:<ul>
<li>Amazon S3: large blob, not a file system</li>
<li>Amazon EFS: scale IOPS based on total size, or use provisioned IOPS</li>
<li>Amazon FSx for Lustre:<ul>
<li>HPC optimized distributed file system, millions of IOPS</li>
<li>Backed by S3</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>[SAA-C02]</p>
<h3 id="Automation-and-Orchestration"><a href="#Automation-and-Orchestration" class="headerlink" title="Automation and Orchestration"></a>Automation and Orchestration</h3><ul>
<li><p>AWS Batch</p>
<ul>
<li><strong>AWS Batch</strong> supports multi-node parallel jobs, which enables you to run single
jobs that span multiple <strong>EC2</strong> instances.</li>
<li>Easily schedule jobs and launch EC2 instances accordingly</li>
</ul>
</li>
<li><p>AWS ParallelCluster</p>
<ul>
<li>Open source cluster management tool to deploy HPC on AWS</li>
<li>Configure with text files</li>
<li>Automate creation of VPC, Subnet, cluster type and instance types</li>
</ul>
</li>
</ul>
<hr>
<p>More Solution Architectures - Quiz
Quiz 22</p>
<p>Question 1:
Your Lambda function is processing events coming through S3 events and distributed through an SNS topic. You have decided to ensure that events that can not be processed are sent to a DLQ. In which service should you set up the DLQ?
A: Lambda function</p>
<p>  the invocation is asynchronous (coming from the SNS topic) so the DLQ has to be set on the Lambda side</p>
<p>Question 2:
You have created an architecture including CloudFront with WAF, Shield, an ALB, and EC2 instances. You would like to block an IP, where should you do it?
A: WAF</p>
<p>Question 3:
Your instances are deployed in an EC2 placement group of type cluster in order to perform HPC. You would like to maximize network performance between your instances. What should you use?
A: Elastic Fabric Adapar</p>
<hr>
<p>Section 24: Other Services</p>
<h3 id="Continuous-Integration"><a href="#Continuous-Integration" class="headerlink" title="Continuous Integration"></a>Continuous Integration</h3><ul>
<li>Developers push the code to a code repository often (GitHub &#x2F; CodeCommit &#x2F;
Bitbucket &#x2F; etc…)</li>
<li>A testing &#x2F; build server checks the code as soon as it’s pushed (CodeBuild &#x2F; Jenkins CI &#x2F; etc…)</li>
<li>The developer gets feedback about the tests and checks that have passed &#x2F; failed</li>
<li>Find bugs early, fix bugs</li>
<li>Deliver faster as the code is tested</li>
<li>Deploy often</li>
<li>Happier developers, as they’re unblocked</li>
</ul>
<h3 id="Continuous-Delivery"><a href="#Continuous-Delivery" class="headerlink" title="Continuous Delivery"></a>Continuous Delivery</h3><ul>
<li>Ensure that the software can be released reliably whenever needed.</li>
<li>Ensures deployments happen often and are quick</li>
<li>Shift away from “one release every 3 months” to ”5 releases a day”</li>
<li>That usually means automated deployment<ul>
<li>CodeDeploy</li>
<li>Jenkins CD</li>
<li>Spinnaker</li>
<li>Etc…</li>
</ul>
</li>
</ul>
<h3 id="Infrastructure-as-Code"><a href="#Infrastructure-as-Code" class="headerlink" title="Infrastructure as Code"></a>Infrastructure as Code</h3><ul>
<li>Currently, we have been doing a lot of manual work</li>
<li>All this manual work will be very tough to reproduce:</li>
<li>In another region</li>
<li>in another AWS account</li>
<li>Within the same region if everything was deleted</li>
<li>Wouldn’t it be great, if all our infrastructure was… code?</li>
<li>That code would be deployed and create &#x2F; update &#x2F; delete our
infrastructure</li>
</ul>
<h3 id="What-is-CloudFormation"><a href="#What-is-CloudFormation" class="headerlink" title="What is CloudFormation"></a>What is CloudFormation</h3><ul>
<li><p>CloudFormation is a declarative way of outlining your AWS
Infrastructure, for any resources (most of them are supported).</p>
</li>
<li><p>For example, within a CloudFormation template, you say:</p>
<ul>
<li>I want a security group</li>
<li>I want two EC2 machines using this security group</li>
<li>I want two Elastic IPs for these EC2 machines</li>
<li>I want an S3 bucket</li>
<li>I want a load balancer (ELB) in front of these machines</li>
</ul>
</li>
<li><p>Then CloudFormation creates those for you, in the right order, with the
exact configuration that you specify</p>
</li>
</ul>
<h3 id="Benefits-of-AWS-CloudFormation-1-x2F-2"><a href="#Benefits-of-AWS-CloudFormation-1-x2F-2" class="headerlink" title="Benefits of AWS CloudFormation (1&#x2F;2)"></a>Benefits of AWS CloudFormation (1&#x2F;2)</h3><ul>
<li>Infrastructure as code</li>
<li>No resources are manually created, which is excellent for control</li>
<li>The code can be version controlled for example using git</li>
<li>Changes to the infrastructure are reviewed through code</li>
<li>Cost</li>
<li>Each resources within the stack is tagged with an identifier so you can easily see how
much a stack costs you</li>
<li>You can estimate the costs of your resources using the CloudFormation template</li>
<li>Savings strategy: In Dev, you could automation deletion of templates at 5 PM and
recreated at 8 AM, safely</li>
</ul>
<h3 id="Benefits-of-AWS-CloudFormation-2-x2F-2"><a href="#Benefits-of-AWS-CloudFormation-2-x2F-2" class="headerlink" title="Benefits of AWS CloudFormation (2&#x2F;2)"></a>Benefits of AWS CloudFormation (2&#x2F;2)</h3><ul>
<li>Productivity</li>
<li>Ability to destroy and re-create an infrastructure on the cloud on the fly</li>
<li>Automated generation of Diagram for your templates!</li>
<li>Declarative programming (no need to figure out ordering and orchestration)</li>
<li>Separation of concern: create many stacks for many apps, and many layers. Ex:</li>
<li>VPC stacks</li>
<li>Network stacks</li>
<li>App stacks</li>
<li>Don’t re-invent the wheel</li>
<li>Leverage existing templates on the web!</li>
<li>Leverage the documentation</li>
</ul>
<h3 id="How-CloudFormation-Works"><a href="#How-CloudFormation-Works" class="headerlink" title="How CloudFormation Works"></a>How CloudFormation Works</h3><ul>
<li>Templates have to be uploaded in S3 and then referenced in
CloudFormation</li>
<li>To update a template, we can’t edit previous ones. We have to reupload a new version of the template to AWS</li>
<li>Stacks are identified by a name</li>
<li>Deleting a stack deletes every single artifact that was created by
CloudFormation.</li>
</ul>
<h3 id="Deploying-CloudFormation-templates"><a href="#Deploying-CloudFormation-templates" class="headerlink" title="Deploying CloudFormation templates"></a>Deploying CloudFormation templates</h3><ul>
<li>Manual way:</li>
<li>Editing templates in the CloudFormation Designer</li>
<li>Using the console to input parameters, etc</li>
<li>Automated way:</li>
<li>Editing templates in a YAML file</li>
<li>Using the AWS CLI (Command Line Interface) to deploy the templates</li>
<li>Recommended way when you fully want to automate your flow</li>
</ul>
<h3 id="CloudFormation-Building-Blocks"><a href="#CloudFormation-Building-Blocks" class="headerlink" title="CloudFormation Building Blocks"></a>CloudFormation Building Blocks</h3><p>Templates components</p>
<ol>
<li>Resources: your AWS resources declared in the template (MANDATORY)</li>
<li>Parameters: the dynamic inputs for your template</li>
<li>Mappings: the static variables for your template</li>
<li>Outputs: References to what has been created</li>
<li>Conditionals: List of conditions to perform resource creation</li>
<li>Metadata
Templates helpers:</li>
<li>References</li>
<li>Functions</li>
</ol>
<h3 id="Note"><a href="#Note" class="headerlink" title="Note:"></a>Note:</h3><p>This is an introduction to CloudFormation</p>
<ul>
<li>It can take over 3 hours to properly learn and master CloudFormation</li>
<li>This lecture is meant so you get a good idea of how it works</li>
<li>The exam expects you to understand how to read CloudFormation</li>
</ul>
<p>[SAA-C02]</p>
<h3 id="CloudFormation-StackSets"><a href="#CloudFormation-StackSets" class="headerlink" title="CloudFormation - StackSets"></a>CloudFormation - StackSets</h3><ul>
<li>Create, update, or delete stacks across <strong>multiple accounts and regions</strong> with a single operation</li>
<li>Administrator account to create StackSets</li>
<li>Trusted accounts to create, update,
delete stack instances from StackSets</li>
<li>When you update a stack set, all associated stack instances are updated throughout all accounts and regions.</li>
</ul>
<p>[*]</p>
<h3 id="AWS-ECS-–-Elastic-Container-Service"><a href="#AWS-ECS-–-Elastic-Container-Service" class="headerlink" title="AWS ECS – Elastic Container Service"></a>AWS ECS – Elastic Container Service</h3><ul>
<li>ECS is a container orchestration service</li>
<li>ECS helps you run Docker containers on EC2 machines</li>
<li>ECS is complicated, and made of:<ul>
<li>“ECS Core”: Running ECS on user-provisioned EC2 instances</li>
<li>Fargate: Running ECS tasks on AWS-provisioned compute (serverless)</li>
<li>EKS: Running ECS on AWS-powered Kubernetes (running on EC2)</li>
<li>ECR: Docker Container Registry hosted by AWS</li>
</ul>
</li>
<li>ECS &amp; Docker are very popular for microservices</li>
<li><strong>For now, for the exam, only “ECS Core” &amp; ECR is in scope</strong></li>
<li>IAM security and roles at the ECS task level</li>
</ul>
<h3 id="What’s-Docker"><a href="#What’s-Docker" class="headerlink" title="What’s Docker?"></a>What’s Docker?</h3><ul>
<li>Docker is a “container technology”</li>
<li>Run a containerized application on any machine with Docker installed</li>
<li>Containers allows our application to work the same way anywhere</li>
<li>Containers are isolated from each other</li>
<li>Control how much memory &#x2F; CPU is allocated to your container</li>
<li>Ability to restrict network rules</li>
<li>More efficient than Virtual machines</li>
<li>Scale containers up and down very quickly (seconds)</li>
</ul>
<h3 id="AWS-ECS-–-Use-cases"><a href="#AWS-ECS-–-Use-cases" class="headerlink" title="AWS ECS – Use cases"></a>AWS ECS – Use cases</h3><ul>
<li>Run microservices</li>
<li>Ability to run multiple docker containers on the same machine</li>
<li>Easy service discovery features to enhance communication</li>
<li>Direct integration with Application Load Balancers</li>
<li>Auto scaling capability</li>
<li>Run batch processing &#x2F; scheduled tasks</li>
<li>Schedule ECS containers to run on On-demand &#x2F; Reserved &#x2F; Spot instances</li>
<li>Migrate applications to the cloud</li>
<li>Dockerize legacy applications running on premise</li>
<li>Move Docker containers to run on ECS</li>
</ul>
<p>[*]</p>
<h3 id="AWS-ECS-–-ALB-integration"><a href="#AWS-ECS-–-ALB-integration" class="headerlink" title="AWS ECS – ALB integration"></a>AWS ECS – ALB integration</h3><ul>
<li>Application Load Balancer (ALB) has a direct integration feature with ECS called “port mapping”</li>
<li>This allows you to run multiple instances of the same application on the same EC2 machine</li>
<li>Use cases:<ul>
<li>Increased resiliency even if runningon one EC2 instance</li>
<li>Maximize utilization of CPU &#x2F; cores</li>
<li>Ability to perform rolling upgrades without impacting application uptime</li>
</ul>
</li>
</ul>
<h3 id="AWS-ECS-–-ECS-Setup-amp-Config-file"><a href="#AWS-ECS-–-ECS-Setup-amp-Config-file" class="headerlink" title="AWS ECS – ECS Setup &amp; Config file"></a>AWS ECS – ECS Setup &amp; Config file</h3><ul>
<li><p>Run an EC2 instance, install the ECS agent with ECS config file</p>
</li>
<li><p>Or use an ECS-ready Linux AMI (still need to modify config file)</p>
</li>
<li><p>ECS Config file is at <strong>&#x2F;etc&#x2F;ecs&#x2F;ecs.config</strong>
ECS_CLUSTER&#x3D;
ECS_ENGINE_AUTH_dATA&#x3D;
ECS_AVAILABLE_LOGGING_DRIVERS&#x3D;
ECS_ENABLE_TASK_IAM_ROLE&#x3D;true</p>
</li>
</ul>
<p>[SAA-C02]</p>
<h3 id="ECS-IAM-Task-Roles"><a href="#ECS-IAM-Task-Roles" class="headerlink" title="ECS - IAM Task Roles"></a>ECS - IAM Task Roles</h3><ul>
<li>The EC2 instance should have an IAM role allowing it to access the ECS service (for the ECS agent)</li>
<li>Each ECS task should have an ECS IAM task role to perform their API calls</li>
<li>Use the “taskRoleArn” parameter in a task definition</li>
</ul>
<p>[SAA-C02]</p>
<h3 id="Fargate"><a href="#Fargate" class="headerlink" title="Fargate"></a>Fargate</h3><p>• When launching an ECS Cluster, we have to create our EC2 instances
• If we need to scale, we need to add EC2 instances
• So we manage infrastructure…</p>
<p>• With Fargate, it’s all Serverless!
• We don’t provision EC2 instances
• We just create task definitions, and AWS will run our containers for us
• To scale, just increase the task number. Simple! No more EC2 J</p>
<p>[SAA-C02]</p>
<h3 id="Amazon-EKS-Overview"><a href="#Amazon-EKS-Overview" class="headerlink" title="Amazon EKS Overview"></a>Amazon EKS Overview</h3><p>• Amazon EKS standards for Amazon Elastic <strong>Kubernetes</strong> Service
• It is a way to launch <strong>managed Kubernetes clusters on AWS</strong>
• Kubernetes is an open-source system for automatic deployment, scaling
and management of containerized (usually Docker) application
• It’s an alternative to ECS, similar goal but different API
• EKS supports <strong>EC2</strong> if you want to to deploy worker nodes or <strong>Fargate</strong> to
deploy serverless containers
• <strong>Use case</strong>: if your company is already using Kubernetes on-premises or in
another cloud, and wants to migrate to AWS using Kubernetes</p>
<h3 id="AWS-Step-Functions"><a href="#AWS-Step-Functions" class="headerlink" title="AWS Step Functions"></a>AWS Step Functions</h3><p>• Build serverless visual workflow to orchestrate your Lambda functions
• Represent flow as a JSON state machine
• Features: sequence, parallel, conditions, timeouts, error handling…
• Can also integrate with EC2, ECS, On premise servers, API Gateway
• Maximum execution time of 1 year
• Possibility to implement human approval feature
• Use cases:
  • Order fulfillment
  • Data processing
  • Web applications
  • Any workflow</p>
<h3 id="Quick-word-on-Chef-x2F-Puppet"><a href="#Quick-word-on-Chef-x2F-Puppet" class="headerlink" title="Quick word on Chef &#x2F; Puppet"></a>Quick word on Chef &#x2F; Puppet</h3><p>• They help with managing configuration as code
• Helps in having consistent deployments
• Works with Linux &#x2F; Windows
• Can automate: user accounts, cron, ntp, packages, services…</p>
<p>• They leverage “Recipes” or ”Manifests”</p>
<p>• Chef &#x2F; Puppet have similarities with SSM &#x2F; Beanstalk &#x2F; CloudFormation
but they’re open-source tools that work cross-cloud</p>
<hr>
<h2 id="Other-Services-Cheat-Sheet"><a href="#Other-Services-Cheat-Sheet" class="headerlink" title="Other Services: Cheat Sheet"></a>Other Services: Cheat Sheet</h2><p>Here’s a quick cheat-sheet to remember all these services:</p>
<p><strong>CodeCommit</strong>: service where you can store your code. Similar service is GitHub</p>
<p><strong>CodeBuild</strong>: build and testing service in your CICD pipelines</p>
<p><strong>CodeDeploy</strong>: deploy the packaged code onto EC2 and AWS Lambda</p>
<p><strong>CodePipeline</strong>: orchestrate the actions of your CICD pipelines (build stages, manual approvals, many deploys, etc)</p>
<p><strong>CloudFormation</strong>: Infrastructure as Code for AWS. Declarative way to manage, create and update resources.</p>
<p><strong>ECS</strong> (Elastic Container Service): Docker container management system on AWS. Helps with creating micro-services.</p>
<p><strong>ECR</strong> (Elastic Container Registry): Docker images repository on AWS. Docker Images can be pushed and pulled from there</p>
<p><strong>Step Functions</strong>: Orchestrate &#x2F; Coordinate Lambda functions and ECS containers into a workflow</p>
<p><strong>SWF</strong> (Simple Workflow Service): Old way of orchestrating a big workflow.</p>
<p><strong>EMR</strong> (Elastic Map Reduce): Big Data &#x2F; Hadoop &#x2F; Spark clusters on AWS, deployed on EC2 for you</p>
<p><strong>Glue</strong>: ETL (Extract Transform Load) service on AWS</p>
<p><strong>OpsWorks</strong>: managed Chef &amp; Puppet on AWS</p>
<p><strong>ElasticTranscoder</strong>: managed media (video, music) converter service into various optimized formats</p>
<p><strong>Organizations</strong>: hierarchy and centralized management of multiple AWS accounts</p>
<p><strong>Workspaces</strong>: Virtual Desktop on Demand in the Cloud. Replaces traditional on-premise VDI infrastructure</p>
<p><strong>AppSync</strong>: GraphQL as a service on AWS</p>
<p><strong>SSO</strong> (Single Sign On): One login managed by AWS to log in to various business SAML 2.0-compatible applications (office 365 etc)</p>
<hr>
<p>Other Services: Quiz</p>
<p>Quiz 23|16 questions</p>
<p>Question 1:
You are looking for a service to store docker images in AWS. Which one do you recommend?
A: ECR</p>
<p>  Amazon Elastic Container Registry (ECR) is a fully-managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker containers</p>
<p>Question 2:
You would like to find a managed-service in AWS alternative to GitLab, in order to version control your code entirely in AWS. Which technology do you recommend?
A: CodeCommit</p>
<p>  CodeCommit is used to store and version control your code and as such, it’s an alternative to GitLab and GitHub</p>
<p>Question 3:
As part of your disaster recovery strategy, you would like to make sure your entire infrastructure is code, so that you can easily re-deploy it in any region. Which service do you recommend?
A: CloudFormation</p>
<p>  CloudFormation is the de-facto service in AWS for infrastructure as code.</p>
<p>Question 4:
You need to manage a fleet of Docker containers in the cloud, which service do you recommend?
A: ECS</p>
<p>  ECS is a container orchestrator service and the correct service to manage a fleet of Docker containers in the cloud</p>
<p>Question 5:
You would like to orchestrate your CICD pipeline to deliver all the way to Elastic Beanstalk. Which service do you recommend?
A: CodePipeline</p>
<p>  CodePipeline is a CICD orchestration service, and has an integration with Elastic Beanstalk</p>
<p>Question 6:
You need to deploy your code to a fleet of EC2 instances with a specific strategy. Which technology do you recommend?
A: CodeDeploy</p>
<p>  When deploying code directly onto EC2 instances or On Premise servers, CodeDeploy is the service to use. You can define the strategy (how fast the rollout of the new code should be)</p>
<p>Question 7:
You have a Jenkins CI build server hosted on premise and you would like to de-commission it and replace it by a managed service on AWS. Which service do you recommend?
A: CodeBuild</p>
<p>  CodeBuild is an alternative to Jenkins</p>
<p>Question 8:
You need to orchestrate a series of AWS Lambda function into a workflow. Which service do you recommend?
A: Step Functions</p>
<p>Question 9:
You are looking to create an Hadoop cluster to perform Big Data Analysis. Which service do you recommend on using?
A: EMR (Elastic MapReduce)</p>
<p>  EMR is the AWS way of creating an Hadoop cluster with the tools of your choosing.</p>
<p>Question 10:
You are looking to move data all around your AWS databases using a managed ETL service that has a metadata catalog feature. Which one do you recommend?
A: Glue</p>
<p>  Glue is an ETL service</p>
<p>Question 11:
Your company is already using Chef recipes to manage its infrastructure. You would like to move to the AWS cloud and keep on using Chef. What service do you recommend?
A: OpsWorks</p>
<p>Question 12:
You work for a consulting company which has recently decided to create video training content for their clients. They would like to view the videos on different devices such as iPhone, iPad, Web browsers. Which service do you recommend to convert the videos?
A: Elastic Transcoder</p>
<p>Question 13:
Your organization would like to create various accounts to physically separate their dev, test and production environments. Your IT lead would still like to manage these environments centrally from a billing purposes, in order for management to be simple. Which service do you recommend?
A: Organizations</p>
<p>  AWS Organizations allow you to create multiple AWS accounts and centralize them around a single organization for simplified and unified billing.</p>
<p>Question 14:
You have a VDI (Virtual Desktop Infrastructure) on premise and as a solution architect, you would like to optimize maintenance and management cost by switching to virtual desktops on the AWS Cloud. Which service do you recommend?
A: Workspaces</p>
<p>  Amazon WorkSpaces is a managed, secure cloud desktop service. You can use Amazon WorkSpaces to provision either Windows or Linux desktops</p>
<p>Question 15:
Your developers are creating a mobile application and would like to have a managed GraphQL backend. Which service do you recommend?
A: AppSync</p>
<p>Question 16:
You are deploying your application on an ECS cluster made of EC2 instances. The cluster is hosting one application that has been issuing API calls to DynamoDB successfully. Upon adding a second application, which issues API calls to S3, you are getting authorization issues. What should you do to resolve the problem and ensure proper security?
A: Create an IAM task role for the new application</p>
<hr>
<h2 id="Section-25-WhitePapers-and-Architectures-AWS-Certified-Solutions-Architect-Associate"><a href="#Section-25-WhitePapers-and-Architectures-AWS-Certified-Solutions-Architect-Associate" class="headerlink" title="Section 25: WhitePapers and Architectures - AWS Certified Solutions Architect Associate"></a>Section 25: WhitePapers and Architectures - AWS Certified Solutions Architect Associate</h2><h3 id="Well-Architected-Framework-General-Guiding-Principles"><a href="#Well-Architected-Framework-General-Guiding-Principles" class="headerlink" title="Well Architected Framework General Guiding Principles"></a>Well Architected Framework General Guiding Principles</h3><p>• Stop guessing your capacity needs
• Test systems at production scale
• Automate to make architectural experimentation easier
• Allow for evolutionary architectures
• Design based on changing requirements
• Drive architectures using data
• Improve through game days
• Simulate applications for flash sale days</p>
<h2 id="Well-Architected-Framework"><a href="#Well-Architected-Framework" class="headerlink" title="Well Architected Framework"></a>Well Architected Framework</h2><p>5 Pillars
• 1) Operational Excellence
• 2) Security
• 3) Reliability
• 4) Performance Efficiency
• 5) Cost Optimization
• They are not something to balance, or trade-offs, they’re a synergy</p>
<h2 id="Well-Architected-Framework-1"><a href="#Well-Architected-Framework-1" class="headerlink" title="Well Architected Framework"></a>Well Architected Framework</h2><p>• It’s also questions!
• Let’s look into the Well-Architected Tool
• <a target="_blank" rel="noopener" href="https://console.aws.amazon.com/wellarchitected">https://console.aws.amazon.com/wellarchitected</a> AWS Well-Architected Tool</p>
<h2 id="1-Operational-Excellence"><a href="#1-Operational-Excellence" class="headerlink" title="1) Operational Excellence"></a>1) Operational Excellence</h2><p>• Includes the ability to run and monitor systems to deliver business value
and to continually improve supporting processes and procedures
• Design Principles
  • Perform operations as code - Infrastructure as code
  • Annotate documentation - Automate the creation of annotated documentation
  after every build
  • Make frequent, small, reversible changes - So that in case of any failure, you can reverse it
  • Refine operations procedures frequently - And ensure that team members are
  familiar with it
  • Anticipate failure
  • Learn from all operational failures</p>
<hr>
<p>WhitePaper Quiz
Quiz 24|1 question</p>
<p>Question 1:
You would like to get AWS recommendations on actual potential cost savings, performance, service limits improvements amongst other things. Which service do you recommend?
A: Trusted Advisor</p>
<hr>
<hr>
<p>aws configure –profile</p>
<p>aws configure set default.s3.signature_version s3v4</p>
<p>Not working</p>
<ul>
<li>Lecture 100</li>
</ul>
<hr>
<p>UTILS UTLS</p>
<h2 id="AWS-Policy-Generator"><a href="#AWS-Policy-Generator" class="headerlink" title="AWS Policy Generator"></a>AWS Policy Generator</h2><p><a target="_blank" rel="noopener" href="https://awspolicygen.s3.amazonaws.com/policygen.html">https://awspolicygen.s3.amazonaws.com/policygen.html</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://blog.pablo-magro-gaspar.site/2020/05/13/AWS-Certified-Solutions-Architect-Certification-SAA-C02/" data-id="cljtprqzk0001fbop36ce24l5" class="article-share-link">Share</a>
      
        <a href="https://blog.pablo-magro-gaspar.site/2020/05/13/AWS-Certified-Solutions-Architect-Certification-SAA-C02/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Certifications/" rel="tag">Certifications</a></li></ul>

    </footer>
  </div>
  
</article>


  

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/07/08/Getting-Started-With-Hexo/">Getting started with Hexo</a>
          </li>
        
          <li>
            <a href="/2023/05/15/AWS-Certified-SysOps-Administrator-Associate-SOA-C02/">AWS Certified SysOps Administrator – Associate (SOA-C02)</a>
          </li>
        
          <li>
            <a href="/2023/05/01/Prompts/">Prompts</a>
          </li>
        
          <li>
            <a href="/2023/04/14/React-Hooks-All-React-Hooks-Explained/">React Hooks - Some React Hooks Explained</a>
          </li>
        
          <li>
            <a href="/2023/04/03/Coding-Resources/">Coding Resources</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/AI/">AI</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/AWS/">AWS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/CLI/">CLI</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/CLI/youtube-dl/">youtube-dl</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Coding/">Coding</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/GNU-Linux/">GNU/Linux</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/GNU-Linux/CLI/">CLI</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/GNU-Linux/Debian/">Debian</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/GNU-Linux/Installer/">Installer</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/GNU-Linux/KDE/">KDE</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Git/">Git</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Global/">Global</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Google/">Google</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Google/OAuth2-0/">OAuth2.0</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Google/OAuth2-0/Nodemailer/">Nodemailer</a></li></ul></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo/">Hexo</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hosting/">Hosting</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/KDE/">KDE</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/KDE/Dolphin/">Dolphin</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kubernetes/">Kubernetes</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Markdown/">Markdown</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/OS/">OS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Open-AI/">Open AI</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Protractor/">Protractor</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/React/">React</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Refranes-Citas-Proverbios/">Refranes, Citas, Proverbios</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/VS-Code/">VS Code</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Applications/" style="font-size: 10px;">Applications</a> <a href="/tags/Bitbucket/" style="font-size: 10px;">Bitbucket</a> <a href="/tags/Bootable/" style="font-size: 10px;">Bootable</a> <a href="/tags/Certifications/" style="font-size: 20px;">Certifications</a> <a href="/tags/Commands/" style="font-size: 10px;">Commands</a> <a href="/tags/Debian/" style="font-size: 10px;">Debian</a> <a href="/tags/Doc/" style="font-size: 10px;">Doc</a> <a href="/tags/Extensions/" style="font-size: 10px;">Extensions</a> <a href="/tags/File/" style="font-size: 10px;">File</a> <a href="/tags/Filter/" style="font-size: 10px;">Filter</a> <a href="/tags/Free/" style="font-size: 10px;">Free</a> <a href="/tags/GNU-Linux/" style="font-size: 10px;">GNU/Linux</a> <a href="/tags/Ghostscript/" style="font-size: 10px;">Ghostscript</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/Hexo/" style="font-size: 10px;">Hexo</a> <a href="/tags/Hooks/" style="font-size: 10px;">Hooks</a> <a href="/tags/ISO/" style="font-size: 10px;">ISO</a> <a href="/tags/Install/" style="font-size: 20px;">Install</a> <a href="/tags/Java/" style="font-size: 10px;">Java</a> <a href="/tags/Menu-Option/" style="font-size: 10px;">Menu, Option</a> <a href="/tags/Monitor/" style="font-size: 10px;">Monitor</a> <a href="/tags/Node/" style="font-size: 10px;">Node</a> <a href="/tags/PDF/" style="font-size: 10px;">PDF</a> <a href="/tags/Packages/" style="font-size: 10px;">Packages</a> <a href="/tags/Post-Install/" style="font-size: 10px;">Post Install</a> <a href="/tags/Prompts/" style="font-size: 10px;">Prompts</a> <a href="/tags/Protractor/" style="font-size: 10px;">Protractor</a> <a href="/tags/Resources/" style="font-size: 10px;">Resources</a> <a href="/tags/SOA-C02/" style="font-size: 10px;">SOA-C02</a> <a href="/tags/ServletContextListener/" style="font-size: 10px;">ServletContextListener</a> <a href="/tags/SysOps/" style="font-size: 10px;">SysOps</a> <a href="/tags/Token/" style="font-size: 10px;">Token</a> <a href="/tags/Websites/" style="font-size: 10px;">Websites</a> <a href="/tags/hplip/" style="font-size: 10px;">hplip</a> <a href="/tags/youtube-dl/" style="font-size: 10px;">youtube-dl</a>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 Pablo Magro Gaspar<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    
<script>
  var disqus_shortname = 'pablomagro';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>


    
<link rel="stylesheet" href="/scroll2top/scroll2top.css">


<script src="/scroll2top/scroll2top.js"></script>


<div class="scrollup">
  <a title="Scroll up" href="#">Scroll</a>
</div>
    
  </div>
</body>
</html>
